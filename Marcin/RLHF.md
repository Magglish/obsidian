# RLHF

W Reinforcement Learning from Human Feedback chodzi o to aby zupdateować wagi modelu na bazie decyzji ludzkiej - model generuje odpowiedzi, człowiek następnie rankuje które są lepsze a które są gorsze i w ten sposób model updatuje swoje wagi.

W [[Jak był uczony ChatGPT|ChatGPT]] używano [[Proximal Policy Optimization]]