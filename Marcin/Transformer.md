# Transformer:

Oryginalny transformer z paperu [Attention is all you need](https://arxiv.org/abs/1706.03762) to architektura [[Encoder-Decoder]].

Różnica jest taka, że:

1. Są tam [[Positional embedding|pozycyjne embeddingi]] wskazujące pozycje tekstu.
2. Używany jest [[Attention mechanism|mechanizm atencji]]
