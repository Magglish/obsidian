# Multi-Head Self-Attention

Czyli zaaplikowanie [[Self-Attention]] wielokrotnie.

Idea: Nadzieja, że każda atencja będzie uczyła się czegoś innego - np. jedna uczy sie globalnego kontekstu, inna semantycznych zaleznosci pomiedzy słowami. Ale są też takie które są redundantne. 

![[Multi-Head Attention idea.png]]

![[Multi-Head Computations.png]]