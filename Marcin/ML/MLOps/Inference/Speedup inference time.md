# Speedup inference time
#### Kompresja modelu
1. [[Low-rank factorization]]
2. [[Knowledge distillation]]
3. [[Pruning]]
4. [[Quantization]]

#### Przyspieszenie inferencji


#### Lepszy hardware



#dokoncz na podstawie https://arxiv.org/abs/1710.09282