# Knowledge distillation
Knowledge distillation to metoda w której mały model (*student*) uczony jest aby naśladować większy model lub [[Ensemble|ensemble]] modeli (*teacher*).