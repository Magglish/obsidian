# Word Embeddings

Word Embedding to nic innego jak reprezentacja numeryczna słów za pomocą wektorów.

Przykład:

$$human = \begin{bmatrix} -0.107 \\ 0.543 \\ 0.678 \\ -0.203 \\ 0.809 \\ 0.123 \\ 0.005 \\ \end{bmatrix}$$

Zastosowania:

1. Wykorzystanie jako cechy w modelowaniu
2. [[Cosine similarity|Mierzenie podobieństwa]]
3. Wykonywanie operacji na tekście

![[Word Embeddings calculations.png]]

Podejścia:

1. [[One-hot word embeddings]]
2. [[Continuous Bag of Words]]
3. [[Continuous Skip Gram]]
4. [[Word2Vec]]