# Word Embeddings

Word Embedding to nic innego jak reprezentacja numeryczna słów za pomocą wektorów, powstają w wyniku użycia modeli [[Word2Vec]].

$$motel = \begin{bmatrix} 0 \\ 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \end{bmatrix}$$



Podejścia:

1. [[One-hot word embeddings]]
2. [[CBOW]]
3. [[SkipGram]]