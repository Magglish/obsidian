# Word Embeddings

Word Embedding to nic innego jak reprezentacja numeryczna słów za pomocą wektorów, powstają w wyniku użycia modeli [[Word2Vec]].

Przykład:

$$human = \begin{bmatrix} -0.107 \\ 0.543 \\ 0.678 \\ -0.203 \\ 0.809 \\ 0.123 \\ 0.005 \\ \end{bmatrix}$$



Podejścia:

1. [[One-hot word embeddings]]
2. [[CBOW]]
3. [[SkipGram]]