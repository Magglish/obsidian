# Stochastic Gradient Descent with Momentum

Algorytm [[Stochastic Gradient Descent|SGD]] z momentum.

![[Pasted image 20240216130716.png]]

Momentum to inaczej wartośći poprzednich gradientów, aby nadać większego pedu optymalizacji. Momentum można uznać też jako przyśpieszenie.

Przeważnie momentum $\beta$ ustalane jest na 0.5, 0.9 albo 0.99.

Intuicja:


1. Jeżeli poruszamy się w