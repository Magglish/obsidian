# Parametric ReLU

To samo co [[Leaky ReLU]], ale zamiast stałej wartości np. $0.01$, wprowadzamy zmienną $\alpha$, która jest optymalizowana w trakcie [[Backpropagation|wstecznej propagacji błędów.]]

![[Parametric ReLU Activation Function.png]]