# Maxout

![[Maxout Activation Function.png]]

1.  Generalizacja wokół [[ReLU]] i [[Leaky ReLU]]
2. Jest nieliniowa
3. Nie ma problemu z [[Vanishing Gradient]]

Problemy:

1. Podwaja liczbę parametrów w neuronie