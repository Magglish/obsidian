# Leaky ReLU

![[Leaky ReLU Activation Function.png]]

Te same zalety co [[ML/Neural Networks/Optimization/Activation Function/ReLU|ReLU]] jednakże dzięki temu, że dla wartości $x < 0$ mamy gradient, rozwiązuje on problem jaki występuje w [[ReLU]] z "martwymi neuronami".