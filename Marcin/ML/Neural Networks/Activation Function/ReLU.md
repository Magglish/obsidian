# ReLU

![[ReLU Activation Function.png]]

1. Efektywne obliczeniowo - łatwiej policzyć niż np. `exp` w [[Sigmoid]] czy [[Tanh]]
2. Nie ma problemu z [[Vanishing Gradient]] (ale w wymiarze pozytywnym, w "połowie regionu")
3. Sieci neuronowe z tą funkcją aktywacji znacznie szybciej się uczą, łatwiej zbiegają do optimum 

Wady:

1. 
