# Batch Normalization

![[Batch Normalization in a nutshell.png]]

Sposób na rozwiązanie problemów podczas [[Weight Initialization|inicjalizacji wag]] - chcielbyśmy żeby wartości aktywacji w neuronach były zcentrowane wokół zera, aby uniknąć problemów z ich uczeniem przez 

https://www.reddit.com/r/datascience/comments/1aihddg/visualizing_what_batch_normalization_is_and_its/

Czy BatchNorm jest używane w NLP? Czy może LayerNorm albo GRoupNorm?

  

[https://www.reddit.com/r/datascience/comments/1aihddg/comment/kovfrrd/?utm_source=share&utm_medium=web2x&context=3](https://www.reddit.com/r/datascience/comments/1aihddg/comment/kovfrrd/?utm_source=share&utm_medium=web2x&context=3)