# Transformer:

Oryginalny transformer z paperu [Attention is all you need](https://arxiv.org/abs/1706.03762) to architektura [[Encoder-Decoder]].

![[Pasted image 20240227113452.png]]

Różnica jest taka, że:

1. Są tam [[Positional Embeddings|pozycyjne embeddingi]] wskazujące pozycje tekstu.
2. Używany jest [[Multi-Head Self-Attention|mechanizm atencji]]
3. Używane [[Residual Connection]]
4. Używane [[Layer Normalization]]
