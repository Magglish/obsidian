# BERT 

BERT - [Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805).

To tylko i wyłącznie [[Encoder-Decoder|Encoder]]

Uczony techniką [[Masked language modelling]] oraz [[Next-sentence prediction]], ale jak sie okazało ten task nie jest potrzebny

![[BERT 1.png]]

![[BERT 2.png]]

Ograniczenia:

1. Nie użyjemy ich do generowania tekstu

![[BERT limitations.png]]

Lepsze:

1. [[RoBERTa]]
