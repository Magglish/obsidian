# Momentum

To samo co [[Stochastic Gradient Descent]] jednakże w tym podejściu wykorzystujemy średnio ważoną gradientów z poprzednich iteracji/przebiegów aby dodać ją do naszego gradientu. Czyli tworzymy tak zwane momentum albo przyśpieszenie. Zaletą jest to, że możemy w ten sposób wyjść z lokalnego minimum, który napotkamy.