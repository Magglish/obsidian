# Background tasks

1. Niestety ale z zapisywaniem danych do baz - czy to Redis czy to Postgres wiąże się pewne wyzwanie. Ale zanim do tego przejdziemy to... tak jak Wam mówiłem - podczas projektowania API powinny nam przyświecać dwa cele - prostota i szybkość. Jak widzieliście, dodanie cache zwiększyło szybkość naszego API. Ale czy na pewno?
2. Spójrzmy prosze w kod na ten endpoint i zastanówmy się, które elementy w nim spowalniają nasze API. Czy ktoś z Was widzi takie elementy?
3. Podzielmy to na dwie części:
	1. Uzyskujemy odpowiedź z cachea - to co nam spowalnia działanie naszego API to zapisanie do bazy postgres `decision_postgres_client.write(request, response)`.
	2. Uzyskujemy odpowiedzi z modelu - to co nam spowalnia działanie naszego API to:
		- czytanie z bazy Redisa: `decision_redis_client.read(request)`
		- zapisanie do Redisa: `decision_redis_client.write(request, response)`
		- zapisanie do Postgresa: `decision_postgres_client.write(request, response)`
4. W naszym przypadku zapisywanie do baz Redisa czy do Postgresa trwa dosłownie kilka milisekund, wręcz niezauważalne. Ale na produkcji może to być znacznie dłuższy czas - to może być uzależnione od wielu różnych czynników: wielkość danych które chcemy zapisać do bazy, jej obłożenie, w jakiej lokalizacji znajduje się baza danych np. nasze serwisy działają w Warszawie, a baza danych jest w Belgii.

![[GCP Latencies.png]]

Jak sobie spojrzymy jakie są opóźnienia na Google Clud w zależności od regionów to można zobaczyć, że serwis działający w Warszawie (europe-central) a baza danych w Belgii (europe-west1) to już jest 20 ms opóźnienia na samej sieci. A gdybyśmy chcieli wysłać dane do Ameryki to mamy ponad 100 ms. To jest znacznie, znacznie więcej niż same wykonanie predykcji przez nasz model - bo nasz model zwraca predykcje w 2 ms. 

5. To teraz jak ten problem rozwiązać? FastAPI przychodzi nam z pomocą i możemy w nim użyć tak zwanych Background Tasks, czyli po prostu możemy zdefiniować co ma być wykonane w tle.  
6. Zaimplementujmy to, a potem omówie dokładnie jak to działa.

Zaimportujmy obiekt Background Tasks

```python
from fastapi import BackgroundTasks
```

Do naszego endpointa musimy dodać parametr `background_tasks: BackgroundTasks`

```python
@app.post("/decisions")  
async def decisions(  
    request: DecisionRequest,  
    background_tasks: BackgroundTasks,  
) -> DecisionResponse:
```

Aby dodać coś do działania w tle, musimy zakodzić to jako funkcję. Zatem ja w `src/service` dodam nowy folder `tasks` a w nim dwa pliki `redis.py` oraz `postgres.py`

W `postgres.py` zapiszemy sobie funkcje:

```python
def write_to_postgres(postgres_client, request, response):  
    postgres_client.write(request, response)
```

A w `redis.py` zapiszemy sobie funkcje:

```python
def write_to_redis(redis_client, request, response):
    redis_client.write(request, response)
```

Teraz jedna uwaga: te funkcje zasługują na dokumentację oraz type hinty, ale musicie mi wybaczyć za to, że ich nie będzie, bo chce żebyśmy mieli więcej czasu na omówienie rzeczy związanych z API, więc chce te rzeczy pominąć.

Następnie zaimportujmy je do `main.py`:

```python
from src.service.tasks.redis import write_to_redis  
from src.service.tasks.postgres import write_to_postgres
```

I teraz sprawa jest bardzo prosta, bo w nasze wszystkie miejsca w których mieliśmy zapisanie do Redisa i do Postgresa, będziemy dodawać taski do tła:

Sprawa jest prosta: `background_tasks` ma metode `add_task`, gdzie pierwszym argumentem jest funkcja, a pozostałe parametry to argumenty do niej:

```python
@app.post("/decisions")
async def decisions(
    request: DecisionRequest,
    background_tasks: BackgroundTasks,
) -> DecisionResponse:
    response = decision_redis_client.read(request)
    if response is not None:
        background_tasks.add_task(
            func=write_to_postgres,
            postgres_client=decision_postgres_client,
            request=request,
            response=response,
        )
        return response

    decision = model.predict_decision(request.to_dataframe())[0]
    response = DecisionResponse(decision=decision)
    background_tasks.add_task(
        func=write_to_postgres,
        postgres_client=decision_postgres_client,
        request=request,
        response=response,
    )
    background_tasks.add_task(
        func=write_to_redis,
        redis_client=decision_redis_client,
        request=request,
        response=response,
    )
    return response
```

Ale zanim wytłumacze jego działanie to jeszcze zrefactoruje go delikatnie, bo tak jak widzicie wydłużył się nam oraz mamy powielony kod w dwóch miejscach, tam gdzie jest zapisanie do postgresa.

Zrefactorujemy do:

```python
@app.post("/decisions")  
async def decisions(  
    request: DecisionRequest,  
    background_tasks: BackgroundTasks,  
) -> DecisionResponse:  
    response = decision_redis_client.read(request)  
    if response is not None:  
        pass  
    else:  
        decision = model.predict_decision(request.to_dataframe())[0]  
        response = DecisionResponse(decision=decision)  
        background_tasks.add_task(  
            func=write_to_redis,  
            redis_client=decision_redis_client,  
            request=request,  
            response=response,  
        )  
    background_tasks.add_task(  
        func=write_to_postgres,  
        postgres_client=decision_postgres_client,  
        request=request,  
        response=response,  
    )    return response
```

Ok to na koniec jeszcze sprawdźmy czy to faktycznie działa 
Sprawdz czy:
1. request jest przetworzony
2. dane sa w bazie sql

Ok to teraz co my zaimplementowaliśmy? Jak już wspomniałem i też sama nazwa tego obiektu wskazuje, to Background Tasks odpowiada za uruchamianie kodu w tle. ALE nie w momencie wywołania tej funkcji, a dopiero po zwróceniu odpowiedzi z API.

Żeby Wam to pokazać, dodajmy sobie proste printy do naszego kodu żebyśmy zobaczyli jaki jes tflow - dodaje printy ponieważ jeszcze nie mieliśmy lekcji o logowaniu.

1. Dodam sobie printy na poczatek i koniec endpointa:

```python
@app.post("/decisions")  
async def decisions(  
    request: DecisionRequest,  
    background_tasks: BackgroundTasks,  
) -> DecisionResponse:  
    print("Starting processing request")
	...
	...
	...
	print("Returning response")
	return response
```
1. Oraz wewnatrz tych tasków

```python
def write_to_redis(redis_client, request, response):  
    print("Starting writing to redis")  
    redis_client.write(request, response)  
    print("Finished writing to redis")
```

```python
def write_to_postgres(postgres_client, request, response):  
    print("Starting writing to postgres")  
    postgres_client.write(request, response)  
    print("Finished writing to postgres")
```

Zrestartujmy nasze API i wyślijmy requesta ze zmieniona wartoscia w zmiennej.

To co powinniśmy zobaczyć to to, że request został przetworzony i odpowiedź zostałą zwrócona, a dopiero później uruchamiane są nasze background taski. I to jest to co nas w tej chwili interesuje i wręcz ratuje. Background taski pozwalają API na zwrócenie odpowiedzi do klienta/osoby odpytującej API tak szybko jak odpowiedź jest już gotowa - natomiast a wszelkie inne operacje, które dodamy do background tasks ZAWSZE uruchamiają się po zwróceniu odpowiedzi - i do nich dodajemy wszelkie zadania/operacje które są długie i mogą po prostu wydłużyć nasz czas odpowiedzi. W naszym przypadku takimi operacjami są zapisy do bazy danych. 

Sprawdźmy jak to działa gdybyśmy jednak do API wysłali więcej requestów.

Założmy, że zapisanie danych do naszej bazy postgres `write_to_postgres.py` trwa 10 sekund

```python
import time


def write_to_postgres(postgres_client, request, response):
    print("Starting writing to postgres")
    time.sleep(10)
    postgres_client.write(request, response)
    print("Finished writing to postgres")
```

Zresetujmy API i odpytajmy je najpierw raz, a potem kilka razy. Widać, że dostajemy odpowiedzi z API od razu, a procesy w tle po prostu kończą się w swoim czasie.

Background Taski to naprawdę świetne rozwiązanie. Nie tylko przydają się do takich długich zapisów do baz danych, czy w ogóle jakichś innych operacji. Ale mają również inne zastosowanie. Nasze API obecnie zwraca predykcje z modelu w ciągu 2 milsekund. To szybko. Ale co gdyby nasz model zwracał odpowiedzi w ciągu 2 minut? W przypadku gdy modele generują odpowiedzi bardzo długo, to API projektuje się inaczej. My tego nie bedziemy robić na naszym zjeździe, więc opiszę Wam jak to może wyglądać

1. Można wykorzystać do tego background taski. Tak na prawdę do background tasków dodajemy zadanie związane z wykonaniem predykcji. Natomiast kllientowi, który odpytuje nasze API zwracamy `status_code=202` czyli `Accepted` - informacja dla niego, że jego żądanie zostało zaakceptowane i jest w trakcie przetwarzania oraz w ciele requesta `ID joba`. Następnie w naszym API zdefiniowany jest drugi endpoint typu `GET` np. `statuses`, który może Ci zwrócić informacje o tym, czy dana predykcja została już dla Ciebie przygotowana, np. zwraca status `PENDING` albo `READY` albo `ERROR`. Jeśli jest `READY` to możemy wtedy odpytać ponownie `decisions` typu `GET`, który dla danego `job_id` zwraca Ci predykcje. Te rozwiązanie sprawdza się, wtedy kiedy ilość requestów jest mała, robimy PoCa, żeby sprawdzić czy takie rozwiązanie jest satysfakcjonujące.
2. Natomiast docelowe rozwiązanie, jak już chcemy iść z tym na produkcję na większą skalę, jest już trudniejsze i bardziej angażujące, tutaj już zachaczamy o tematy związane projektowaniu architektury/system design, dlatego, że docelowe rozwiązanie zakłada użycie asynchronicznych kolejek. Tak na prawdę nasze API jest tylko pośrednikiem w komunikacji. Ponieważ w tym rozwiązaniu mamy system kolejkowy oraz oddzielne serwisy odpowiedzialne za przetwarzanie requestów. Proszę spojrzeć na grafikę. To jest nasze API, które jest dostępne dla klientów. Wszelkie requesty działają tak jak wcześniej. Zwracamy klientowi status_code=202 z informacją ACCEPT oraz job_id. Informacja trafia do kolejki - to może być Kafka, PubSub, RabbitMQ czy inne rozwiązania. Za nią stoją już oddzielne serwisy z modelami, które przyjmują zadania wykonania predykcji i jak tylko je wykonają to zapisują swoje wyniki np. w bazie danych, czy w jakimś storage. Nasze API ma z kolei endpoint, tak jak poprzednio, statuses, który sprawdza czy dana predykcja już jest odłożona w bazie, w storage czy też nie. Jeśli jest gotowe, zwraca odpowiedź, a jeśli nie to np. status `PENDING`.
Ten sposób który prezentuje dotyczy case kiedy nasze modele bardzo długo wykonują predykcje. Natomiast te rozwiązanie można również wykorzystać w sytuacji gdy modele zwracają predykcje bardzo szybko, jak np. nasz model w ciągu 2 ms. ALE rozwiązanie z Background Taskami nas nie satysfakcjonuje. Możemy wtedy zastosować ten sam schemat do tego żeby zapisywać dane do bazy danych czy np. logi do plików. Jest zdefiniowana kolejka, która odpowiedzialna jest za zapisywanie rzeczy do baz danych, a my z API wysyłamy do niej tylko requesty z danymi jakimi chcemy żeby zapisała do bazy danych.

Na koniec chciałbym Wam pokazać pewną ciekawą rzecz, o której nie ma napisane w dokumentacji FastAPI, a powinno być, a natkniecie się dopiero podczas implementowania i może to nie być takie łatwie do zauważenia. Jak widzicie, nasza funkcja z `write_to_postgres` została zapisana jako `def`. I działa to dobrze - od razu dostaje odpowiedź. A co jeżeli zdefiniowałbym to jako `async def` - zresetujmy API i zobaczmy. 
Zobaczcie: nie dostałem odpowiedzi od razu, a w logach API jest info, że zwracamy odpowiedź. Odpale kolejny terminal i wysle zapytanie - wiszą. I czekają aż skończy się background tasks. A logi FastAPI mówią co innego. Temu tematowi poświęcimy uwagę później, przy tematach związanych z asynchronicznością. Jak spojrzymy sobie na dokumentację FastAPI z Background taskami to tego tematu nie ma w ogóle omówionego