# Czym jest "wdrożenie na produkcję"

No dobrze w takim razie zaczniemy dzisiejsze konkrety od zadania sobie pytania: czym jest te przysłowiowe “wdrożenie na produkcję”? Bo na pewno każdy z Was spotkał się z takim terminem, ale teraz wyjaśnimy sobie co to dokładnie oznacza.

Generalnie rzecz biorąc w momencie kiedy już skończycie pracę nad swoim modelem, to macie ten model który spełnia wasze wszelkie oczekiwania, jakość jest zadowalająca, jest akceptacja ze strony biznesu - tak wdrażamy, to chcemy użyć na produkcji.

Teraz spójrzmy na to wszystko z takiego lotu ptaka, żebyśmy złapali kontekstu co w ogóle będziemy robić w ramach tych zajęć.

Wasz model jest w notebooku, możecie sobie wykonywać predykcje na danych które tam podajecie. Super. Ale tak to nie będzie wyglądało rzeczywistości.

Trzeba zacząć od tego żeby ten model jakoś zapisać. Z tym już na pewno mieliście styczność bo szeroko w pythonie jest używane `pickle` który po prostu pozwala pewien obiekt pythonowy zapisać do formatu binarnego jako pewien plik leżący sobie na waszym dysku. Różne frameworki zapisują to w różnych formatach. W przypadku sieci neuronowych bardzo często używany format jest tak zwany ONNX jest to Open Neural Network Exchange. Taki format pozwala nam na to że ktoś kto stworzy sieci neuronową w PyTorchu, może ją przetransformować do ONNX i na przykład inne deweloper mógłby użyć tej sieci korzystając z Tensorflow. 

### WSTAW TUTAJ ZDJECIE PROCESU WDROZENIA Z LOTU PTAKA

Ale samo zapisanie modelu to nie jest wystarczająca rzecz. Dlatego że często wraz z modelem jest w waszym repozytorium stworzona pewna implementacja czyli kod, który w jakiś sposób z niego korzysta. Często to nie jest taki czysty model wzięty prosto z scikit-learna, tylko wokół tego może być na przykład jakaś klasa która ten model nam wrapuje, dostarczając dodatkowe funkcjonalności wokół tego modelu - mogą to być zdefiniowane jakieś kroki czyszczące dane, przygotowujące dane, zanim one zostaną podane  do modelu. Te wszystko kroki mogą być zaimplementowane w jakąś klasę. W efekcie czego korzystanie z tej implementacji może być proste - wpuszczacie surowe dane do niej i otrzymujecie jako wynik predykcje z modelu. Ale w środku dzieje się bardzo rzeczy zanim ten model zostanie użyty. I generalnie wraz z zapisaniem modelu do jakiegoś formatu binarnego również musi być z nim wpięty wasz kod. Czyli oprócz modelu, udostępnicie też Wasz kod.

W tym celu najczęściej wykorzystywano technologią są oczywiście kontenery, do kontenerów przejdziemy oczywiście na drugim zjeździe ale wyobraźcie teraz sobie że kontener, jak sama nazwa może wskazywać, jest pewną paczką, pewnym obiektem, który potrafi trzymać w sobie waszą implementację oraz wasz model jako jedność. I pozwala na dzielenie się tym kontenerem z innym użytkownikiem bądź wysłaniem tego na jakiś serwer.  Zaletą tego podejścia jest to że ten kontener jest wyizolowany i w momencie kiedy go uruchomimy to tak jakbyśmy uruchomili wyizolowane środowisko pythonowe które odpowiedzialne będzie za uruchomienie waszego modelu i waszego kodu, ale bez wpływu na wszystkie inne procesy/rzeczy które działają na tym serwerze. Jest to powszechnie stosowane podejście i bardzo wygodne rozwiązanie jak się przekonacie na kolejny zjazdach.

Czyli podsumowując - wdrożenie polega na tym abyśmy mogli zapisać wyniki naszej pracy czyli nasz kod, nasz końcowy produkt naszej pracy  - czyli w tym wypadku model uczenia maszynowego - i  udostępnienie go w taki sposób żeby można było wziąć ten wasz kod i ten wasz model i wdrożyć go na produkcję.

Teraz czym jest ta przysłowiowa “produkcja”. To wszystko zależy od tego w jakich warunkach będziecie pracować. Podam przykłady:
1.  może to być jakiś fizyczny serwer - komputer stacjonarny - który sobie stoi obok was albo w zupełnie innym pomieszczeniu,
2.  może to być jakiś rack z setką procesorów i kart graficznych i tak dalej. Ale wciąż jest to jakaś serwerownia,  i wciąż są to fizyczne maszyny
3. i powszechnie stosowana obecnych czasach chmura która de facto wciąż też jest serwerem tylko stojącym zupełnie gdzie indziej, nie u was w domu czy w waszej firmie, tylko gdzieś wielkiej serwerowni zarządzanej przez inną firmę.  i komunikujecie się z tą serwerownią poprzez właśnie Interfejs graficzny dostępny w sieci.  Tak można by określić chmura w dużym skrócie.

Niemniej jednak jest to jakiś fizyczny server na którym ten model będzie działał.  na tych serwerach będą wdrożone również inne usługi,  które bardzo chętnie chciałyby skorzystać z waszych rozwiązań - bo chcecie żeby Wasz produkt opierał się o modele uczenia maszynowego. Generalnie te serwery komunikują się ze sobą po sieci, czyli nasz przysłowiowy “internet”. Teraz żeby ten nasz serwer mógł się komunikować z innymi serwerami to jest “wystawiony na świat”, czyli innymi słowy przypisywany jest mu IP który jednocześnie identyfikuje go w sieci i przy okazji też nadawany mu jest adres na przykład [www.mojadomena.zmodelami.pl](http://www.mojadomena.zmodelami.pl).  Czyli poprzez dostęp z internetu możemy  zkomunikować się z naszym serwerem, który właśnie dostępny jest pod tym adresem. I wdrożenie polega na tym, aby nasz model i nasz kod uruchomić na tym serverze, w taki sposób że możemy skomunikować się z naszą maszyną właśnie korzystając z internetu, z tego adresu który macie, w taki sposób aby móc wysłać dane do tego serwera, a server rzekazał te dane do naszego modelu, model wykonał predykcję i serwer odpowie nam przesyłając nam dane z predykcjami modelu. I w ten sposób z naszego modelu mogą skorzystać zarówno fizyczne osoby znające adres naszego servera jak i też inne serwisy/usługi które działają w naszej serverowni.

To co mam teraz przestawiłem to jest taki generalny rzut na proces wdrożeniowy z samej góry, bez wnikania w szczegóły, bo dopiero co zaczynamy zajęcia z wdrożenia. To jest rzut z lotu ptaka i w ramach naszych zjazdów będziemy wnikać w szczegóły każdego z tego elementu.