# Czym jest i jak wygląda"wdrożenie na produkcję"

No dobrze w takim razie zaczniemy dzisiejsze konkrety od zadania sobie pytania: czym jest te przysłowiowe “wdrożenie na produkcję”? I jak ono wygląda. Bo na pewno każdy z Was spotkał się z takim terminem, ale teraz wyjaśnimy sobie co to dokładnie oznacza.

Generalnie rzecz biorąc w momencie kiedy już skończycie pracę nad swoim modelem, to macie ten model który spełnia wasze wszelkie oczekiwania, jakość jest zadowalająca, jest akceptacja ze strony biznesu - tak wdrażamy, to chcemy użyć na produkcji. Przedstawia to punkt 1-wszy, mamy tutaj wszystkie elementy, których nauczyliścię na poprzednich zajęciach.

Od razu na wstępie powiem, że kończymy z Notebookami. Świetne narzędzie do eksploracji, przygotowania danych, eskperymentowania z modelami. Jednakże finalnie nasze implementacją muszą być zakodzone w moduły, utworzona struktura projektu - po prostu łatwiej się tym zarządza.

Pytanie jakie sobie można zadać. Czy Notebooka można wdrożyć na produkcję? Są pewne próby i narzędzia, które np. w jakiś sposób Wam tranformują notebooka do np. skryptów, albo robią z tego kontenery itd. ale to się nie sprawdza. Jeśli myślimy o wdrożeniu Notebooka to raczej wtedy kiedy generujemy cykliczne raporty, tworzone ostatecznie w HTMLu czy PDFie.

Trzeba zacząć od tego żeby ten model jakoś zapisać. Z tym już na pewno mieliście styczność bo szeroko w pythonie jest używane `pickle` który po prostu pozwala pewien obiekt pythonowy zapisać do formatu binarnego jako pewien plik leżący sobie na waszym dysku. Różne frameworki zapisują to w różnych formatach. W przypadku sieci neuronowych bardzo często używany format jest tak zwany ONNX jest to Open Neural Network Exchange. Taki format pozwala nam na to że ktoś kto stworzy sieci neuronową w PyTorchu, może ją przetransformować do ONNX i na przykład inne deweloper mógłby użyć tej sieci korzystając z Tensorflow. 


Ale samo zapisanie modelu to nie jest wystarczająca rzecz. Dlatego że często wraz z modelem jest w waszym repozytorium stworzona pewna implementacja czyli kod, który w jakiś sposób z niego korzysta. Często to nie jest taki czysty model wzięty prosto z scikit-learna, tylko wokół tego może być na przykład jakaś klasa która ten model nam wrapuje, dostarczając dodatkowe funkcjonalności wokół tego modelu - mogą to być zdefiniowane jakieś kroki czyszczące dane, przygotowujące dane, zanim one zostaną podane  do modelu. Te wszystko kroki mogą być zaimplementowane w jakąś klasę. W efekcie czego korzystanie z tej implementacji może być proste - wpuszczacie surowe dane do niej i otrzymujecie jako wynik predykcje z modelu. Ale w środku dzieje się bardzo rzeczy zanim ten model zostanie użyty. I generalnie wraz z zapisaniem modelu do jakiegoś formatu binarnego również musi być z nim wpięty wasz kod. Czyli oprócz modelu, udostępnicie też Wasz kod.

Tutaj od razu Wam powiem, że często jest tak, że w jednym repozytorium macie i notebooki z analizami, i kod źródłowy oraz wszelkie inne pliki konfiguracyjne czy skrypty odpowiedzialne za wdrożenie. Jest to najcześciej spotykane, że w jednym repozytorium jest wszystko.

Oczywiście są też podejścia takie, że część eksperymentalna jest w innym repozytorium, a repozytorium z servisami i deploymentem są oddzielne. Ale przy takim podejściu mamy spore wyzwanie z współdzieleniem kodu. Nie będe wnika w szczegóły. Z takim podejściem można spotkać się w dużych zespołach, w firmach, które mają dużą ilośc modeli i przeważnie jest wydzielony jakiś zespół wspierający, np. u mnie jest zespół dedykowany w którym są sami MLOpsi i zajmują się automatyzacją deploymentu i wspieraniem innych zespów w tym procesie.

Następnie tworzymy sposób w jaki możemy udostępnić nasz model - naturalny wybór to API, ale powiem troche wiecej o innych sposbach wdrożenia modeli później.

Ostatecznie jak już mamy punkt 2 i punkt 3, to teraz trzeba jakoś zrobić z tego coś, czym można się podzielić. W tym celu najczęściej wykorzystywano technologią są oczywiście kontenery, do kontenerów przejdziemy oczywiście na drugim zjeździe ale wyobraźcie teraz sobie że kontener, jak sama nazwa może wskazywać, jest pewną paczką, pewnym obiektem, który potrafi trzymać w sobie waszą implementację oraz wasz model jako jedność. I pozwala na dzielenie się tym kontenerem z innym użytkownikiem bądź wysłaniem tego na jakiś serwer.  Zaletą tego podejścia jest to że ten kontener jest wyizolowany i w momencie kiedy go uruchomimy to tak jakbyśmy uruchomili wyizolowane środowisko pythonowe które odpowiedzialne będzie za uruchomienie waszego modelu i waszego kodu, ale bez wpływu na wszystkie inne procesy/rzeczy które działają na tym serwerze. Jest to powszechnie stosowane podejście i bardzo wygodne rozwiązanie jak się przekonacie na kolejny zjazdach.

Mając taki wirtualny zasobnik, następuje teraz wdrożenie na produkcję.

Teraz czym jest ta przysłowiowa “produkcja”. To wszystko zależy od tego w jakich warunkach będziecie pracować. Podam przykłady:
1.  może to być jakiś fizyczny serwer - komputer stacjonarny - który sobie stoi obok was albo w zupełnie innym pomieszczeniu,
2.  może to być jakiś rack z setką procesorów i kart graficznych i tak dalej. Ale wciąż jest to jakaś serwerownia,  i wciąż są to fizyczne maszyny
3. i powszechnie stosowana obecnych czasach chmura która de facto wciąż też jest serwerem tylko stojącym zupełnie gdzie indziej, nie u was w domu czy w waszej firmie, tylko gdzieś wielkiej serwerowni zarządzanej przez inną firmę.  i komunikujecie się z tą serwerownią poprzez właśnie Interfejs graficzny dostępny w sieci.  Tak można by określić chmura w dużym skrócie.

Niemniej jednak jest to jakiś fizyczny server na którym ten model będzie działał.  na tych serwerach będą wdrożone również inne usługi,  które bardzo chętnie chciałyby skorzystać z waszych rozwiązań - bo chcecie żeby Wasz produkt opierał się o modele uczenia maszynowego. Generalnie te serwery komunikują się ze sobą po sieci, czyli nasz przysłowiowy “internet”. Teraz żeby ten nasz serwer mógł się komunikować z innymi serwerami to jest “wystawiony na świat”, czyli innymi słowy przypisywany jest mu IP który jednocześnie identyfikuje go w sieci i przy okazji też nadawany mu jest adres na przykład [www.mojadomena.zmodelami.pl](http://www.mojadomena.zmodelami.pl).  Czyli poprzez dostęp z internetu możemy  zkomunikować się z naszym serwerem, który właśnie dostępny jest pod tym adresem. I wdrożenie polega na tym, aby nasz model i nasz kod uruchomić na tym serverze, w taki sposób że możemy skomunikować się z naszą maszyną właśnie korzystając z internetu, z tego adresu który macie, w taki sposób aby móc wysłać dane do tego serwera, a server rzekazał te dane do naszego modelu, model wykonał predykcję i serwer odpowie nam przesyłając nam dane z predykcjami modelu. I w ten sposób z naszego modelu mogą skorzystać zarówno fizyczne osoby znające adres naszego servera jak i też inne serwisy/usługi które działają w naszej serverowni.

Ten widok jest na wdrożenie na server, natomiast spotkać się możecie również z wdrożeniem bezpośrednio na urządzenia mobilne. Sytuacja jest tutaj zgoła odmienna. My poruszać się w tej sferze nie będziemy. Nasza praca kończy się na konwersji modelu do odpowiedniego formatu. Ale w tym procesie bedziecie bardzo mocno współpracować z zespołem utrzymującym Waszą aplikację. Może też bedziecie musieli zakodzić coś w Swifcie albo Kotlinie, np. kawałek kodu odpowiedzialny za użycie modeli i jakiś post processing, ale wdrożenie na urządzenia jest to inna bajka. My będziemy skupiać się na tym z czym najczesciej sie spotkacie - czyli wdrożenie na servery. Ale jak spojrzycie sobie na ten diagram, to tutaj też jest telefon. Bo można zrobić to na dwa sposoby - albo model jest juz na telefonie i predykcje sa w real-time korzystajac z procka telefonu, albo nastepuje komunikacja z serverami, ktore wykonuja dla nas prace, a telefon otrzymuje tylko odpowiedź z nich. To wszystko zależy od tego jaki macie use case.

Na zajęciach z wdrożenia będziemy poruszać teraz zupełnie inną problematykę. Wcześniej dotykaliście zagadnień związanych z Data Scientistem - analiza danych,  budowa modeli, uczenie modeli. Na zajęciach z Programowania w pythonie i Implementacji systemów AI dotykaliście obszarów Software inżynieringu i ML Engineeroingu w zakresie pisania kodu w sposób obiektowy czy definiowania pipelinów MLowych i np. wpinania w nie MLFlowa który odpowiedzialny jest za trackowanie eksperymentów.

Teraz z kolei będziemy wchodzić w obszary związane z DevOps i Backend. Problematyka jest zupełnie inna i wyzwania są zupełnie inne. Wyzwania nie będą już typowo analityczne, tylko bardziej inżynierskie - czyli innymi słowy będziemy zastanaowiać się jak coś zaimplementować aby działało jak najszybciej, zajmowało jak najmniej miejsca,  wykorzystywało jak najmniej pamięci RAM-u, żeby infrastruktura kosztowała nas jak najmniej pieniędzy itd..

NIe będe Was oszukiwał - ta problematyka jest na tyle inna, że nie każdemu może się spodobać. Są osoby które wolą rzeczy związane właśnie z analizą, budową i uczeniem algorytmów. A są osoby które uwielbiają te rzeczy związane z infrastrukturą i wdrażaniem modeli. Niemniej jednak niezależnie od tego jaką drogą finalnie pójdziecie w swojej kariery zawodowej. To wiedza związana z tym jak wasze modele są wdrożone na produkcję, z jakimi problemami się mierzymy, jak je rozwiązujemy spowoduje to że praca w projektach, które wykorzystują uczenie maszynowe będzie dla was jeszcze łatwiejsza, bo będziecie lepiej rozumieć całą problematykę z tym związaną. Jeżeli ktoś z was pójdzie w Data Scientista i stwierdzi że tematy wdrożeniowe są nie dla niego, to gwarantuję wam że i tak i tak ta wiedza wam się przyda, dlatego że oprócz zadawania pytań biznesowych, analitycznych które pozwolą wam jeszcze lepszy model zbudować i jeszcze lepiej rozwiązać masz problem, to zadacie sobie pytanie też w jakich warunkach model będzie działał, bo to też ma wpływ na to jaki model ostatecznie użyjecie. Jeżeli usłyszycie zdanie że słuchajcie my nie mamy kart graficznych na produkcji to nie ma sensu używania modeli, które potrzebują GPU w momencie ich działania. Jeśli usłyszycie że mamy takie warunki że predykcje muszą być zwracane w 100 milisekundach, to może nie zbudujecie XGBoosta składającego się z 3000 drzew, tylko wybierzecie algorytm, którego złożoność obliczeniowa w momencie wykonywania predykcji jest znacznie mniejsza. Czyli innymi słowy, oprócz analizy algorytmów pod względem ich jakości, będziecie również analizować algorytm pod względem jego złożoności obliczeniowej. I drugi aspekt jest taki że będziecie w stanie lepiej komunikować się z waszymi kolegami zespołu którzy też odpowiedzialnie są za wdrożenie tych rozwiązań. I co najważniejsze, będzie lepiej pisać kod, bo najgorsze co może zrobić Data Scientist to przygotować spaghetti kod w notebooku, wysłać go do osoby wdrażającej i niech sobie chłop radzić sam, bo moja robota jest skończona. A jeżeli ktoś bierze ścieżkę zupełnie nie związaną z Data Science To wiedza o Wdrożeniu pozwoli wam po prostu lepiej pracować w takich projektach których te uczenie masz nowe będzie wykorzystywane. 

Dlatego nie będę owijał w bawełnę - tematy z wdrożeniem modeli są trudne, ale warto poświęcić na nie czas, uwagę i po prostu się postarać bo efekty będą z tego bardzo pozytywne.

To co mam teraz przestawiłem to jest taki generalny rzut na proces wdrożeniowy z samej góry, bez wnikania w szczegóły, bo dopiero co zaczynamy zajęcia z wdrożenia. To jest rzut z lotu ptaka i w ramach naszych zjazdów będziemy wnikać w szczegóły każdego z tego elementu.