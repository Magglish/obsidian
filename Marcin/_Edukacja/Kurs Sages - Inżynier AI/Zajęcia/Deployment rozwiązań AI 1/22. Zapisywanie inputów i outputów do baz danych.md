# Zapisywanie do bazy danych inputów i outputów

1. Zostaniemy jeszcze przy kwestiach bazodanowych, ponieważ w serwisach MLowych oprócz cachea, który jak widzicie znacznie może przyśpieszyć Wasze API istotne jest również przechowywanie informacji o tym jakie dane zostały wysłane do naszego API oraz jakie odpowiedzi nasze modele zwróciły. Dlaczego mamy zapisywać te dane? Odpowiedź jest bardzo prosta - jedną z najważniejszych rzeczy przy systemach opartych o ML jest monitoring ich jakości. A żeby monitorować nasz model pod względem jakości, potrzebujemy danych. Czyli w API, tymi danymi są po prostu requesty, które zawierają w swoim ciele inputy do modeli oraz response, który w swoim ciele zawiera predykcje z modelu. Dlatego istotne jest to aby każdy request i każdą odpowiedź z naszego API gdzieś zapisać. W ten sposób, w trakcie działania Waszego modelu na produkcji, gromadzicie dane, które później możecie porównać z Waszymi zbiorami testowymi, na których wykonywaliśmy ewaluację modelu i sprawdzić czy faktycznie jego jakość wciąż jest na poziomie takim, jaki była w momencie wdrożenia. Oprócz tego można te dane również porównać z danymi treningowymi i sprawdzić czy rozkłady cech, które wchodzą w skład modelu mają wciąż ten sam rozkład.  Natomiast bazą całego Waszego monitoringu modeli są - dane. Musimy je mieć i gromadzić.
2. W przypadku przechowywania danych z requestów i response, ten temat nie jest tak skomplikowany jak temat cacheowania. Więc tak na prawdę ten punkt omówimy szybko. To jaką bazę danych wybierzecie, w której chcielibyście przechowywać dane - to zależy od Waszych własnych preferencji, czy też tego gdzie Wasze API będzie stało - lokalnie na jakimś serverze czy może w chmurze. Z racji tego, że my pracujemy na razie lokalnie nad naszym API, ja po prostu zdecydowałem się na użycie Postgresa w tym celu. I takiego Postgresa możemy sobie lokalnie postawić używająć komendy:
```bash
make postgres
```
3. Tak jak wcześniej zrobiłem, tak też i tutaj - implementacja na połączenie się z Postgresem i interakcje z nim już stworzyłem - znajduje się w `src/databases/postgres`. Nie będę implementacji integracji z Postgresem omawiał, bo każdy z Was z bazami danych pracował już wcześniej i to co te implementacje robią to po prostu wyciągają z naszych requestów i response dane i zapisują do bazy danych. Wielkiej filozofii nie ma. Ale wspomnę tylko o jednej rzeczy, żeby nikogo nie przestraszyć - jeżeli ktoś z Was wejdzie w skrypty stawiające baze postgres albo w samą implementację Connectora który nawiązuje połączenie z bazą zobaczycie... zhardkodowane hasło do bazy danych, które jest ustawione na 12345. Zrobiłem to dlatego, że ćwiczymy i pracujemy lokalnie na naszych maszynach. Natomiast produkcyjnie tak nie powinno być. O przechowywaniu haseł i przekazywaniu ich do kodu będziemy mówić na zjeździe 3-cim poświęconym Kubernetesowi - wytłumacze Wam jak to się powinno robić.
4. Ok w takim razie dodajmy do naszej implementacji zapisywanie requestów i response do naszej bazy danych. 
   Zaimportujmy te rzeczy:
```python
from src.databases.postgres.connection.connector import PostgresConnector
from src.databases.postgres.clients.decision import DecisionPostgresClient
from src.databases.postgres.clients.probability import ProbabilityPostgresClient
from src.databases.postgres.clients.risk_category import RiskCategoryPostgresClient
```

Zainicjujmy obiekty

```python
postgres_connector = PostgresConnector()  
decision_postgres_client = DecisionPostgresClient(postgres_connector)  
probability_postgres_client = ProbabilityPostgresClient(postgres_connector)  
risk_category_postgres_client = RiskCategoryPostgresClient(postgres_connector)
```

I dodajmy je do naszego endpointu. Musimy dodać to w dwóch miejscach:

```python
@app.post("/decisions")
async def decisions(request: DecisionRequest) -> DecisionResponse:
    response = decision_redis_client.read(request)
    if response is not None:
        decision_postgres_client.write(request, response)
        return response

    decision = model.predict_decision(request.to_dataframe())[0]
    response = DecisionResponse(decision=decision)
    decision_redis_client.write(request, response)
    decision_postgres_client.write(request, response)
    return response
```

Sprawdźmy czy działa

Włączmy API

```bash
make api
```

Wykonajmy kilka requestów do endpointa `decisions`

```bash
make request
```

Połączmy się z naszą bazą danych

```bash
psql --dbname=monitoring --user=postgres --host=127.0.0.1 --port=5432
```

Hasło to `12345`.

Mamy nasze tabelki:

```bash
\dt
```

Puśćmy selecta i zobaczmy czy sa zapisane dane:

```sql
SELECT * FROM decisions;
```

Mamy dane. Zatem będziemy mieli na czym budować monitoring modelu w przyszłości..

Ok i to w sumie kończy temat zapisywania requestów i response do bazy danych - jak mówiłem pójdzie szybko, bo temat jest banalny. Ale niestety są pewne wyzwania z tym związane. Ale zanim przejdziemy, to czy są jakieś pytania do tej części?