# Testy jednostkowe

Obejrzyj: https://www.youtube.com/watch?v=vlLSgdQNgMI

Przechodzimy sobie do kolejnego tematu jakim jest testowanie poprawności naszego API. W przypadku testowania będziemy mówić o dwóch wymiarach: przede wszystkim w wymiarze jej implementacyjnym, czyli Innymi słowy czy nasze API zwraca to czego się po nim spodziewamy oraz też w wymiarze obciążeniowym to znaczy będziemy sprawdzać jak szybko nasze API jest w stanie zwrócić odpowiedzi pod różnymi wariantami obciążenia, jak API działa kiedy jest wysłane tylko jeden request na sekundę, kiedy wysłanych jest 5 requestów na sekundą itd. i to jest bardzo ważny etap który warto  sprawdzić żeby mieć ogólny pogląd na to jak nasze API będzie zachowywał się pod obciążeniem na produkcji.

Testowanie kodu było już omawiane na poprzednich zajęciach z programowania w Pythonie jeden i dwa. Ja w tym module będę oczywiście nawiązywał do pewnych pewnych pojęć ,które wtedy poznaliście ale osadzając jej całkowicie w kontekście API. 
 
Od razu wam powiem że pisanie testów API moim zdaniem jest trudniejsze niż otestowywanie implementacji naszych kodów. Jest to powiązane z tym że w API dzieje się dosyć sporo. Jak sobie spojrzymy w `main.py` to to widzicie, że nasze requesty są odpowiednio przekształcane na pydanticowy obiekt, tutaj mamy sprawdzenie czy jest odpowiedź w cache'y, potem wykonanie predykcji modelu, potem zapisanie tych danych do bazy. Zatem w ramach jednego endpointu dzieje się dosyć sporo. W dodatku każde API ma dużo oczywiście elementów wspólnych, czy to będzie API związane z tak tutaj w tym przypadku modelami oceniającymi ryzyko kredytowe, ale też są elementy wspólne w przypadku API do modelu tekstowym jak się przekonaliśmy na przykładzie swoich ćwiczenia. Jednakże wciąż są pewne różnice i generalnie nawet jak się pisało sporo testów do różnych API to i tak mogą być one momentami trudne, nawet dla bardziej doświadczonych osób. Niestety też pisanie niektórych testów jest dosyć czasochłonne , dlatego tym bardziej jeśli ma się z tym kontakt po raz pierwszy, bo trzeba zwrócić uwagę na bardzo wiele rzeczy. Intuicja co testować dopiero przyjdzie po napisaniu wielu, wielu testów. I też fackupy na produkcji uczą na co zwracać uwagę.  

Dodatkowo trudnością w testowaniu API z zaimplementowanym modelem jest fakt że nasz model może być niedeterministyczny, w szczególności w przypadku kiedy będziemy implementować API dla modeli tekstowych generatywnych które mogą zwrócić dosłownie dowolny tekst. Nawet jeżeli pracujemy z bardziej klasycznymi algorytmami w klasycznym problemie jak na przykład regresja logistyczna, która za każdym razem zwróci nam ten sam wynik jeżeli podamy mu te same parametry, to problemem drugim jest to taki że nasz model może się zmienić. Może to być i nawet wciąż regresja logistyczna ale już z innymi parametrami. Czy zmiana algorytmu na drzewa decyzyjne, czy inny algorytm spowoduje wtedy że dla danego wejścia zmieni się wynik, dlatego też takie testy które porównują dwie wartości względem siebie 1:1 po prostu w przypadku wyników zwracanych przez będą nieużyteczne, dlatego też do testowania API który ma w sobie model uczenia maszynowego trzeba podejść w pewien specyficzny sposób.

Ja w tym module poświęconym testom chcę przekazać wam taką najbardziej generalną wiedzę odnośnie to jak testować API. Chodzi mi o to że nie chciałbym nauczyć was testowania konkretnej implementacji tego API który jest API do modelu scrongiwoego jak w tym przykładzi tylko chcę się skupić na takich generalnych zasadach czy nawet sposobie myślenia w przypadku pisania testów.  

Domyślam się że na poprzednich zjazdach mieliście powiedziane to jakie są rodzaje testów Ja tylko tak o nich wspomnę w punktach po to żeby nakreślić wam kontest co dokładnie będziemy robić teraz.

Generalnie można wyróżnić 5 takich grup. Ale powiem Wam szczerze, że jak przejrzycie sobie internet, aby dowiedzieć się jakie są rodzaje testów, to jeden podaje 5, inny 7, inny 10 a jeszcze inny 18. Mi się podoba podział na 5 grup, bo wchodzenie w większą ilość grup powoduje, że granice pomiędzy nimi już są tak zatarte, że bardzo trudno jest je rozróżnić.

1. testy jednostkowe - Testy jednostkowe czyli test badający pojedyncze funkcjonalności w danej implementacji API
2. testy integracyjne - Testy integracyjne z kolei testują znacznie więcej niż testy jednostkowe i tutaj mówimy już o interakcji z innymi serwisami. Jeśli taka interakcja następuje w naszym kontekście są to bazy danych Redis oraz postgres
3. testy funkcjonalne - Testy funkcjonalne lub inaczej nazywane akceptacyjne lub kompleksowe. Dotyczą już pewnych aspektów biznesowych działania naszego API. Dla przykładu w naszym kontekście modelu scoringowego mógłby to być fakt że dana predykcja modelu może być na przykład wykorzystywana tylko przez 7 dni. Czyli Innymi słowy gdyby klient chodził i pytał o kredyt w różnych miejscach to poprzez okres 7 dni używalibyśmy tej samej decyzji którą na początku otrzymaliśmy, niezależnie od tego jak zmieniły by się dane na jego temat. Ae na przykład po 7 dniach chcielibyśmy żeby nasze API zwróciło nową decyzję ale już w oparciu o nowe dane, które mogły się zmienić na przestrzeni tych 7 dni (czyli nie odczytywalibyśmy teraz predykcji z cache'a a po prostu kazalibyśmy wygenerować ją na nowo z modelu) może wtedy temu klientowi uda się dostać kredyt. Także te testy funkcjonalne/kompleksowe/akceptacyjne są bardzo mocno powiązane z warunkami biznesowymi w jakich wasze API będzie implementowane. My w naszym przypadku takich wymagań biznesowych nie mamy oczywiście ,ale od strony implementacyjnej one nie różnią się specjalnie od testów jednostkowych czy integracyjnych, więc spokojnie sobie z nimi poradzicie.
4. testy wydajności - Kolejną grupą są oczywiście testy wydajności Czyli to co będziemy dzisiaj robić w dalszej części czyli sprawdzanie tego jak nasze API działa pod różnym obciążeniem.
5. testy smoke - No i na końcu testy smoke, które sprawdzają czy nasze API w ogóle działa i jest w stanie cokolwiek zrobić z testami. Dzisiaj się nie będziemy zajmować tylko dopiero na zjeździe następnym kiedy poznamy czym są kontenery bo smoke testy najlepiej wykonuje się właśnie już na kontenerach.

My teraz skupimy się na testach jednostkowych, testach integracyjnych oraz na testach wydajności. Testami smoke zajmiemy sie na kolejnym zjeździe, i dodatkowo wrócimy też do testów integracyjnych jak poznamy docker compose. 

W naszych testach oczywiście będziemy docelowo korzystać z biblioteki `pytest`, która była omawiana na poprzednich zjazdach.  Pewne rzeczy będę oczywiście przypominał ale nie będę aż tak bardzo wchodzić w szczegóły. Mam nadzieję że pamiętacie to co było na poprzednich zjazdach jeżeli nie no to śmiało pytajcie ja wtedy dopowiem o kilka zdań więcej na temat tego co implementujemy. 

Zanim zaczniemy pisać pierwsze testy Chciałbym żebyście poznali moje zdanie Dlaczego warto pisać testy.  możliwe że takie zalety testowania już były omawiane na poprzednich zajęciach ale chciałbym żebyś się też poznali mój punkt widzenia:

1. oczywistą oczywistością jest to że testy walidują logikę działania naszego API bez dwóch zdań natomiast to nie jest moim zdaniem  jedyny powód dla którego warto pisać testy
2. Drugi powód jest taki że testy nie jako stanowią też dokumentację waszego kodu. W momencie kiedy ja czytam implementację która jest może być dosyć skomplikowana napisana przez inną osobę to lubię też spojrzeć na to jak są napisane testy, dlatego że testy mówią mi o tym co zostało wpuszczone do funkcji, jaki jest oczekiwany rezultat i mając te dwie informacje znacznie łatwiej jest mi analizować implementację bo wiem co wchodzi i czego się spodziewać
3. trzeci powód jest taki że podczas pisania testów wy tak naprawdę jeszcze raz analizujecie to dokładnie napisaliście i mogą wyjść wszelkie błędy, czy to w implementacji czy właśnie w logice działania kodu. Dodatkowo jeżeli jest problem z napisaniem testu to jest to potencjalny sygnał że wasza implementacja jest po prostu zbyt skomplikowana i należy ją zrefaktorować na przykład rozdzielić funkcje na mniejsze części wydzielając pewną logikę działania do oddzielnych i przetestować te pojedyncze rzeczy w oddzielnych testach. Generalnie ja widzę po sobie że im więcej pisze testów ten kod jest mój lepszy, łatwiejszy i krótszy.
4. czwarty Powód jest taki że bez testów no niemożliwe jest praktycznie refactoring waszych kodu.  Jeżeli są potrzebne zmiany w API to wprowadzanie zmian bez dobrego testowania to powiem szczerze jest ryzykowne bo nie macie gwarancji tego że po waszych zmianach wciąż ona działa tak jak ma działać. Dlatego moim zdaniem tak bardzo ważne jest to żeby te testy były nawet w kontekście prostego POCa który zrealizowany jest po jakimś czasie. Teesty uchronią was przed dużą ilością bugów które mogą powstać w momencie kiedy już to zostanie wdrożone. Lepiej te bugi wykryć na etapie developmentu a nie potem stresować się bo dane serwisy na produkcję przestają działać, a są wykorzystywane bo na przykład są już odpytywane przez jakiś system na produkcji..   

Generalnie nie wiem czy to zauważacie ale na rynku istnieje już coraz większa tendencja do tego żeby wymagać od przyszłego Data Scientista i przede wszystkim Machine Learning Engineera znajomości programowania obietkwoego, wzorców obiektowych czy też właśnie umiejętności pisania testów jednostkowych.  Dlatego że tak mówiłem wcześniej machine learning to nic innego jak software engineering. Jest inny w pewnych aspektach, bo bardziej pracujemy z danymi i modelami, ale wciąż programujemy, więc wszystkie zasady i dobre praktyki nas obowiązują i nasze rozwiązania są wdrażane na produkcję, na taką na jakiej działają inne systemy i serwisy. My sami osobiście jak rekrutowaliśmy Data Scientistów do swojego zespołu jakiś czas temu to też zwracaliśmy uwagę na to jak piszą kod i to też jest jedno z kryteriów decyzyjne przy wyborze kandydatów.

Okej w takim razie myślę że możemy zacząć już po tym wstępie pisać pierwsze testy. To co chciałbym żebyście stworzyli sobie folder `test` tutaj w głównym folderze naszego repozytorium i potem trzy podfoldery odpowiednio `unit`, `integration` i `stress`. Pamiętajmy o tym żeby były w nich `__init__.py`. Stwórzmy też plik `fixtures.py` w `tests` oraz w `unit` i `integration`.

Teraz Zanim zacząłem pisać pierwsze testy chciałbym żebyśmy weszli do opcji w Pycharmie: Settings -> Tools -> Python Integrated Tools -> Default test runner ustawić jako Pytest. Następnie znajdźmy Run configuration (Alt + Shift + F10). Edit Configurations i usuńmy wszystkie konfiguracje dla Python tests jeśli występują. 

Teraz pytanie jakie możemy sobie zadać to co będziemy testować. Jeżeli spojrzymy sobie teraz na `main.py`, w którym mamy zdefiniowane naszą logikę działania API to de facto odpowiedź brzmi - wszystko. 
1. czy faktycznie exception handlery poprawnie wyłapują błędy 
2. czy Eventy startup Shutdown poprawnie inicjalizują obiekty a shutdown odpowiednio zamyka połączenia z bazami danych
3. No i oczywiście działanie naszego enpointa decisions Czy poprawnie są czytane dane z bazy Redis 
4. czy poprawnie model wykonuje predykcję 
5. czy poprawnie wszelkie background taski zapisują dane i tak dalej i tak dalej 

także jest trochę rzeczy do przetestowania. Te API jest proste, tutaj mało funkcjonalności ale jak sami zobaczycie są pewne wyzwania w testowaniu API. Pewne działania naszego API będzie testowane w ramach testów jednostkowych a część rzeczy w ramach testów integracyjnych to wszystko się wyjaśni w trakcie będziemy pisać już te testy.   

Zacznijmy od zdefiniowana zdefiniowania sobie struktur danych które będziemy używać naszych testach. Przejdźmy sobie do `fixtures.py` i tam będziemy definiować struktury danych używane właśnie w testach. Na początek zaimportujmy sobie wszystkie  wszystkie obiekty które będą nam potrzebne 

```python
import pytest
```

Zaczniemy od tego że zdefiniujemy sobie body naszego requesta, jego nagłówki oraz URL naszego endpointa:


```python
@pytest.fixture(scope="session")
def request_body():
   return {
       "installment_rate_in_percentage_of_disposable_income": 0.25,
       "age_in_years": 40,
       "foreign_worker": "yes",
       "present_employment_since": "unemployed",
       "personal_status_and_sex": "male: single",
   } 

@pytest.fixture(scope="session")
def request_headers():
   return {
       "Content-Type": "application/json",
   }
  
@pytest.fixture(scope="session")
def request_endpoint():
   return "/decisions"

```

Krótkie przypomnienie pytestowe `fixtures` to nic innego jak po prostu struktury danych które można używać w naszych testach.  ten `scope` który mówi o sesji to po prostu oznacza że w momencie rozpoczyna testów te obiekty zostaną utworzone i trzymane przez całą sesję testowania. 

Teraz przejdźmy do `fixtures.py` w folderze `unit` i stwórzmy nasz obiekt z API:
```python
@pytest.fixture(scope="package")
def initialized_app():
   with TestClient(app=app, raise_server_exceptions=False) as client:
       yield client
```

Nasze API posiada w implementacji funkcje `startup` oraz `shutdown`. Żeby można było uruchomić te funkcje to musimy uruchomić naszą nasz obiekt API jako context manager Dlatego piszemy tutaj `with` oraz Fast API dostarcza nam taką klasę jak TestClient, która służy właśnie do uruchomienia takiej testowej wersji naszego API. 

Teraz bardzo ważna rzecz o której musicie pamiętać. Musimy tutaj użyć `yield` a nie `return`. Nie wiem czy pamiętacie jak działają context managery, które powstają właśnie z użyciem klauzuli `with` ale krótko mówiąc działa to tak  że jeżeli dana klasa ma zaimplementowane takie Magic metody jak `__enter__` i `__exit__` to klauzula `with` właśnie używa tych metod w momencie kiedy ten mój `with` zaczyna działać i w momencie kiedy kończy się jego działanie. W tym kontekście działanie `with` będzie następujące: rozpoczęcie tego kodu z TestClient po prostu uruchamia naszą funkcję `startup`, a w momencie kiedy kod w klauzuli `with` wykonał się cały to wtedy zostanie uruchomiona funkcja `shutdown`. I żeby używać API w kontekście naszych testów musimy tutaj użyć `TestClient` który nam dostarcza Fast API właśnie po to żeby testy tworzyć. 

Druga ważna rzecz to to żeby w takiej sytuacji użyć `yield` a nie `return`. Dlaczego `yield`? w przypadku testów pisanych w `pytest`, `yield` działa tak że ta funkcja `initialized_app`  zwróci nam zainicjowany testowe klient, następnie zostaną wykonane testy jednostkowe które zdefiniujemy, a po tych wszystkich testach kod wróci do tego miejsca, będzie dalej się wykonywał. Ale jak widzimy dalej już nic nie ma, czyli tak naprawdę zakończy się działanie `with` i wtedy zostanie uruchomiona funkcja `shutdown`. Czy to jest w miarę jasne dla Was? Jeśli nie to jak napiszemy pierwszy test to zobaczycie jak to działa i wtedy jeszcze raz sobie do tego wrócimy już na pełnym przykładzie i wtedy myślę że się bardziej rozjaśni.  Gdybyśmy tutaj użyli `rutern` z kolei to niestety ale zadziałałby to tak że klient uruchomił  by funkcje `startup`  i od razu zaraz po nim  `shutdown` script, w efekcie czego nasze nasze połączenia z bazami danych od razu byłyby zamknięte i np. testowanie interakcji z bazą danych byłoby niemożliwe.

Mamy zdefiniowane nasze struktury danych testowe które będziemy używać w trakcie pisania testów. Teraz przejdźmy sobie do foldery z testami jednostkowymi czyli `unit` i spróbujmy napisać test na pierwszy przypadek który chcemy sprawdzić. Stwórzmy sobie plik pythonowy `test_decisions.py`. Importujmy obiekty które będą nam potrzebne 

```python
import pytest
from tests.fixtures import request_body, request_headers, request_endpoint
from tests.unit.fixtures import initialized_app
from src.databases.redis.clients.base import BaseRedisClient
from src.databases.postgres.clients.base import BasePostgresClient
from src.models.credit_score import CreditScoringModel
from src.utils.errors import APIError
import collections.abc
```

Pierwszy test jaki napiszemy to sprawdzimy czy faktycznie nasze API zwraca  poprawną decyzję w sytuacji kiedy przekażemy do niej poprawne dane, a API je poprawnie przetworzy.

```python
def test_decision_returned_correctly(initialized_app, request_body, request_headers, request_endpoint, mocker):
    pass

```

Ale chciałbym żebyśmy na razie zdefiniowali ten test jako pusty całkowicie i spróbowali to uruchomić. **Upewnijmy się że nie ma żadnych baz danych włączonych Jeśli są to musi mnie ubić docker kill.**

Teraz może krótkie wyjaśnienie odnośnie argumentów które podajemy niestety minusem `pytest` jest to że nie do końca te argumenty mogą być dla was zrozumiałe. Generalnie jest tak że `pytest` przechowuje w sobie pewne obiekty, które można przekazać do testów, które tworzycie i żeby `pytest` je przekazał w momencie uruchamiania testów to trzeba je właśnie zdefiniować w argumentach tej funkcji. Czyli tak jak tutaj widzicie - ja w tym teście dodałem nasze `fixtures` które przed chwilą zdefiniowaliśmy i to oznacza że w momencie uruchomienia naszych testów te obiekty będą dostępne w środku tego testu i możemy odpowiednio na nich działać tak żeby nasz test stworzyć. Dodałem też na końcu taki obiekt `mocker` który zaraz nam się przyda. Ten obiekt `mocker` jest właśnie obiektem który dostarcza nam `mocker`. Teraz pytanie skąd wiedzieć w ogóle co jest nam dostarczy?Możemy to sprawdzić Korzystając z terminala:

```bash
pytest –fixtures
```

Jak sobie spojrzymy na to co ta komenda zwróciła bo to widzimy że mamy tutaj różne obiekty i mamy też informacje o naszych obiektach które my sobie zdefiniowaliśmy i to są obiekty które możemy wykorzystać w naszych testach pod warunkiem że umieścimy je w naszych argumentach. 

Okej to teraz włączmy sobie nasz test pierwszy i zobaczymy co się wydarzy:

```bash
pytest tests -v
```

No jak widzicie dostaliśmy błąd. Baza Redis nie istnieje. Tak samo będzie w przypadku bazy Postgres - nie istnieje. My chcemy napisać pierwsze testy jednostkowe które będą skupiać się na podstawowych funkcjonalnościach naszego API, ale testy jednostkowe charakteryzują się tym że one nie testują interakcji z innymi usługami jeżeli one są wykorzystywane w naszym API. Takie testy które wykorzystują inne usługi, czyli w tym przypadku bazy danych, to testy integracyjne i one są troszeczkę bardziej skomplikowane - do nich przyjdziemy później. Teraz pytanie powstaje jak stworzyć testy jednostkowe dla API które korzysta z innych serwisów. W naszym przypadku sprawa będzie dosyć prosta bo to są tylko i wyłącznie bazy danych, ale rzeczywistości API może wyglądać tak że będzie jeszcze korzystało z innych serwisów wewnętrznych firmy, ale o tym powiem wiecej przy testach integracyjnych.

Są trzy podejścia w tej sytuacji. 

1. Mockowanie - Mocowanie to nic innego jak po prostu symulowanie działania danej implementacji w taki sposób w jaki chcemy - czyli w naszym kontekście moglibyśmy zasymulować to, że połączenie z bazą danych się powiodło
2. Stubs -  STubs zakłada że stworzymy sobie sztuczną sztuczną implementację danego obiektu i tej sztucznej implementacji będziemy używać zamiast tego docelowego obiektu - czyli moglibyśmy zdefiniować swój własny PostgresConnector oraz klientów, którzy zapisywali by dane i odczytywali by dane do sztucznej bazy danych.
3. Wykorzystanie kontenerów do testów - z kolei można też wykorzystać kontenery, o których dowiecie sie dopiero na drugim zjeździe. To już jest bardziej zaawansowana rzecz bo tutaj zakładamy że tak naprawdę serwisy które wchodzą w interakcje z naszym API są deployowane np. lokalnie i używane w ramach naszych testów. Pomysł wygląda atrakcyjnie są biblioteki do tego stworzone, natomiast to nie jest takie proste jak się wydaje o tym powiem więcej na następnym zjeździe. 

My w ramach testów jednostkowych poznamy teraz technikę Mockowania. W przypadku testów integracyjnych poznamy czym są stubsy i pokaże wam taki przedsmak wykorzystania kontenerów do testów jednostkowych, ale dokładnie temu zagadnieniu przyjrzymy się na drugim zjeździe.

Okej wróćmy do naszego błędu. Zobaczcie, nie możemy połączyć się z naszą bazą danych Redis. Mocowanie jest to symulowanie pewnego zachowania więc to co my tu zrobimy to po prostu zasymulujemy że jednak z tą bazą się połączyliśmy. 

Aby to osiągnąć do naszej `initialized_app.py` w skrypcie `fixtures.py`  musimy dodać argument `package_mocker`. W zależności od tego jaki scope będzie waszego `fixtures` to to taki `mocker` trzeba będzie wrzucić do funkcji. U nas scope jest package więc używam `package_mocker`. Gdyby scope był inny to wtedy mamy inne `session_mocker`, `module_mocker` itd. 

```python

@pytest.fixture(scope="package")
def initialized_app(package_mocker):
   with TestClient(app=app, raise_server_exceptions=False) as client:
       yield client
```

Teraz co chcemy osiągnąć to jest zasymulować połączenie z bazą danych. Musimy spojrzeć w takim razie jak ten `RedisConnector` jest zaimplementowany i w którym momencie on te połączenie z bazą danych tworzy. Zobaczmy w takim razie jego implementację. W momencie inijalizacji jest uruchomiona funkcja `_create_connection`. Zobaczmy jej działanie. Czyli tak naprawdę jej działanie jest to żeby stworzyć połączenie z bazą danych i wysłać pinga czyli sygnał do bazy aby sprawdzić czy faktycznie została uruchomiona. I to jest ten kawałek kodu który spowodował wywołanie naszego błędu który tutaj widzicie w konsoli. Jak to można zmockować? To co my chcemy zrobić to oszukać działanie funkcji `_create_connection` tak, żeby w ogóle nie rzuciła tym błędem. To pokażę wam od razu jak można to zrobić i potem wytłumaczę co się zadziało:


```python
@pytest.fixture(scope="package")
def initialized_app(package_mocker):
   package_mocker.patch.object(RedisConnector, "_create_connection", return_value=None)
   with TestClient(app=app, raise_server_exceptions=False) as client:
       yield client
```

Okej spójrzmy co my tu mamy. To co ja zrobiłem to skorzystałem z takich metod jak `patrz.object` z tego `package.mocker` który właśnie pozwoli mi symulować te działanie. Dla klasy `RedisConnector` który właśnie jest implementacją połączenia z naszym redisem wskazałem na funkcje `_create_connection` i określiłem jaka wartość ma zostać zwrócona - w tym przypadku `None`. Co to oznacza? Jeżeli nasz kod uruchomi funkcję `_create_connection` z klasy `RedisConnector` to zamiast wywołać całe jej działanie tak naprawdę od razu zwrócona zostanie wartość None z funkcji. Żeby wam łatwiej to zwizualizować wróćmy do `RedisConnector`. Ten `package_mocker` działa tak jakbyśmy na samym początku tej funkcji dodali `return None`. Czyli dalsza część funkcji nie zostanie wykonana. Dlaczego tak zrobiłem? Dlatego że po prostu my w testach jednostkowych  będziemy skupiać się tylko i wyłącznie na podstawowej funkcjonalności naszego API a wszelkie takie dodatkowe interakcje będą testowane w ramach testów integracyjnych. Dlatego w testach jednostkowych będziemy symulować połączenie, ale w taki sposób jakby go w ogóle nie było. Oprócz utworzenia połączenia musimy też sobie zmockować jeszcze zamknięcie tego połączenia, bo tam też jest taka metoda i ona z kolei też rzuci błędem w momencie kiedy zakończymy testy, więc od razu zróbmy to sobie. 


```python
@pytest.fixture(scope="package")
def initialized_app(package_mocker):
   package_mocker.patch.object(RedisConnector, "_create_connection", return_value=None)
   package_mocker.patch.object(RedisConnector, "close", return_value=None)
   with TestClient(app=app, raise_server_exceptions=False) as client:
       yield client
```

Okej teraz spróbuj uruchomić nasze testy i zobaczmy jaką mamy teraz. Jak widzicie udało się tak. Naprawdę to co nam teraz pokazuje konsola to to że nie mam połączenia z Postgresem, więc tutaj analogicznie robimy tą samą sytuację.

```python
@pytest.fixture(scope="package")
def initialized_app(package_mocker):
   package_mocker.patch.object(RedisConnector, "_create_connection", return_value=None)
   package_mocker.patch.object(RedisConnector, "close", return_value=None)
   package_mocker.patch.object(PostgresConnector, "_create_connection", return_value=None)
   package_mocker.patch.object(PostgresConnector, "_create_cursor", return_value=None)
   package_mocker.patch.object(PostgresConnector, "close", return_value=None)
   with TestClient(app=app, raise_server_exceptions=False) as client:
       yield client
```

W Postgresie są trzy w sumie funkcje - tworzenie połączenia, tworzenie kursora i funkcja zamykające połączenie - więc od razu sobie to wszystko tutaj zmokujmy. No i co zobaczymy czy działa. 

Jak widać udało się teraz już nie mamy żadnych błędów w momencie inicjalizacji naszego obiektu więc sprawa związana z połączeniem baz danych została rozwiązana . Słuchajcie jeżeli macie jakieś teraz pytania do tego albo coś niezrozumiałe to poczekajcie jeszcze chwilę z tym jak napiszemy cały test i bo tam jeszcze będzie też kilka rzeczy, które trzeba będzie zmockować i zobaczyć je sobie od początku do końca co się dzieje to wtedy będziemy dyskutować na temat tego co było niejasne A co nie I wtedy jeszcze raz to wytłumaczę.

Okej to teraz przejdźmy sobie do naszego testu. Teraz możemy w końcu go zacząć pisać. Spróbujmy na początek wysłać requesta do naszego API testowego I zobaczmy co się stanie: 

```python
def test_decision_returned_correctly(initialized_app, request_body, request_headers, request_endpoint, mocker):
   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)
```

Niestety mamy błąd jak sobie spojrzymy teraz na tą treść błędu która jest dość obszerna To zobaczymy że mamy problem taki że `None` nie ma metody `ping`. To wynika z tego że podczas inicjalizacji naszego testowego klienta tutaj w `fixtures` Zdefiniowaliśmy że po prostu w momencie wyłania tych funkcji są wrzucane None i teraz podczas działania naszego API na tych `None` wykonywane są po prostu wywołania konkretnych dalszych metod. Czyli Innymi słowy jak przejdziemy sobie do `main.py` i do endpointa `decisions` to w momencie czytania z redisa jest coś takiego jak `response = None.read(request)` Oczywiście to nie ma prawa się udać - i tak oto dostajemy treść błędu. To co musimy zrobić to zrobić podobną operację tak jak już zrobiliśmy wcześniej, czyli zmockować pewne funkcjonalności naszych klientów czytających dane z redisa i postgresa. Czyli to co musimy zrobić to po prostu zasymulować działanie metod read i write w naszych implementacjach. 

```python
def test_decision_returned_correctly(initialized_app, request_body, request_headers, request_endpoint, mocker):  
    mocker.patch.object(BaseRedisClient, "read", return_value=None)  
    mocker.patch.object(BaseRedisClient, "write", return_value=None)  
    mocker.patch.object(BasePostgresClient, "write", return_value=None)  
    response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)
```

No i włączmy nasz test I zobaczmy co się stało. Jak widzicie udało się teraz ten test przeszedł czyli tak naprawdę ten `reponse` został zwrócony i możemy zacząć sprawdzać jego zawartość i upewniać się że otrzymaliśmy poprawne dane takie jakie się spodziewamy. 

Dobra to teraz mamy już  najważniejszą logikę zaimplementowaną w naszym teście - dostaliśmy odpowiedź z naszego API i teraz możemy wykonywać sprawdzenie, ale zanim to zrobimy to teraz przejdźmy sobie jeszcze raz co się zadziało i dlaczego mockujemy to co teraz mockujemy.  

Wróćmy sobie do naszego `main.py` I zobaczmy na to jak zdefiniowany jest endpoint decisions. My w ramach testów jednostkowych chcemy sprawdzić podstawowe funkcjonalności naszego API. Jak sobie spojrzymy teraz na to co stworzyliśmy w tym endpoint to tak naprawdę co jest podstawową funkcjonalnością naszego API? Tylko tak naprawdę dwie linijki kodu. 

```python
@app.post("/decisions")
async def decisions(
   request: DecisionRequest,
   background_tasks: BackgroundTasks,
) -> DecisionResponse:
   # logging.info(f"Received {request=}")
   # response = app.state.decision_redis_client.read(request)
   # if response is None:
   decision = app.state.model.predict_decision(request.to_dataframe())[0]
   response = DecisionResponse(decision=decision)
       # background_tasks.add_task(
       #     func=write_to_redis,
       #     redis_client=app.state.decision_redis_client,
       #     request=request,
       #     response=response,
       # )
   #
   # background_tasks.add_task(
   #     func=write_to_postgres,
   #     postgres_client=app.state.decision_postgres_client,
   #     request=request,
   #     response=response,
   # )
   # logging.info(f"Returning {response=}")
   return response
```

Podstawową funkcjonalnością w API jest to że otrzymujemy dane, nasz model wykonuje predykcje i je po prostu zwracamy -  to jest core działania naszego API. A to że dodaliśmy do niego czytanie z Redisa, zapisywanie do Redisa, zapisywanie do Postgresa  To są dodatkowe funkcjonalności, które dodatkowo przyspieszają działanie naszego API lub właśnie zapisują dane które później będziemy analizować. Ale to nie jest `core` działania naszego API.  Główną funkcjonalności to jest po prostu wykonanie predykcji dla podanych danych i zwrócenie ich w odpowiedzi. I my właśnie w ramach testów jednostkowych chcemy sprawdzić te podstawową funkcjonalności naszego API. Dlatego też te wszelkie dodatkowe funkcjonalności które tutaj są zakomendowane w tej chwili - my nie chcemy żeby one brały udział w tych testach jednostkowych. One będą testowane  w testach integracyjnych. Dlaczego też te mokowanie które dodaliśmy powoduje to że te dodatkowe funkcjonalności są jakby wyłączane i jakby "nie działają". Czyli zmockowaliśmy sobie połączenia z bazą Redis, i Postgres bo nie jest w ogóle ona wykorzystywana oraz zlokowaliśmy sobie metody które powodują interakcje z tymi bazami danych czyli `read` i `write` bo też nie są w tej podstawowej funkcjonalności wykorzystywane. Zobaczcie że nie zmonkowaliśmy modelu to był model to jest `core` funkcjonalność naszego API on był potrzebny po to żeby ten tą odpowiedź zwrócić. 

Odkomentujmy to wszystko tak żeby Apis z powrotem działało poprawnie jak chcieliśmy. Teraz pytanie do was czy to tutaj stworzyliśmy jest dla was jasne Czy macie jakieś pytania?

To teraz wróćmy sobie do tego kodu z `with` i `yield`, żebyście mieli pewność jak to dokładnie działa. My nasze fixture `initialized_app`  użyliśmy w naszym  teście i to działa tak że w momencie kiedy uruchomione są testy, `pytest` sprawdza jakie fixtures są przekazywane do testu i je tworzy. Czyli uruchamiany jest kod związany z `initialized_app`, w którym jest  wykonywane mockowanie i tworzony ten testowy obiekt `client`  który zwracany jest z klauzulą `yield`. Dzięki temu ten kod w tym `fixture` jest niejako by wstrzymywany, dalej wykonywany jest kod związany z testem czyli nasze `test_decision_returned_correctly`. W momencie zakończenia testów,  wykonywanie z powrotem wraca do naszego `fixture` do `initialized_app` i gdyby coś było po `yield` to ten kod zostałby dalej wykonany. W naszym przypadku nic nie ma więc tak naprawdę oznacza to koniec działania klauzuli `with`, w efekcie czego nasze API kończy swoje działanie uruchamiając funkcje `shutdown`. Dla łatwiejszego zrozumienia można zobaczyć przykład w dokumentacji pytest z usługą wysyłająca maile: [https://docs.pytest.org/en/6.2.x/fixture.html#yield-fixtures-recommended](https://docs.pytest.org/en/6.2.x/fixture.html#yield-fixtures-recommended)

Okej to teraz wróćmy naszego testu I zobaczmy jaki response dostaliśmy. W tym celu dodajmy sobie `breakpoint` do naszego kodu i włączmy nasz test w trybie debuggera, ale zanim to zrobimy to musimy ustawić konfigurację tego testu, czyli klikamy sobie po lewej stronie w ten nasz zielony trójkąt i wybieramy Modify run configuration i w working directory musimy wskazać na ścieżkę naszego projektu, ale bez tych folderów testowych. Klikamy znowu ten zielony trójkącik i uruchamiamy to w trybie debugera. Okej jak widzicie  powinno się to zatrzymać właśnie w breakpoincie. 

Okej to teraz to co musimy zrobić to sprawdzić czy odpowiedź zawiera te dane w takiej strukturze jakiej się spodziewamy otrzymać. No i w tym wypadku musimy posiłkować się znajomością naszej implementacji oraz spojrzeć na to co ona zwraca ale drugą ważną rzeczą jest też sprawdzenie dokumentacji naszego API. Niestety nie mamy jak spojrzę teraz na dokumentację naszego API ponieważ testowy klient stworzony podczas testów nie udostępniania dokumentacji, więc musimy sobie troszkę przypomnieć jak to nasza odpowiedź z API wyglądała. Zresztą kilka razy ją widzieliśmy w konsoli więc myślę że mniej więcej pamiętacie co tam dostawaliśmy. Teraz rozpatrujemy przypadek poprawnie przetworzonego zapytania  w wyniku czego otrzymujemy decyzję. 

generalnie podczas sprawdzania odpowiedzi z API pierwsze co robimy to sprawdzamy czy jego status jest taki jakiś spodziewaliśmy. W naszym przypadku poprawnie zwrócona odpowiedź z API miała status code równy 200, więc na początku sprawdzamy właśnie to.

```python
assert response.status_code == 200
```

Czyli wykorzystujemy tutaj sobie nasz `assert` i upewniamy się że status równy jest 200.

Kolejna rzecz jest taka że upewniamy się czy dane przyszły w takiej strukturze jakieś spodziewamy. W kontekście naszego API  odpowiedzi powinny być zwrócone w strukturze JSON, a zatem upewniamy się czy nasza odpowiedź faktycznie zwrócone są w formacie JSON. Można to sprawdzić w nagłówkach:

```python
assert response.headers.get('content-type') == 'application/json'
```

Jeżeli to sprawdzanie przejdzie to znaczy że nasza odpowiedź jest w tej sonie więc wyciągnijmy ją sobie 

```python
response_json = response.json()
```

Czyli po prostu struktura klucz wartość. W naszym przypadku API zwraca takiego JSON gdzie kluczem, jest słowo `decision` a w wartościach jest decyzja z modelu - możemy to sobie sprawdzić w naszej implementacji pydanticowego modelu. Czyli upewniam się że pole `decision` istnieje faktycznie w tym obiekcie:

```python
assert "decision" in response_json
```

Okej czyli ten etap przejdzie to znaczy że znajduje że decyzja znajduje się w naszej strukturze JSONowej teraz musimy sprawdzić wartość odpowiedzi z naszego API i teraz To się musimy chwileczkę zatrzymać. Jeżeli sobie podejrzymy jaka jest odpowiedź dla naszego przypadku testowego, To jest to `DECLINE`. I teraz można by się pokusić o to żeby przyrównać czy faktycznie wartość jest równa `DECLINE`, ale nie możemy tego zrobić. Tak jak powiedziałem na początku wyzwanie w testowaniu odpowiedzi z API w którym zaimplementowany jest model uczenia maszynowego jest takie, że  po pierwsze model może zwracać wyniki niepowtarzalne. To będzie prawdziwe w szczególności w przypadku modeli językowych które tekst generują za każdym razem inny. Kiedy wyślecie te same zapytanie to odpowiedź będzie inna więc sprawdzanie w testach czy dana odpowiedź równa się danej odpowiedzi jest kompletnie bezsensowna bo nigdy to nie zadziała. Drugi przypadek jest taki że nawet jeżeli mamy model deterministyczny tak jak w tym przypadku regresja logistyczną która zawsze przy podaniu tych samych wartości będzie zwracać tą samą odpowiedź to musicie wziąć pod uwagę to że ten model może się zmienić w przyszłości. Na przykład
1. to wciąż będzie regresja Logistyczna ale o innych parametrach zatem ta decyzja się zmieni 
2. albo zupełnie będzie inny model który z racji faktu że to jest inny model też może zmienić decyzję 
dlatego jeżeli podejdziemy do tego tak że będziecie przyrównywać wartość do konkretnej wartości to gwarantuje wam to że wasze testy nie będą przechodzić później jak się coś zmieni w modelu. Będziecie się głowić co jest powodem, gdzie jest bak natomiast to będzie problem ze złą konstrukcją testu. To co my musimy zrobić w kontekście API MLowego to to żeby sprawdzić właściwości tej odpowiedzi a nie jej konkretną wartość. O jakich właściwościach tu możemy mówić? No przede wszystkim to że dana odpowiedź na przykład jest stringiem, albo dana odpowiedź ma maksymalnie długość taką i taką, albo że dane wartości nie przekraczają jakieś wartości i tak dalej czy skupiamy się na pewnych No właśnie właściwościach odpowiedzi a nie konkretne jej wartości. W tym przypadku co możemy zrobić no przede wszystkim upewnijmy się  że nasza odpowiedź nie jest w ogóle pusta 

  

```python

assert response_json["decision"] is not None

```

Skoro nie jest pusta to możemy sprawdzić czy jest stringiem w tym przypadku

  

```python

assert isinstance(response_json["decision"], str)

```

  

Jeżeli będzie stringiem to musimy iść dalej upewnijmy się że string nie jest całkowicie pusty.

  

```python

assert response_json["decision"] != ""

``` 

  

Okej To teraz wyłączmy to bugara i usuń breakpointa z naszego kodu i włączmy nasze testy zobaczymy co dostaniemy. Jak widzicie mamy `test_decisions` przeszło to znaczy że wszystkie warunki zostały spełnione które to jest zdefiniowaliśmy.

  

czy macie pytania do tego co napisaliśmy A może ktoś z was widzi co jeszcze można było sprawdzić żeby upewnić się że na przykład odpowiedzi z API są poprawne? 

  

Tak wam wspomniałem na początku testów ja bym chciał się skupić na takiej głównej ideach tak tak żebyście mogli sobie poradzić po prostu z każdym przypadkiem. Oczywiście takich przypadków sprawdzających można mnożyć naprawdę ogranicza nas tylko wyłącznie wyobraźnia oraz Oraz to jak ty a pies z tobą zaimplementowane. Im więcej takich testów się napisze tym więcej nabierzesz takie intuicji co warto sprawdzić A co nie warto sprawdzić. I też musicie być gotowi na to że w trakcie pisania testów No nieraz patrzycie każdego przypadku jaki może zacząć się dosłownie waszym API. Zawsze kiedyś zdarzy się jakiś ecz Case który I został uwzględniony w waszych testach i dopiero taki Case wyszedł na produkcji to jest rzecz normalna która się zdarza każdemu jeżeli taka sytuacja zaistnieje To wy nauczeni tym doświadczeniem będzie nie pamiętać że taki Case War rozpocząć w testach po pierwsze a po drugie że takie sytuacje wtedy wraca się do testów i ten Case który bierze na produkcji dodatkowo się otestowuje jeszcze w testach jednostkowych czyli integracyjnych tak żeby on po prostu ponownie nam się nie pojawił.

  

To co rozpatrzyliśmy w ramach tego naszego pierwszego testu dotyczy tak zwanego przypadku pozytywnego czyli Innymi słowy sprawdzenie działania API wtedy kiedy przychodzi prawidłowy request i wtedy kiedy  API przetwarza nasze zapytanie bez żadnych problemów ale oprócz przypadków pozytywnych musimy również rozpatrzeć przypadki negatywne Czyli co się zadzieje w momencie kiedy API dostanie od nas błędne dane bądź co się zadzieje wtedy kiedy nasze API rzuci błędem w środku swojego działania i my też takie przypadki musimy rozpatrzeć i teraz sobie do nich przejdziemy. Ale zanim to zrobimy to uruchomimy sobie nasze API tak normalnie i Sprawdźmy co ona mówi dokumentacja 

  

Jak widzicie ten pozytywny przypadek który teraz przetestowaliśmy jest oświetlony tutaj na zielono jako Successfull response. Ale jak widać jest jeszcze validation error oraz to co nie jest wypisane ale w każdym serwisie może się zdarzyć czyli błąd 500 internal server error czyli musimy rozpatrzeć te dwa przypadki. 

  

Zacznijmy od tego naszego błędu 422 czyli mówiącego o tym że nasze dane wejściowe są złym w formacie. 

  

```python

def test_received_422_when_atleast_one_body_param_is_invalid(

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

):

  

```

Teraz popsujmy sobie nasze request body i Dodajmy jakiś jakąś wartość do zmiennej które nie istnieje A potem wyślijmy requesta.

  

No i teraz patrzę w dokumentację musimy napisać test i upewnić się czy faktycznie otrzymujemy to co powinniśmy otrzymać. Standardowe sprawdzenie status code i format odpowiedzi

  

```python

assert response.status_code == 422

assert response.headers.get("content-type") == "application/json"

response_json = response.json()

```

  

Dalej musimy się upewnić czy zawartość tego responsa jest taka powinna być 

  

Zaczynamy  od klucza `detail` czy znajduje się w odpowiedzi i dalej musimy sprawdzić czy ta struktura jest taka jaka jest dokumentacji. 

  

```python

assert "detail" in response_json

```

  

Dokumentacja wskazuje na to że pewne obiekty są a rajem więc tutaj w tym przypadku musimy skorzystać z biblioteki `collections.abc` aby sprawdzić tak faktycznie jest:

  

```python

response_detail = response_json["detail"]

assert isinstance(response_detail, collections.abc.Sequence)

assert len(response_detail) == 1

assert "loc" in response_detail[0]

assert isinstance(response_detail[0]["loc"], collections.abc.Sequence)

assert "msg" in response_detail[0]

assert "type" in response_detail[0]

```

  

I to moim zdaniem wyczerpuje już do sprawdzenie włączmy testy i zobaczymy czy się udało. Powinno się udać. Udało się.

  

Ale jeszcze ten minus z tym testem dlatego że Spójrzcie my tak naprawdę sprawdziliśmy tylko czy walidacja nie przechodzi dla zmiennej `foreign_worker`. A z kolei w modelu mamy pięć zmiennych więc musimy sprawdzić wszystkie 5 teraz jak to uzyskać. To jest stosunkowo proste skorzystamy tutaj z `pytest.mark.parametrize`, Która pozwala nam na uruchamianie naszych testów wielokrotnie w zależności od danych wejściowych. 

  

```python

@pytest.mark.parametrize(

   ("key", "value"),

   [

       ("installment_rate_in_percentage_of_disposable_income", -1),

       ("age_in_years", 10),

       ("age_in_years", 100),

       ("foreign_worker", "CATEGORY_NON_EXISTENT"),

       ("present_employment_since", "CATEGORY_NON_EXISTENT"),

       ("personal_status_and_sex", "CATEGORY_NON_EXISTENT"),

   ],

)

```

  

Bardzo ważne jest to żeby te wartości kiwali przekazać jako pierwszy parametry funkcji. A dasz argumenty To są nasze pytestowe fixtures. I zaktualizujemy kod odpowiedzialny za stworzenie body nieprawidłowego.

  

Włączmy testy i zobaczymy czy że faktycznie wszystkie case'y powinny przejść. 

  

Ale to nie są jeszcze wszystkie sprawdzania które warto zrobić w API ja jeszcze dodatkowo Sprawdzam odpowiedź W momencie kiedy wszystkie parametry są złe także mieć pewność że ten ta odpowiedź zwróci nam wszystkie informacje:

  

```python

def test_received_422_when_all_body_params_are_invalid(

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

):

   request_body_invalid = {

       "installment_rate_in_percentage_of_disposable_income": -1,

       "age_in_years": 10,

       "foreign_worker": "CATEGORY_NON_EXISTENT",

       "present_employment_since": "CATEGORY_NON_EXISTENT",

       "personal_status_and_sex": "CATEGORY_NON_EXISTENT",

   }

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body_invalid)

   assert response.status_code == 422

   assert response.headers.get("content-type") == "application/json"

   response_json = response.json()

   assert "detail" in response_json

   response_detail = response_json["detail"]

   assert isinstance(response_detail, collections.abc.Sequence)

   assert len(response_detail) == 5

```

  

 wtedy zrobiłem małą zmianę dlatego że zatrzymałem się tylko i wyłącznie na sprawdzaniu czy długość tej listy jest pięć  dlatego że każdy z parametrów był błędny nie sprawdzam zawartości poszczególnych elementów dlatego że jest to robione w poprzednim teście.

  

Jeśli chodzi o walidacje parametrów to prawie skońćzyliśmy. Spójrzmy sobie na dokumentację i Zwróćmy uwagę na dwie rzeczy. Mamy tam dwie zmienne  w których określone są przedziały jak widzicie one są domknięte te większe bądź równy 0 bądź 1865 przy czym 18 i 65 są akceptowalne.  jeżeli w API są takie warunki to musimy koniecznie sprawdzić czy przedziały bo mam doświadczenia że to jest najczęstszy bug w API.  więc to co my musimy zrobić de facto to po prostu powtórzyć test związany z tą ścieżką pozytywną Z taką zmianą że wpuścimy  requesty które zawierają te wartości brzegowe. Więc jest to niejako połączenie testu pozytywnego z pewnymi parametrami z testu negatywnego. 

  
  
  

```python

@pytest.mark.parametrize(

   ("key", "value"),

   [

       ("installment_rate_in_percentage_of_disposable_income", 0),

       ("age_in_years", 18),

       ("age_in_years", 65),

   ],

)

def test_decision_returned_correctly_for_boundary_values(

   key,

   value,

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

   mocker,

):

   request_body = request_body | {key: value}

   mocker.patch.object(BaseRedisClient, "read", return_value=None)

   mocker.patch.object(BaseRedisClient, "write", return_value=None)

   mocker.patch.object(BasePostgresClient, "write", return_value=None)

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

   assert response.status_code == 200

   assert response.headers.get("content-type") == "application/json"

   response_json = response.json()

   assert "decision" in response_json

   assert response_json["decision"] is not None

   assert isinstance(response_json["decision"], str)

   assert response_json["decision"] != ""

```

  

Jak widzicie stosuję długie nazwy w testach jednostkowych ale jest to  dobra praktyka be tak robić bo Zwróć uwagę na to jaki dostajecie wynik w konsoli  nazwa testów wprost wskazuje na to co on sprawdza i przy okazji jak używacie `pytest.mark.parametrize`  to wsadzone są również Wartości dla których są normalne testy.

  

Okej przypadek z błędnie przekazanymi wartościami do API został powtórzone zostało nam ostatni Case tutaj już nie będziemy aż tak bardzo się rozpisywać czyli  obsługa przypadków wtedy kiedy wystąpi jakiś błąd w naszym API.

  

Przy testowaniu tego przypadku musimy spojrzeć jak mamy zdefiniowane exception handlery bo to one definiują logikę zwracania odpowiedzi z API w momencie kiedy występuje jakieś błędy w naszym API.

  

Tego exception handlera dla `RequestValidationError` Już tak naprawdę przetestowaliśmy w momencie sprawdzenia inputu do naszego API nic nie musimy tego robić Zostały nam dwa czyli dla naszego APIError  oraz dla generalnego Exception. 

  

Utwórzmy sobie ten test

```python

def test_received_500_when_error_raised_in_api(

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

   mocker,

):

   mocker.patch.object(BaseRedisClient, "read", return_value=None)

   mocker.patch.object(BaseRedisClient, "write", return_value=None)

   mocker.patch.object(BasePostgresClient, "write", return_value=None)

```

  

Teraz to co my tu musimy dodatkowo zrobić że to  aby nasze API rzuciło jakąś błędem.  tutaj zrobimy to w ten sposób że zmokujemy sobie działanie naszego modelu i tym razem skorzystamy z argumentu `side_effect`  do którego podamy mu błąd `Exception`.  i w ten sposób w momencie użycia funkcji `predict_decision` po prostu rzucone zostanie `Exception` 

  

```python

mocker.patch.object(CreditScoringModel, "predict_decision", side_effect=Exception)

response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

assert response.status_code == 500

assert response.headers.get("content-type") == "application/json"

response_json = response.json()

assert "message" in response_json

assert response_json["message"] is not None

assert isinstance(response_json["message"], str)

assert response_json["message"] != ""

```

  

I Dodajmy sobie jeszcze `pytest.mark.parametrize` bo mamy 2 exception handlery 

  

```python

@pytest.mark.parametrize("error", [Exception, APIError])

def test_received_500_when_error_raised_in_api(

   error,

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

   mocker,

):

   mocker.patch.object(BaseRedisClient, "read", return_value=None)

   mocker.patch.object(BaseRedisClient, "write", return_value=None)

   mocker.patch.object(BasePostgresClient, "write", return_value=None)

   mocker.patch.object(CreditScoringModel, "predict_decision", side_effect=error)

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

   assert response.status_code == 500

   assert response.headers.get("content-type") == "application/json"

   response_json = response.json()

   assert "message" in response_json

   assert response_json["message"] is not None

   assert isinstance(response_json["message"], str)

   assert response_json["message"] != ""

```

  

Okej Słuchajcie Moim zdaniem na tym zakończymy sekcję związaną z testami jednostkowymi Oczywiście tutaj jeszcze można pewnie mnożyć przypadki jak się dłużej tym zastanowimy ale uważam że to wam w zupełności wystarczy jako taki punkt startu i odniesienia bo mamy tutaj omówione najważniejsze rzeczy ścieżka pozytywną czyli sytuację w której nasz reques jest poprawnie przetwarzany pod warunkiem że przekażemy do niego poprawne dane i w naszym API  dostanie wszystko poprawnie przetworzone. Mamy obsłużone też przypadki kiedy to ktoś wyśle nam błędne dane i upewniamy się że faktycznie zwracamy mu poprawne odpowiedzi w takiej sytuacji. Mamy też obsłużone eczkejsy czyli ta walidacje naszych parametrów na ich wartościach brzegowych żeby mieć pewność że faktycznie to działa tak jak chcemy. No i przypadek gdy w naszym API  coś nie zadziała i tutaj zwracamy mu wtedy 500 z wiadomością.  Czy do tej części macie pytania Jeszcze zanim przejdziemy dalej do testów integracyjnych. 

  

Moim zdaniem testów nigdy za wiele jeżeli w trakcie ćwiczeń wpadnie wam jeszcze jakieś dodatkowy pomysł żeby też sprawdzić to jak najbardziej zrobię tak samo w swoich projektach możecie Jeszcze to lepiej o testować sprawdzić pewne warianty graniczne w zależności od tego jaki projekt wybierzecie generalnie tak jak widzicie pisząc testy drugi raz zastanawiamy się nad tym co ten nasz kod robi, co chcemy żeby zwrócił. To jest ten  ten jeden z benefitów o którym wam mówiłem na początku generalnie powiem wam ciekawostkę że czasami zdarza się nawet tak że pisanie testów może zająć wam więcej czasu niż przygotowanie całej implementacji to jest rzecz normalna. Mój rekord to pisanie cały tydzień test od poniedziałku do piątku implementacji którą przygotowałem w 3 dni. Ale naprawdę warto ten czas poświęcić po prostu Potraktujcie pisanie testów jak inwestycje długoterminową zwroty będą za jakiś czas wtedy kiedy będziecie dalej rozwijać wasz API zmieniać wnieść jakieś implementacje to testy obronią was przed tym żeby na produkcję wdrożyć coś co po prostu nie działa.

  

Okej to w takim razie czas rozpocząć testy integracyjne…

  
  

## MOŻE DODAJ TEŻ CO JESZCZE MOŻNA SPRAWDZIĆ W UNIT TESTACH ALE NIE ROZPISUJEMY WSZYSTKICH PRZYPADKÓW BO NIE STARCZY NA TO CZASU

  

https://www.zartis.com/api-testing-components-and-tips/

  

## MOŻE NA KONCU WSPOMNIJ JAK TO MOZNA POPRAWIĆ UŻYWAJĄC OBIEKTU Mock() ALBO MagicMock()**