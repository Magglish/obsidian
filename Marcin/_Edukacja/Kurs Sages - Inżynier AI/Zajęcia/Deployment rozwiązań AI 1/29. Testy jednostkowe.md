# Testy jednostkowe

Obejrzyj: https://www.youtube.com/watch?v=vlLSgdQNgMI

Przechodzimy sobie do kolejnego tematu jakim jest testowanie poprawności naszego API. W przypadku testowania będziemy mówić o dwóch wymiarach: przede wszystkim w wymiarze jej implementacyjnym, czyli Innymi słowy czy nasze API zwraca to czego się po nim spodziewamy oraz też w wymiarze obciążeniowym to znaczy będziemy sprawdzać jak szybko nasze API jest w stanie zwrócić odpowiedzi pod różnymi wariantami obciążenia, jak API działa kiedy jest wysłane tylko jeden request na sekundę, kiedy wysłanych jest 5 requestów na sekundą itd. i to jest bardzo ważny etap który warto  sprawdzić żeby mieć ogólny pogląd na to jak nasze API będzie zachowywał się pod obciążeniem na produkcji.

Testowanie kodu było już omawiane na poprzednich zajęciach z programowania w Pythonie jeden i dwa. Ja w tym module będę oczywiście nawiązywał do pewnych pewnych pojęć ,które wtedy poznaliście ale osadzając jej całkowicie w kontekście API. 
 
Od razu wam powiem że pisanie testów API moim zdaniem jest trudniejsze niż otestowywanie implementacji naszych kodów. Jest to powiązane z tym że w API dzieje się dosyć sporo. Jak sobie spojrzymy w `main.py` to to widzicie, że nasze requesty są odpowiednio przekształcane na pydanticowy obiekt, tutaj mamy sprawdzenie czy jest odpowiedź w cache'y, potem wykonanie predykcji modelu, potem zapisanie tych danych do bazy. Zatem w ramach jednego endpointu dzieje się dosyć sporo. W dodatku każde API ma dużo oczywiście elementów wspólnych, czy to będzie API związane z tak tutaj w tym przypadku modelami oceniającymi ryzyko kredytowe, ale też są elementy wspólne w przypadku API do modelu tekstowym jak się przekonaliśmy na przykładzie swoich ćwiczenia. Jednakże wciąż są pewne różnice i generalnie nawet jak się pisało sporo testów do różnych API to i tak mogą być one momentami trudne, nawet dla bardziej doświadczonych osób. Niestety też pisanie niektórych testów jest dosyć czasochłonne , dlatego tym bardziej jeśli ma się z tym kontakt po raz pierwszy, bo trzeba zwrócić uwagę na bardzo wiele rzeczy. Intuicja co testować dopiero przyjdzie po napisaniu wielu, wielu testów. I też fackupy na produkcji uczą na co zwracać uwagę.  

Dodatkowo trudnością w testowaniu API z zaimplementowanym modelem jest fakt że nasz model może być niedeterministyczny, w szczególności w przypadku kiedy będziemy implementować API dla modeli tekstowych generatywnych które mogą zwrócić dosłownie dowolny tekst. Nawet jeżeli pracujemy z bardziej klasycznymi algorytmami w klasycznym problemie jak na przykład regresja logistyczna, która za każdym razem zwróci nam ten sam wynik jeżeli podamy mu te same parametry, to problemem drugim jest to taki że nasz model może się zmienić. Może to być i nawet wciąż regresja logistyczna ale już z innymi parametrami. Czy zmiana algorytmu na drzewa decyzyjne i gdybym usta cokolwiek innego i wtedy dla danego wejścia zmieni się wynik dlatego też takie testy które porównują czy dany  czy dany wynik z modelu jest równa jakiś tam konkretnej wartości po prostu  będą nieużyteczne dlatego też do testowania API  który ma w sobie model masz learningowi trzeba podejść w pewien specyficzny sposób

  

 ja w tym module poświęconym testom Chcę przekazać wam taką najbardziej generalną wiedzę odnośnie to jak testować API Chodzi mi o to że nie chciałbym nauczyć was testowania konkretnej implementacji tego API który jest apim do modelu skorygowego jak w tym przykładzie tylko chcę się skupić na takich generalnych zasadach czy nawet sposobie myślenia w przypadku pisania testów tak żebyście spokojnie mogli sobie poradzić z dowolnym API jakie będzie tworzyć  tak jak powiedziałem pomiędzy niektórymi API będą różnica tak czy to będzie API do klasycznego programu czy do tekstu Czy do obrazu Więc pomimo więc wciąż będziecie mieli pewne wyzwanie  ale po takiej Generalnej zasadach które poznacie Myślę że sobie z tym poradzicie z mniejszym bądź większym trudem ale na pewno sobie poradzić.

  

Domyślam się że na poprzednich zjazdach obaj to nie mieliście powiedziane to Jakie są rodzaje testów Ja tylko tak o nich wspomnę w punktach po to żeby nakreślić wam kontest co dokładnie będziemy robić teraz.

generalnie można wyróżnić 7 rodzajów testów:

  

1. testy jednostkowe - Testy jednostkowe czyli test badający pojedyncze funkcjonalności w danej implementacji
    
2. testy integracyjne - Testy integracyjne z kolei  testują znacznie więcej niż testy jednostkowe i Tutaj mówimy już o interakcji z innymi serwisami Jeśli taka interakcja następuje w naszym kontekście są to bazy danych Redis oraz postgres
    
3. testy funkcjonalne - Testy funkcjonalne akceptacyjne kompleksowe Dotyczą już pewnych aspektów biznesowych działania naszego API Dla przykładu w naszym kontekście modelu skolimowego mogła to być fakt taki że dana predykcja modelu może być na przykład wykorzystywana tylko przez 7 dni czyli Innymi słowy gdyby klient chodził i pytał o kredyt w różnych miejscach poprzez okres 7 dni używalibyśmy tej samej decyzji którą na początku otrzymaliśmy Niezależnie od tego jak warunki by się zmieniły ale na przykład po 7 dniach chcielibyśmy żeby nasze API ponownie Ponownie zwróciło nową decyzję ale już w oparciu o model a nie decyzje z przechowywane w cashu I może wtedy temu klientowi uda się dostać kredyt także te testy są bardzo mocno powiązane z warunkami biznesowymi w jakich  wasze API będzie implementowane my w naszym przypadku taki by wymagań biznesowych nie mamy oczywiście ale  ale od strony implementacyjnej one nie różnią się specjalnie  od testów jednostkowych czy czy integracyjnych natomiast powiem wam szczerze wciąż mam problemy ze zrozumieniem z różnic pomiędzy funkcjonalnym akceptacyjnym a kompleksowym one są tak do siebie podobne że ja i tak traktuję jako jedno po prostu testy funkcjonalne
    
4. testy akceptacyjne - patrz wyżej
    
5. testy kompleksowe - patrz wyżej
    
6. testy wydajności - Kolejną grupą są oczywiście testy wydajności Czyli to co będziemy dzisiaj robić w dalszej części czyli sprawdzanie tego jak nasze API działa pod różne obciążeniem 
    
7. testy smoke - No i na końcu testy smoke  które Sprawdzają czy nasze API w ogóle działa i jest w stanie cokolwiek zrobić testami swą dzisiaj się nie będziemy zajmować tylko dopiero na zjeździe następnym kiedy  poznamy czym są kontenery poza Jokera bo smok testy najlepiej wykonuje się właśnie już na kontenerach.
    

  

My teraz skupimy się na testach jednostkowych, testach integracyjnych oraz na testach wydajności. Testami smoke zajmiemy sie na kolejnym zjeździe, i dodatkowo wrócimy też do testów integracyjnych jak poznamy docker compose. 

  

W naszych testach oczywiście będziemy docelowo korzystać z biblioteki bajtest która była omawiana na poprzednich zjazdach.  pewne rzeczy będę Oczywiście przypominał ale nie będę aż tak bardzo wchodzić w szczegóły Mam nadzieję że pamiętacie to co było na poprzednich zjazdach jeżeli nie no to śmiało pytajcie ja wtedy dopowiem o kilka zdań więcej na temat tego co implementujemy. 

  

Zanim zaczniemy pisać pierwsze testy Chciałbym żebyście poznali moje zdanie Dlaczego warto pisać testy.  możliwe że takie zalety testowania już były omawiane na poprzednich zajęciach ale chciałbym żebyś się też poznali mój punkt widzenia:

  

1.  oczywistą oczywistością jest to że testy walidują logikę działania naszego API bez dwóch zdań natomiast to nie jest moim zdaniem  jedyny powód dla którego warto pisać testy
    
2.  Drugi powód jest taki że testy nie jako stanowią też dokumentację weszło kodu w momencie kiedy ja czytam implementację która jest może być dosyć skomplikowana napisana przez inną osobę to lubię też spojrzeć na to jak są napisane test dlatego że testy mówią mi o tym co zostało wpuszczone do funkcji Jaki jest oczekiwanie rezultat i mając te dwie informacje znacznie łatwiej jest mi analizować implementację bo wiem co wchodzi i czego się spodziewać
    
3.  trzeci powód jest taki że podczas pisania testów wy tak naprawdę jeszcze raz analizujecie to dokładnie napisaliście i mogą wyjść wszelkie błędy  Czy to w implementacji czy właśnie w logice działania kodu.  dodatkowo jeżeli jest problem z napisaniem testu  to jest to potencjalny sygnał że wasza implementacja jest po prostu zbyt skomplikowana i należy ją zrefaktorować na przykład rozdzielić funkcje na mniejsze części wydzielając pewne pewne pewne logikę działania właśnie oddzielny funkcji i przetestować te pojedyncze rzeczy Generalnie ja widzę po sobie że im więcej pisze testów ten kod jest mój lepszy łatwiejszy krótszy zatem Uwaga na to żeby to było żeby dane funkcje robił tylko jedną konkretną rzecz jaką mają robić zgodnie z tą zasadą S  w solidzie. 
    
4.  czwarty Powód jest taki że bez testów no niemożliwe jest praktycznie refaktoring waszych kodu.  jeżeli są potrzebne zmiany w API to wprowadzanie zmian bez dobrego testowania to powiem szczerze jest ryzykowne bo nie macie gwarancji tego że po waszych zmianach wciąż ona działa tak jak ma działać Dlatego moim zdaniem tak bardzo ważne jest to żeby te testy były nawet w kontekście prostego Poka który zrealizowany przez was uchroni was to przed dużą ilością bagów które mogą powstać w momencie kiedy już to zostanie wdrożone lepiej ten bak wykryć na etapie developmentu a nie potem stresować się bo dane serwisy na produkcję działają A chcą ludzie z nich korzystać tak bo na przykład są już zaimplementowane w jakimś produk. 
    

  

generalnie nie wiem czy to zauważacie ale na rynku istnieje już coraz większa tendencja do tego żeby wymagać od przyszłej tej scientist I przede wszystkim masz Inżynierów to jest oczywiste umiejętności  programowania znajomości wzorców  obiektowych czy też właśnie umiejętności pisania testów jednostkowych.  dlatego że tak mówiłem wcześniej maszyna ryj to nic innego jak softów inżyniering troszkę inny bo pracujemy z modelami ale wciąż piszemy kod w pythonie wciąż jest na produkcji na taką Jakie istnieją pozostałe serwisy jest bardzo dużo podobieństw i generalnie od osób zajmujących danych coraz częściej wymaga się po prostu dobry umiejętności programowania dlatego że te rzeczy które wy robicie będą wydawane na produkcję ja sam osobiście jak rekrutowałem listów do swojego zespołu jakiś czas temu to też zwracam uwagę  na to jak piszą kot i to też jest jedno z kryteriów decyzyjne przy wyborze kandydatów .

  

Obejrzyj: [https://www.youtube.com/watch?v=vlLSgdQNgMI](https://www.youtube.com/watch?v=vlLSgdQNgMI) 

  

Okej w takim razie myślę że możemy zacząć już po tym wstępie pisać pierwsze testy. To co chciałbym żebyście stworzyli sobie folder test tutaj w głównym folderze naszego repozytorium i potem trzy podfoldery odpowiednio unit i tygration i stres. Pamiętajmy o tym żeby były w nich `__init__.py`. Stwórzmy też plik fixtures.py w `tests` oraz w `unit` i `integration`.

  

Teraz Zanim zacząłem pisać pierwsze testy chciałbym żebyśmy weszli do opcji w Pycharmie: Settings -> Tools -> Python Integrated Tools -> Default test runner ustawić jako Pytest. Następnie znajdźmy Run configuration (Alt + Shift + F10). Edit Configurations i usuńmy wszystkie konfiguracje dla Python tests jeśli występują. 

  

Teraz pytanie jakie możemy sobie zadać to co będziemy testować jeżeli spojrzymy sobie teraz na mainpay w którym mamy zdefiniowane naszą logikę działania API to de facto odpowiedź brzmi wszystko czym musimy przetestować czy faktycznie exception handlery poprawnie wyłapują błędy czy Eventy startup Shutdown poprawnie inicjalizują obiekty a shotgun odpowiednio zamyka połączenia z bazami danych No i oczywiście działanie naszego enpointa decisions Czy poprawnie są czytane dane z bazy Redis czy poprawnie model wykonuje predykcję czy poprawnie wszelkie bagram taski zapisują dane i tak dalej i tak dalej także jest trochę rzeczy do przetestowania te Apis stosunkowo jest proste tutaj mało funkcjonalności ale jak sami zobaczycie są pewne wyzwania w testowaniu API. Pewne działania naszego API będzie testowane w ramach testów jednostkowych a część rzeczy w ramach testów integracyjnych to wszystko się wyjaśni w trakcie będziemy pisać już te testy za picie myślę taką intuicję co będziemy sprawdzać w ramach testu wypadkowych A co będziesz robione w ramach testem integracyjnych.

  

Zacznijmy od zdefiniowana zdefiniowania sobie struktur danych które będziemy używać naszych testach Przejdźmy sobie do fixtures.py i tam będziemy definiować struktury danych używane właśnie w testach na początek zaimportujmy sobie wszystkie  wszystkie obiekty które będą nam potrzebne 

  

```python

import pytest

```

  

Musimy od tego że zdefiniujemy sobie body naszego requesta jego nagłówki oraz URL naszego endpointa:

  

```python

@pytest.fixture(scope="session")

def request_body():

   return {

       "installment_rate_in_percentage_of_disposable_income": 0.25,

       "age_in_years": 40,

       "foreign_worker": "yes",

       "present_employment_since": "unemployed",

       "personal_status_and_sex": "male: single",

   }

  

  

@pytest.fixture(scope="session")

def request_headers():

   return {

       "Content-Type": "application/json",

   }

  

  

@pytest.fixture(scope="session")

def request_endpoint():

   return "/decisions"

```

  

Krótkie przypomnienie pytestowe `fixtures` nic innego jak po prostu strukturę danych które można używać w naszych testach.  ten Scope który mówi o sesji to po prostu oznacza że w momencie rozpoczyna testów te obiekty zostaną utworzone i trzymane przez całą sesję testowania. 

  

Teraz przejdźmy do `fixtures.py` w folderze `unit` i zainicjować nasz obiekt z API:

  

```python

@pytest.fixture(scope="package")

def initialized_app():

   with TestClient(app=app, raise_server_exceptions=False) as client:

       yield client

```

  

Nasz API posiada w implementacji startup z funkcją oraz Shutdown żeby można było uruchomić te funkcje to musimy uruchomić naszą nasz obiekt up jako kontekst manager Dlatego piszemy tutaj `with` oraz Fast API dostarcza nam taką klasę jak TestClient Która służy właśnie do uruchomienia takiej testowej wersji naszego API.

  

Teraz bardzo ważna rzecz o której musicie pamiętać Musimy tutaj użyć `yield` a nie `return`. Nie wiem czy pamiętacie jak działają kontekst menedżery które powstają właśnie z użyciem klauzuli with ale krótko mówiąc działa to tak że  że jeżeli dana klasa ma zaimplementowane takie Magic metody jak Ender i exit to klauzula with właśnie używa tych metod w momencie Kiedy ten mój film zaczyna działać i w momencie kiedy kończy się jego działanie. W tym  działają IFA będzie następujące rozpoczęcie tego kodu po prostu zostanie uruchomiony startup skrypt który mieliśmy zdefiniowane a w momencie kiedy Kot zrobił się zakończy to wtedy zostanie uruchomiony Shutdown script. I żeby używać API w kontekście naszych testów musimy tutaj użyć test Klein który nam dostarcza Fast API właśnie po to żeby testy tworzyć. Druga ważna rzecz to to żeby w takiej sytuacji użyć yielda a nie returna. Dlaczego wilk w przypadku testów pisanych w bajce działa tak że ta funkcja i initialized app  zwróci nam zainicjowany klient następnie zostaną wykonane testy jednostkowe które zdefiniujemy a po tych wszystkich testach kot wróci do tego miejsca Będzie dalej się wykonywał ale jak widzimy dalej już nic nie ma Czyli tak naprawdę zakończymy działanie wifa i wtedy zostanie uruchomiony Shutdown skrypt. Nie wiem czy to jest do końca od was jasne jeszcze ale jak już napiszemy pierwszy test to zobaczycie jak to działa i wtedy jeszcze raz sobie tego wrócimy już na pełnym przykładzie i wtedy myślę że się bardziej rozjaśni.  gdybyśmy tutaj użyli returna z kolei to niestety ale zadziałałby to tak że klient uruchomił były startupscript  i od razu inaczej się shutdown script,  w efekcie czego otrzymamy tak naprawdę puste API bez żadnych obiektów które chcemy. 

  

Mamy zdefiniowane nasze struktury danych testowe które będziemy używać w trakcie pisania testów teraz przejdźmy sobie do  do foldery z testami i spróbuj napisać test na pierwszy przypadek który chcemy sprawdzić. Stwórzmy sobie plik pythonowy `test_decisions.py`. Importujmy obiekty które będą nam potrzebne 

  

```python

import pytest

  

from tests.fixtures import request_body, request_headers, request_endpoint

from tests.unit.fixtures import initialized_app

  

from src.databases.redis.clients.base import BaseRedisClient

from src.databases.postgres.clients.base import BasePostgresClient

from src.models.credit_score import CreditScoringModel

from src.utils.errors import APIError

  

import collections.abc

  

```

  

I Słuchajcie pierwszy test jaki napiszemy to sprawdzimy czy faktycznie nasze API zwraca  poprawną decyzję w sytuacji kiedy przekażemy do niej poprawne dane nie zostanie do mnie rzucony bo żaden błąd i tak dalej.

  

```python

def test_decision_returned_correctly(initialized_app, request_body, request_headers, request_endpoint, mocker):

    pass

```

  

Ale chciałbym żebyśmy na razie zdefiniowali ten test jako pusty całkowicie spróbowali to uruchomić. Upewnijmy się że nie ma żadnych baz danych włączonych Jeśli są to musi mnie ubić docker kill.

  

Teraz może krótkie wyjaśnienie odnośnie argumentów które podajemy niestety minusem bytes-a jest to że nie do końca te argumenty mogą być dla was zrozumiałe generalnie jest tak że by test przechowuje w sobie pewne obiekty które można przekazać do testów które tworzycie i żeby pajtest je przekazał w momencie uruchamiania testów to trzeba je właśnie zdefiniować w argumentach tej funkcji czyli tak tutaj widzicie ja w tym teście dodałem nasze fixtures które przed chwilą zdefiniowaliśmy i to oznacza że w momencie uruchomienia naszych testów te obiekty będą dostępne w środku tego testu i możemy odpowiednio na nich działać tak żeby nasz test  stworzyć dodałem też na końcu taki obiekt moker który zaraz nam się przyda. Ten obiekt moker jest właśnie obiektem który dostarcza nam pajtest teraz pytanie skąd wiedzieć w ogóle co jest nam dostarczy możemy to sprawdzić Korzystając z terminala:

  

```bash

pytest –fixtures

```

  

Jak sobie spojrzymy na to co ta komenda zwróciła bo to widzimy że mamy tutaj różne obiekty i mamy też informacje o naszych obiektach które my sobie zdefiniowaliśmy i to są obiekty które możemy wykorzystać w naszych testach pod warunkiem że umieścimy je w naszych argumentach. 

  

Okej to teraz włączmy sobie nasz test pierwszy i zobaczymy co się wydarzy:

  

```bash

pytest tests -v

```

  

No jak widzicie dostaliśmy błąd baza Redis nie istnieje tak samo będzie w przypadku bazy postgress nie istnieje my chcemy napisać pierwsze testy jednostkowe które będą skupiać się na podstawowych funkcjonalnościach naszego API ale testy jednostkowe charakteryzują się tym że one nie wchodzą w interakcje z innymi usługami jeżeli one są wykorzystywane w naszym API takie testy które wykorzystują inne usługi w czyli w tym przypadku bazy danych są testy integracyjne i one są troszeczkę bardziej skomplikowane do niej przyjdziemy później teraz pytanie powstaje Jak stworzyć testy jednostkowe  dla API które korzysta z innych serwisów w naszym przypadku sprawa jest dosyć prosta bo to są tylko i wyłącznie bazy danych ale rzeczywistości Abi może wyglądać tak że będzie jeszcze korzystało z innych serwisów wewnętrznych firmy. 

  

Są trzy podejścia w tej sytuacji. 

  

1. Mockowanie - Mocowanie To nic innego jak po prostu symulowanie działania danej implementacji w taki sposób w jaki chcemy
    
2. Stubs -  staps zakłada że stworzymy sobie sztuczną  sztuczną implementację danego obiektu i tej sztucznej implementacji będziemy używać zamiast tego docelowego obiektu
    
3. Testcontainers -  z kolei test containers to już jest bardziej zaawansowana rzecz bo tutaj zakładamy że tak naprawdę serwisy które wchodzą w interakcje z naszym API są deployowane  na i używane w ramach naszych testów.  aby to osiągnąć używamy właśnie kontenerów  które poznamy dopiero na następnym zjeździe i do tego tematu tam wtedy wrócimy pomysł wygląda atrakcyjnie są biblioteki do tego stworzone w javie w pythonie w javascripcie natomiast to nie jest takie proste jak się wydaje o tym powiem więcej na następnym zjeździe. 
    

  

My w ramach testów jednostkowych pozamy teraz technikę mocowania w przypadku testów integracyjnych poznamy czym są stapsy oraz Pokażę wam taki przedsmak test containers ale dokładnie temu zagadnieniu przyjrzymy się na drugim zjeździe.

  

Okej Wróćmy do naszego błędu Zobaczcie nie możemy połączyć się z naszą bazą danych Redis mocowanie jest to symulowanie pewnego zachowania więc to co my tu zrobimy to po prostu zasymulujemy że jednak z tą bazą się połączyliśmy. 

  

Aby to osiągnąć do naszej `initialized_app.py` W skrypcie `fixtures.py`  musimy dodać argument `packag_mocker`. W zależności od tego  jakiś scope będzie waszego `fixtures` to  to taki moker trzeba będzie wrzucić do funkcji u nas scope jest package więc używam `package_mocker`. Gdyby scope był inny to wtedy mamy inne `session_mocker`, `module_mocker` itd. 

  

```python

@pytest.fixture(scope="package")

def initialized_app(package_mocker):

   with TestClient(app) as client:

       yield client

```

  

Teraz Co to są osiągnąć to jest zasymulować połączenie z bazą danych musimy spojrzeć w takim razie jak ten Redis z Connector jest zaimplementowany i w którym momencie On te połączenie z bazą danych tworzy Zobaczmy w takim razie jego implementację. W momencie jak inicjalizacji jest uruchomiona funkcja Create Connection Zobaczmy jej działanie. Czyli tak naprawdę jej działanie jest to żeby stworzyć połączenie z bazą danych i wysłać pinga czyli  czyli sygnał  tą bazę ten aby sprawdzić czy faktycznie Ona została uruchomiona i to jest ten kawałek kodu który spowodował wywołanie naszego błędu który tutaj widzicie w konsoli.  jak to można zmockować. To co my chcemy zrobić to oszukać działanie funkcji `_create_connection` No tak żeby ona w ogóle nie rzuciła tym błędem. To pokażę wam od razu jak można to zrobić i potem wytłumaczę co się zadziało:

  

```python

@pytest.fixture(scope="package")

def initialized_app(package_mocker):

   package_mocker.patch.object(RedisConnector, "_create_connection", return_value=None)

   with TestClient(app) as client:

       yield client

```

  

Okej Spójrzmy Co my tu mamy to co ja zrobiłem to skorzystałem z takich metod jak Patrz Object z tego Social makera który właśnie pozwoli mi symulować te działanie dla klas Redis Connector który właśnie jest implementacją połączenia z naszym redisem Wskazałem na funkcje Create Connection i określiłem jaka wartość ma zwrócona jako None. Co to oznacza jeżeli nasz kod uruchomi funkcję Creed Connection z klasy re Dice Connector to zamiast wywołać całe jej działanie tak naprawdę od razu zwrócona zostanie wartość nocnej funkcji żeby wam łatwiej to zwizualizować Wróćmy do rady z konektora. I ten session mocker działa tak jakbyśmy na samym początku tej funkcji dodali `return None`. Praktycznie dalsza część funkcji zostanie wykonana. Dlaczego tak zrobiłem dlatego że po prostu my w testach jednostkowych  będziemy skupiać się tylko i wyłącznie na podstawowej funkcjonalności naszego API a wszelkie takie dodatkowe interakcje będą testowane w ramach testów integracyjnych. Oprócz utworzenia połączenia musimy też sobie zmonkować zamknięcie tego połączenia bo tam też jest taka metoda i Ona z kolei też rzuci błędem w momencie kiedy zakończymy testy więc od razu zróbmy to sobie 

  

```python

@pytest.fixture(scope="package")

def initialized_app(package_mocker):

   package_mocker.patch.object(RedisConnector, "_create_connection", return_value=None)

   package_mocker.patch.object(RedisConnector, "close", return_value=None)

   with TestClient(app) as client:

       yield client

```

  

Okej teraz spróbuj uruchomie nasze testy i zobaczmy jaką mamy teraz. Jak widzicie udało się tak naprawdę to co nam teraz pokazuje konsola to to że nie mam połączenia z podgresem więc tutaj analogicznie robimy tą samą sytuację -  symulujemy po prostu tak jakby te połączenie było. 

  

```python

@pytest.fixture(scope="package")

def initialized_app(package_mocker):

   package_mocker.patch.object(RedisConnector, "_create_connection", return_value=None)

   package_mocker.patch.object(RedisConnector, "close", return_value=None)

   package_mocker.patch.object(PostgresConnector, "_create_connection", return_value=None)

   package_mocker.patch.object(PostgresConnector, "_create_cursor", return_value=None)

   package_mocker.patch.object(PostgresConnector, "close", return_value=None)

   with TestClient(app) as client:

       yield client

```

  

Po zgraj się z tym filmowane są trzy w sumie funkcje tworzenie połączenia stworzenia kursora i funkcja zamykające połączenie więc od razu sobie to wszystko tutaj zmokujmy No i co zobaczymy czy działa. 

  

Jak widać udało się teraz już nie mamy żadnych błędów w momencie inicjalizacji naszego obiektu więc sprawa związana z połączeniem baz danych została rozwiązana . Słuchajcie jeżeli macie jakieś teraz pytania do tego albo coś niezrozumiałe to Poczekajcie jeszcze chwilę z tym jak napiszemy cały test i bo tam jeszcze będzie też kilka rzeczy które trzeba będzie zmontować i zobaczyć je sobie od początku do końca co się dzieje to wtedy Będziemy dyskutować na temat tego co było niejasne A co nie I wtedy jeszcze raz to wytłumaczę.

  

Okej to teraz przejdźmy sobie do naszego testu Teraz możemy w końcu go zacząć pisać. Spróbujmy na początek wysłać requesta do naszego API testowego I zobaczmy co się stanie:

  

```python

def test_decision_returned_correctly(initialized_app, request_body, request_headers, request_endpoint, mocker):

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

```

Niestety mamy błąd jak sobie spojrzymy teraz na tą treść błędu która jest dość Obszerna To zobaczymy że mamy problem taki że non Time czyli nasz No nie ma atrybutu Pink to wynika z tego że podczas inicjalizacji naszego testowego klienta tutaj w fixtures Zdefiniowaliśmy że po prostu w momencie wyłania tych funkcji są wrzucane nony i teraz podczas działania naszego API na tych nogach Wykonywane są po prostu wywołania konkretnych dalszych metod czyli Innymi słowy Jak przejdziemy sobie docisions to w momencie czytania z redisa jest coś takiego jak `response = None.read(request)` Oczywiście to nie ma prawa się udać I tak oto dostajemy treść błędu. To co musimy zrobić to zrobić podobną operację Tak jak już Zrobiliśmy to wcześniej czyli zmockować pewne funkcjonalności naszych klientów czytających dane z redisa i postgresa. Czyli to co musimy zrobić to po prostu zasymulować działanie metod read i write w naszych implementacjach. No i włączmy nasz test I zobaczmy co się stało. Jak widzicie udało się teraz ten test przeszedł Czyli tak naprawdę ten Response są zwrócony i możemy zacząć sprawdzać jego zawartość i upewniać  się że otrzymaliśmy poprawne dane takie jakie się spodziewamy. 

  

Dobra to teraz mamy już  najważniejszą logikę zaimplementowaną dostaliśmy odpowiedź naszego API teraz możemy wykonywać sprawdzenie to teraz przejdźmy sobie jeszcze raz to co się zadziało i dlaczego mockujemy to co teraz mockujemy. 

  

Wróćmy sobie do naszego main.py I zobaczmy na to jakie zdefiniowany endpoint decisions. My w ramach testów jednostkowych chcemy sprawdzić podstawowe funkcjonalności naszego API jak sobie spojrzymy teraz na to co stworzyliśmy w tym endpoint to tak naprawdę co jest podstawową funkcjonalnością naszego API tylko tak naprawdę dwie linijki kodu 

  

```python

@app.post("/decisions")

async def decisions(

   request: DecisionRequest,

   background_tasks: BackgroundTasks,

) -> DecisionResponse:

   # logging.info(f"Received {request=}")

   # response = app.state.decision_redis_client.read(request)

   # if response is None:

   decision = app.state.model.predict_decision(request.to_dataframe())[0]

   response = DecisionResponse(decision=decision)

       # background_tasks.add_task(

       #     func=write_to_redis,

       #     redis_client=app.state.decision_redis_client,

       #     request=request,

       #     response=response,

       # )

   #

   # background_tasks.add_task(

   #     func=write_to_postgres,

   #     postgres_client=app.state.decision_postgres_client,

   #     request=request,

   #     response=response,

   # )

   # logging.info(f"Returning {response=}")

   return response

```

  

Podstawową funkcjonalnością w API jest to że otrzymujemy dane nasz model wykonuje danej predykcje i je po prostu zwracamy to jest kolor działania naszego API A to że dodaliśmy do niego czytanie z redisa zapisywanie do redisa zapisywanie do postgresa  To są dodatkowe funkcjonalności które dodatkowo przyspieszają działanie API czy właśnie zapisują dane które później będziemy analizować ale to nie jest core działa jeszcze grabi.  znaczy tym główną funkcjonalności to jest po prostu wykonanie predykcji dla podanych danych i zwrócenie ich jako responds. I my właśnie w ramach testów jednostkowych chcemy sprawdzić tą podstawową funkcjonalność naszego API dlatego też te wszelkie dodatkowe funkcjonalności które tutaj są zakomendowane w tej chwili my nie chcemy żeby one brały udział w tych testach jednostkowych one będą testowane  w testach integracyjnych. Dlaczego też te mokowanie które dodaliśmy powoduje to że te dodatkowe funkcjonalności są jakby wyłączane i nie biorą nie są w ogóle brane pod uwagę. Czyli smakowaliśmy sobie połączenia z bazą Redis i postres bo nie jest w ogóle ona wykorzystywana oraz zlokowaliśmy sobie metody które powodują interakcje z tymi bazami danych czyli read iwrite bo też nie są w tej podstawowej funkcjonalności wykorzystywane. Zobaczcie że nie zmonkowaliśmy modelu to był model to jest Korona funkcjonalność naszego API on był potrzebny po to żeby ten tą odpowiedź zwrócić. Odkomentujmy to wszystko tak żeby Apis z powrotem działało poprawnie jak chcieliśmy. Teraz pytanie do was czy to tutaj stworzyliśmy jest dla was jasne Czy macie jakieś pytania?

  

To teraz wróćmy sobie do tego kodu z with i yield Żebyście mieli pewność jak to dokładnie działa. My nasze fixture `initialized_app`  użyliśmy w naszym  naszym teście i to działa tak że w momencie kiedy uruchomione są testy by test sprawdza jakie fiction są przekazywane do testu  i je tworzy. Czyli uruchamiany jest kod związany z `initialized_app`  w którym jest  wykonywane mocowanie i tworzony ten testowy obiekt `client`  który zwracany jest z klauzulą `yield`. Dzięki temu ten kod w tym Fik Szczur jest niejako by wstrzymywany,  dalej wykonywany jest kod związany z testem czyli nasze `test_decision_returned_correctly`. W momencie zakończenia testów,  wykonywanie z powrotem wraca do naszego `fixture` `initialized_app` i I gdyby coś było po `yield` To ten kod zostałby dalej wykonany, W naszym przypadku nic nie ma więc tak naprawdę oznacza to koniec działania klauzuli `with`  w efekcie czego nasze API kończy swoje działanie uruchamiając Shutdown skrypt. Dla łatwiejszego zrozumienia można zobaczyć przykład w dokumentacji pytest z usługą wysyłająca maile: [https://docs.pytest.org/en/6.2.x/fixture.html#yield-fixtures-recommended](https://docs.pytest.org/en/6.2.x/fixture.html#yield-fixtures-recommended)

  

Okej to teraz wróćmy naszego testu I zobaczmy jaki respon zostaliśmy w tym celu Dodajmy sobie breakpoint do naszego kodu I włączmy nasz test w trybie debuggera ale zanim to zrobimy to musimy  konfigurację tego testu czyli klikamy sobie po lewej stronie w ten nasz Zielony trójkąt Wybieramy modify run configuration I w working directory musimy wskazać na ścieżkę naszego projektu,  ale bez tych folderów testowych. Jutro znowu ten zielony trójkącik i uruchamiamy to w trybie debugera. Okej jak widzicie  powinno się to zatrzymać właśnie w breakpoincie. 

  

Okej to teraz to co musimy zrobić to sprawdzić czy odpowiedź zawiera te dane w takiej strukturze jakiej się spodziewamy otrzymać. No i w tym wypadku musimy posiłkować się znajomością nasze implementacji oraz spojrzeć na to co ona zwraca ale drugą ważną rzeczą jest też sprawdzenie dokumentacji naszego API. Niestety  nie mamy jak spojrzę teraz na dokumentację naszego API ponieważ  testowy klient stworzony podczas testów nie udostępniania dokumentacji więc musimy sobie troszkę przypomnieć jak to nasza odpowiedź z API wyglądała. Zresztą kilka razy ją widzieliśmy w konsoli więc myślę że mniej więcej pamiętacie co tam dostawaliśmy. Teraz rozpatrujemy przypadek poprawnie przetworzonego zapytania  w wyniku czego otrzymujemy decyzję.

  

 generalnie podczas sprawdzania odpowiedzi API Pierwsze co robimy to sprawdzamy czy jego status jest taki jakiś spodziewaliśmy jeżeli rozpatrujemy  test poprawnie przetworzonego zapytania To tutaj statusem musi być 200:

  

```python

assert response.status_code == 200

```

  

Czyli wykorzystujemy tutaj sobie nasz assert i upewniamy się że status równy jest 200.

  

Kolejna rzecz jest taka że upewniamy się czy dane przyszły w takiej strukturze jakieś spodziewamy w kontekście naszego API malowego odpowiedzi powinny być zwrócone w Jasona w takiej sytuacji przetwarzać później przez kolejne systemy czy przez osoby które zostały do naszego API a ten upewniamy się czy nasza odpowiedź faktycznie zwraca Jasona Można to sprawdzić ile razy nagłówków:

  

```python

assert response.headers.get('content-type') == 'application/json'

```

  

Jeżeli to sprawdzanie przejdzie to znaczy że nasza odpowiedź jest w tej sonie więc wyciągnijmy ją sobie 

  

```python

response_json = response.json()

```

  

 Jason Czyli po prostu struktura klucz wartość w naszym przypadku API zwraca takiego Jasona gdzie Kluczem jest słowo decision a wartości jest decyzja z modelu możemy to sobie sprawdzić w naszej implementacji pydanticowego modelu. Czyli upewniam się że pole Design istnieje Faktycznie w tym obiekcie:

  

```python

assert "decision" in response_json

```

  

 Okej czyli ten etap przejdzie to znaczy że znajduje że decyzja znajduje się w naszej strukturze jasonowej teraz musimy sprawdzić  wartość odpowiedzi z naszego API i teraz To się musimy chwileczkę zatrzymać. Jeżeli sobie podejrzymy Jaka jest odpowiedź dla naszego przypadku testowego, To jest to `DECLINE`. I teraz można by się pokusić o to żeby przyrównać czy faktycznie wartość jest równa `DECLINE` ale nie możemy tego zrobić. Tak bo powiedziała na początku wyzwanie w testowaniu odpowiedzi z API w którym zaimplementowany jest model uczenia maszynowego jest takie, że  że po pierwsze model może zwracać wyniki niepowtarzalne to będzie prawdziwe W szczególności w przypadku modeli językowych które tekst generują za każdym razem kiedy wyślecie te same zapytanie to odpowiedź będzie inna więc sprawdzanie w testach czy dana odpowiedź równa się danej odpowiedzi jest kompletnie bezsensowna to nigdy nie zadziała drugi przypadek jest taki że nawet jeżeli mamy model deterministyczny tak jak w tym przypadku regresja logistyczną która zawsze przy podaniu tych samych wartości będzie zwracać tą samą odpowiedź to musicie wziąć pod uwagę to że ten model może się zmienić w przyszłości na przykład  to wciąż będzie regresja Logistyczna ale o innych parametrach za tę decyzja się zmieni albo zupełnie będzie inny model który z racji faktu że to jest inny model też Też może zmienić decyzję dlatego jeżeli podejdziesz do tego tak że będziecie przyrównywać wartość do konkretnej wartości to Gwarantuję wam to że wasze testy będą przechodzić później jak się coś zmieni w modelu. Będziecie się łowić Co jest powodem Gdzie jest bak natomiast to będzie problem ze złą konstrukcją testu to co my musimy zrobić w kontekście API amelowego to to żeby sprawdzić właściwości tej odpowiedzi a nie jej konkretną wartość. O jakich właściwościach to możemy mówić no przede wszystkim to że dana odpowiedź na przykład jest stringiem albo dana odpowiedź ma maksymalnie długość taką i taką albo że dane wartości nie przekraczają jakieś wartości i tak dalej czy skupiamy się na pewnych No właśnie właściwościach odpowiedzi a nie konkretne jej wartości. W tym przypadku co możemy zrobić no przede wszystkim upewnijmy się  że nasza odpowiedź nie jest w ogóle pusta 

  

```python

assert response_json["decision"] is not None

```

Skoro nie jest pusta to możemy sprawdzić czy jest stringiem w tym przypadku

  

```python

assert isinstance(response_json["decision"], str)

```

  

Jeżeli będzie stringiem to musimy iść dalej upewnijmy się że string nie jest całkowicie pusty.

  

```python

assert response_json["decision"] != ""

``` 

  

Okej To teraz wyłączmy to bugara i usuń breakpointa z naszego kodu i włączmy nasze testy zobaczymy co dostaniemy. Jak widzicie mamy `test_decisions` przeszło to znaczy że wszystkie warunki zostały spełnione które to jest zdefiniowaliśmy.

  

czy macie pytania do tego co napisaliśmy A może ktoś z was widzi co jeszcze można było sprawdzić żeby upewnić się że na przykład odpowiedzi z API są poprawne? 

  

Tak wam wspomniałem na początku testów ja bym chciał się skupić na takiej głównej ideach tak tak żebyście mogli sobie poradzić po prostu z każdym przypadkiem. Oczywiście takich przypadków sprawdzających można mnożyć naprawdę ogranicza nas tylko wyłącznie wyobraźnia oraz Oraz to jak ty a pies z tobą zaimplementowane. Im więcej takich testów się napisze tym więcej nabierzesz takie intuicji co warto sprawdzić A co nie warto sprawdzić. I też musicie być gotowi na to że w trakcie pisania testów No nieraz patrzycie każdego przypadku jaki może zacząć się dosłownie waszym API. Zawsze kiedyś zdarzy się jakiś ecz Case który I został uwzględniony w waszych testach i dopiero taki Case wyszedł na produkcji to jest rzecz normalna która się zdarza każdemu jeżeli taka sytuacja zaistnieje To wy nauczeni tym doświadczeniem będzie nie pamiętać że taki Case War rozpocząć w testach po pierwsze a po drugie że takie sytuacje wtedy wraca się do testów i ten Case który bierze na produkcji dodatkowo się otestowuje jeszcze w testach jednostkowych czyli integracyjnych tak żeby on po prostu ponownie nam się nie pojawił.

  

To co rozpatrzyliśmy w ramach tego naszego pierwszego testu dotyczy tak zwanego przypadku pozytywnego czyli Innymi słowy sprawdzenie działania API wtedy kiedy przychodzi prawidłowy request i wtedy kiedy  API przetwarza nasze zapytanie bez żadnych problemów ale oprócz przypadków pozytywnych musimy również rozpatrzeć przypadki negatywne Czyli co się zadzieje w momencie kiedy API dostanie od nas błędne dane bądź co się zadzieje wtedy kiedy nasze API rzuci błędem w środku swojego działania i my też takie przypadki musimy rozpatrzeć i teraz sobie do nich przejdziemy. Ale zanim to zrobimy to uruchomimy sobie nasze API tak normalnie i Sprawdźmy co ona mówi dokumentacja 

  

Jak widzicie ten pozytywny przypadek który teraz przetestowaliśmy jest oświetlony tutaj na zielono jako Successfull response. Ale jak widać jest jeszcze validation error oraz to co nie jest wypisane ale w każdym serwisie może się zdarzyć czyli błąd 500 internal server error czyli musimy rozpatrzeć te dwa przypadki. 

  

Zacznijmy od tego naszego błędu 422 czyli mówiącego o tym że nasze dane wejściowe są złym w formacie. 

  

```python

def test_received_422_when_atleast_one_body_param_is_invalid(

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

):

  

```

Teraz popsujmy sobie nasze request body i Dodajmy jakiś jakąś wartość do zmiennej które nie istnieje A potem wyślijmy requesta.

  

No i teraz patrzę w dokumentację musimy napisać test i upewnić się czy faktycznie otrzymujemy to co powinniśmy otrzymać. Standardowe sprawdzenie status code i format odpowiedzi

  

```python

assert response.status_code == 422

assert response.headers.get("content-type") == "application/json"

response_json = response.json()

```

  

Dalej musimy się upewnić czy zawartość tego responsa jest taka powinna być 

  

Zaczynamy  od klucza `detail` czy znajduje się w odpowiedzi i dalej musimy sprawdzić czy ta struktura jest taka jaka jest dokumentacji. 

  

```python

assert "detail" in response_json

```

  

Dokumentacja wskazuje na to że pewne obiekty są a rajem więc tutaj w tym przypadku musimy skorzystać z biblioteki `collections.abc` aby sprawdzić tak faktycznie jest:

  

```python

response_detail = response_json["detail"]

assert isinstance(response_detail, collections.abc.Sequence)

assert len(response_detail) == 1

assert "loc" in response_detail[0]

assert isinstance(response_detail[0]["loc"], collections.abc.Sequence)

assert "msg" in response_detail[0]

assert "type" in response_detail[0]

```

  

I to moim zdaniem wyczerpuje już do sprawdzenie włączmy testy i zobaczymy czy się udało. Powinno się udać. Udało się.

  

Ale jeszcze ten minus z tym testem dlatego że Spójrzcie my tak naprawdę sprawdziliśmy tylko czy walidacja nie przechodzi dla zmiennej `foreign_worker`. A z kolei w modelu mamy pięć zmiennych więc musimy sprawdzić wszystkie 5 teraz jak to uzyskać. To jest stosunkowo proste skorzystamy tutaj z `pytest.mark.parametrize`, Która pozwala nam na uruchamianie naszych testów wielokrotnie w zależności od danych wejściowych. 

  

```python

@pytest.mark.parametrize(

   ("key", "value"),

   [

       ("installment_rate_in_percentage_of_disposable_income", -1),

       ("age_in_years", 10),

       ("age_in_years", 100),

       ("foreign_worker", "CATEGORY_NON_EXISTENT"),

       ("present_employment_since", "CATEGORY_NON_EXISTENT"),

       ("personal_status_and_sex", "CATEGORY_NON_EXISTENT"),

   ],

)

```

  

Bardzo ważne jest to żeby te wartości kiwali przekazać jako pierwszy parametry funkcji. A dasz argumenty To są nasze pytestowe fixtures. I zaktualizujemy kod odpowiedzialny za stworzenie body nieprawidłowego.

  

Włączmy testy i zobaczymy czy że faktycznie wszystkie case'y powinny przejść. 

  

Ale to nie są jeszcze wszystkie sprawdzania które warto zrobić w API ja jeszcze dodatkowo Sprawdzam odpowiedź W momencie kiedy wszystkie parametry są złe także mieć pewność że ten ta odpowiedź zwróci nam wszystkie informacje:

  

```python

def test_received_422_when_all_body_params_are_invalid(

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

):

   request_body_invalid = {

       "installment_rate_in_percentage_of_disposable_income": -1,

       "age_in_years": 10,

       "foreign_worker": "CATEGORY_NON_EXISTENT",

       "present_employment_since": "CATEGORY_NON_EXISTENT",

       "personal_status_and_sex": "CATEGORY_NON_EXISTENT",

   }

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body_invalid)

   assert response.status_code == 422

   assert response.headers.get("content-type") == "application/json"

   response_json = response.json()

   assert "detail" in response_json

   response_detail = response_json["detail"]

   assert isinstance(response_detail, collections.abc.Sequence)

   assert len(response_detail) == 5

```

  

 wtedy zrobiłem małą zmianę dlatego że zatrzymałem się tylko i wyłącznie na sprawdzaniu czy długość tej listy jest pięć  dlatego że każdy z parametrów był błędny nie sprawdzam zawartości poszczególnych elementów dlatego że jest to robione w poprzednim teście.

  

Jeśli chodzi o walidacje parametrów to prawie skońćzyliśmy. Spójrzmy sobie na dokumentację i Zwróćmy uwagę na dwie rzeczy. Mamy tam dwie zmienne  w których określone są przedziały jak widzicie one są domknięte te większe bądź równy 0 bądź 1865 przy czym 18 i 65 są akceptowalne.  jeżeli w API są takie warunki to musimy koniecznie sprawdzić czy przedziały bo mam doświadczenia że to jest najczęstszy bug w API.  więc to co my musimy zrobić de facto to po prostu powtórzyć test związany z tą ścieżką pozytywną Z taką zmianą że wpuścimy  requesty które zawierają te wartości brzegowe. Więc jest to niejako połączenie testu pozytywnego z pewnymi parametrami z testu negatywnego. 

  
  
  

```python

@pytest.mark.parametrize(

   ("key", "value"),

   [

       ("installment_rate_in_percentage_of_disposable_income", 0),

       ("age_in_years", 18),

       ("age_in_years", 65),

   ],

)

def test_decision_returned_correctly_for_boundary_values(

   key,

   value,

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

   mocker,

):

   request_body = request_body | {key: value}

   mocker.patch.object(BaseRedisClient, "read", return_value=None)

   mocker.patch.object(BaseRedisClient, "write", return_value=None)

   mocker.patch.object(BasePostgresClient, "write", return_value=None)

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

   assert response.status_code == 200

   assert response.headers.get("content-type") == "application/json"

   response_json = response.json()

   assert "decision" in response_json

   assert response_json["decision"] is not None

   assert isinstance(response_json["decision"], str)

   assert response_json["decision"] != ""

```

  

Jak widzicie stosuję długie nazwy w testach jednostkowych ale jest to  dobra praktyka be tak robić bo Zwróć uwagę na to jaki dostajecie wynik w konsoli  nazwa testów wprost wskazuje na to co on sprawdza i przy okazji jak używacie `pytest.mark.parametrize`  to wsadzone są również Wartości dla których są normalne testy.

  

Okej przypadek z błędnie przekazanymi wartościami do API został powtórzone zostało nam ostatni Case tutaj już nie będziemy aż tak bardzo się rozpisywać czyli  obsługa przypadków wtedy kiedy wystąpi jakiś błąd w naszym API.

  

Przy testowaniu tego przypadku musimy spojrzeć jak mamy zdefiniowane exception handlery bo to one definiują logikę zwracania odpowiedzi z API w momencie kiedy występuje jakieś błędy w naszym API.

  

Tego exception handlera dla `RequestValidationError` Już tak naprawdę przetestowaliśmy w momencie sprawdzenia inputu do naszego API nic nie musimy tego robić Zostały nam dwa czyli dla naszego APIError  oraz dla generalnego Exception. 

  

Utwórzmy sobie ten test

```python

def test_received_500_when_error_raised_in_api(

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

   mocker,

):

   mocker.patch.object(BaseRedisClient, "read", return_value=None)

   mocker.patch.object(BaseRedisClient, "write", return_value=None)

   mocker.patch.object(BasePostgresClient, "write", return_value=None)

```

  

Teraz to co my tu musimy dodatkowo zrobić że to  aby nasze API rzuciło jakąś błędem.  tutaj zrobimy to w ten sposób że zmokujemy sobie działanie naszego modelu i tym razem skorzystamy z argumentu `side_effect`  do którego podamy mu błąd `Exception`.  i w ten sposób w momencie użycia funkcji `predict_decision` po prostu rzucone zostanie `Exception` 

  

```python

mocker.patch.object(CreditScoringModel, "predict_decision", side_effect=Exception)

response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

assert response.status_code == 500

assert response.headers.get("content-type") == "application/json"

response_json = response.json()

assert "message" in response_json

assert response_json["message"] is not None

assert isinstance(response_json["message"], str)

assert response_json["message"] != ""

```

  

I Dodajmy sobie jeszcze `pytest.mark.parametrize` bo mamy 2 exception handlery 

  

```python

@pytest.mark.parametrize("error", [Exception, APIError])

def test_received_500_when_error_raised_in_api(

   error,

   initialized_app,

   request_body,

   request_headers,

   request_endpoint,

   mocker,

):

   mocker.patch.object(BaseRedisClient, "read", return_value=None)

   mocker.patch.object(BaseRedisClient, "write", return_value=None)

   mocker.patch.object(BasePostgresClient, "write", return_value=None)

   mocker.patch.object(CreditScoringModel, "predict_decision", side_effect=error)

   response = initialized_app.post(url=request_endpoint, headers=request_headers, json=request_body)

   assert response.status_code == 500

   assert response.headers.get("content-type") == "application/json"

   response_json = response.json()

   assert "message" in response_json

   assert response_json["message"] is not None

   assert isinstance(response_json["message"], str)

   assert response_json["message"] != ""

```

  

Okej Słuchajcie Moim zdaniem na tym zakończymy sekcję związaną z testami jednostkowymi Oczywiście tutaj jeszcze można pewnie mnożyć przypadki jak się dłużej tym zastanowimy ale uważam że to wam w zupełności wystarczy jako taki punkt startu i odniesienia bo mamy tutaj omówione najważniejsze rzeczy ścieżka pozytywną czyli sytuację w której nasz reques jest poprawnie przetwarzany pod warunkiem że przekażemy do niego poprawne dane i w naszym API  dostanie wszystko poprawnie przetworzone. Mamy obsłużone też przypadki kiedy to ktoś wyśle nam błędne dane i upewniamy się że faktycznie zwracamy mu poprawne odpowiedzi w takiej sytuacji. Mamy też obsłużone eczkejsy czyli ta walidacje naszych parametrów na ich wartościach brzegowych żeby mieć pewność że faktycznie to działa tak jak chcemy. No i przypadek gdy w naszym API  coś nie zadziała i tutaj zwracamy mu wtedy 500 z wiadomością.  Czy do tej części macie pytania Jeszcze zanim przejdziemy dalej do testów integracyjnych. 

  

Moim zdaniem testów nigdy za wiele jeżeli w trakcie ćwiczeń wpadnie wam jeszcze jakieś dodatkowy pomysł żeby też sprawdzić to jak najbardziej zrobię tak samo w swoich projektach możecie Jeszcze to lepiej o testować sprawdzić pewne warianty graniczne w zależności od tego jaki projekt wybierzecie generalnie tak jak widzicie pisząc testy drugi raz zastanawiamy się nad tym co ten nasz kod robi, co chcemy żeby zwrócił. To jest ten  ten jeden z benefitów o którym wam mówiłem na początku generalnie powiem wam ciekawostkę że czasami zdarza się nawet tak że pisanie testów może zająć wam więcej czasu niż przygotowanie całej implementacji to jest rzecz normalna. Mój rekord to pisanie cały tydzień test od poniedziałku do piątku implementacji którą przygotowałem w 3 dni. Ale naprawdę warto ten czas poświęcić po prostu Potraktujcie pisanie testów jak inwestycje długoterminową zwroty będą za jakiś czas wtedy kiedy będziecie dalej rozwijać wasz API zmieniać wnieść jakieś implementacje to testy obronią was przed tym żeby na produkcję wdrożyć coś co po prostu nie działa.

  

Okej to w takim razie czas rozpocząć testy integracyjne…

  
  

## MOŻE DODAJ TEŻ CO JESZCZE MOŻNA SPRAWDZIĆ W UNIT TESTACH ALE NIE ROZPISUJEMY WSZYSTKICH PRZYPADKÓW BO NIE STARCZY NA TO CZASU

  

https://www.zartis.com/api-testing-components-and-tips/

  

## MOŻE NA KONCU WSPOMNIJ JAK TO MOZNA POPRAWIĆ UŻYWAJĄC OBIEKTU Mock() ALBO MagicMock()**