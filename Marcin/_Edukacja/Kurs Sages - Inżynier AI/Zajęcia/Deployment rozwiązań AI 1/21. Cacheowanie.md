# Cacheowanie

1. Zakończyliśmy część podstawową, fundamentalną. Zaczynamy przechodzić do kwestii troche bardziej zaawansowanych, określiłbym je jako średnio zaawansowane. Zaczniemy sobie od bardzo ważnego tematu, jak cacheowanie.
2. Ale zanim to zaczniemy to najważniejsza rzecz moim zdaniem: implementując API, moim zdaniem powinny przyświęcać Wam dwa główne cele:
	1. proste: ogólnie API MLowe nie jest aż tak duże jak API np. serwisu typu CRUD bo obecnie mamy zdefiniowane w sumie 3 endpointy, na tylko 3 metody typu POST i to będzie w tym przypadku maksimum, więcej nie będzie. Natomiast niezależnie od ilości endpointów i reprezentacji jakie zdefiniujecie to moim zdaniem, z własnego doświadczenia, za prostotę API odpowiada w większości jakość dokumentacji. Możemy też API skomplikować samą implementacją użycia końcowek jak np. pokazywałem Wam ten sposób z jedną reprezentacją `predictions`, dla której odpowiedzi sterowaliśmy parametrami w `query`. Jeżeli ktoś jednak zdecyduje się na takie podejście, to aby było to proste w korzystaniu, ważna jest bardzo dobra, najlepiej łopatologiczna dokumentacja. Jeżeli dokumentacja będzie jasna i przejrzysta i nie będziemy mieli wątpliwości jak skorzystać z API, to API dla nas będzie po prostu proste i przyjemne. Fajne w FastAPI jest to, że dokumentacje generuje na podstawie naszego kodu. Będziemy jeszcze do niej wracać i modyfikować je ręcznie aby jeszcze lepiej je usprawnić, ale to później.
	2. szybkie: Z drugim aspektem, czyli szybkość, jest zdecydowanie trudniej. To już wymaga od nas dużej ilości pracy z naszym własnym kodem i zrozumienia jak on działa: ale też takich pojęć jak złożoność obliczeniowa - pojęcie najważniejsze w kontekście pisania szybkiego kodu, czy też wektoryzacja działań, w szczególności kiedy operujemy na wektorach. Są też, ja je określam jako "przyspieszacze kodu" czyli kompilatory JIT (Just In Time) jak np. [numba](https://numba.pydata.org/). My w tym zjeździe nie będziemy tych aspektów poruszać, bo to już bardziej tematyka zaawansowana, dotycząca zajęć związanych z optymalizacją kodu. Natomiast My w API możemy przyśpieszyć jego działanie, nie zmieniając kodu. Po prostu dołączając do API możliwość cacheowania.
3. O Cacheowanie wspomniałem przy okazji omawiania założeń architektury REST. Dzisiaj zaimplementujemy to sobie w naszym API.
4. Czym jest cachoewanie? No myślę, że każdy z Was się z tym terminem spotkań, tłumacząc na polski - trzymanie w pamięci. Za każdym razem kiedy przeglądarcie internet, w waszych przeglądarkach są cachoewane odpowiedzi/response zwracane przez konkretne strony.
5. Dlaczego warto cacheować? Dlatego, że to znacznie przyśpiesza, dlatego, że łatwiej jest po prostu sięgnąc do pamięci podręcznej i odczytać coś co już jest tam przechowywane niż ponownie renderować stronę i jej zawartość. (Pokaż na przykładzie np. stackoverflow.com - z cachem i bez cachea. Jest spadek z 500 ms na jakieś 300 ms.) Ktoś może powiedzieć, że co to te 500 ms i 300 ms. Nie odczuwalne dla użytkownika. Tak to prawda, dla nas to jest wręcz niezauważalne. Ale dla systemów, na których wdrożony jest stackoverflow z których korzysta setki tysięcy ludzi codziennie każda zaoszczędzona milisekunda to mniejsze obciążenie dla systemu. Pamiętajmy też o tym, że z stackoverflow mogą korzystać nie tylko poprzez renderowanie stron, ale ono ma też swoje API, z którego można wyciągnąć surowe dane o forach, treści postów, treści komentarzy i ich statystyki itd. I aplikacje, jakieś systemy zbudowane na bazie tego API też korzystają z tego, że szybciej dostaną jakąś odpowiedź z API. Innymi słowy, przyśpieszenie naszego API oznacza też to, że inne systemy, które z naszego API korzystają, też będą szybsze - korzyść jest ogromna.
6. Ok wiemy czym jest cache i jaki jest z niego benefit. To teraz wróćmy do naszego API MLowego i pójdzmy dalej. Teraz pytanie powstaje - jak stworzyć pamięć podręczna dla naszego API MLowego. W przypadku API MLowego potrzebujemy pewnej bazy danych, która posłuży nam po prostu jako pamięć podręczna, w której będziemy trzymać dane. Teraz co bęðziemy trzymać za dane w przypadku API MLowego? Sprawa jest banalna. W pamięci podręcznej będziemy trzymać predykcje naszego modelu dla konkretnych requestów. 
7. Jak taki cache będzie działał w naszym serwisie? Jeżeli przyjdzie do naszego API request z danymi, dla których już wcześniej wykonaliśmy predykcję, to prostu nie będziemy ponownie uruchamiać naszego modelu aby znowu wyliczyć te same predykcje, tylko po prostu sięgniemy do naszej bazy danych, służąca jako cache, i z niej wyciągniemy już gotową odpowiedź naszego modelu i tą gotową odpowiedź zwrócimy jako response z naszego API. Jeżeli natomiast danego requesta nie będzie w naszym cacheu, to wtedy uruchomimy model, żeby nam wygenerował predykcję, zwrócimy tą predykcje oraz zapiszemy tą odpowiedź w naszym cacheu i jeżeli w przyszłości przyjdzie znowu ten sam request do naszego API, to będziemy mieli już predykcje w cacheu i z cachea ją wyciagnięmy i zwrócimy. Sprawa jest bardzo prosta.
(Do tej pory nie ma żądnych pytań?)
8. Ok to teraz trudniejsza rzecz... Wspomniałem bazie danych. Pytanie jakie można sobie zadać to jaka baza danych? Rodzajów baz danych jest wiele - PostgresSQL, Oracle, Microsoft SQL Server, MySQL, MondoDB, Cassandra, HBase... mogę wymieniać ich wiele. Którą wybrać? Decyzje o wyborze bazy danych do cachoewania powinna kierować się jedną najważniejszą zasadą: szybkość wyciągnięcia danych z bazy, a będąc precyzyjnym czas Random Access (albo Direct Access - z takim terminem można się spotkać). Słuchajcie - nie chce tutaj za dużo mówić o rodzajach baz danych, bo o samych bazach danych i architekturze ich można robić oddzielne weekendy. Dlatego od razu przejdę do tego co jest standardem rynkowym używanym do cacheowania, powiem dlaczego jest on tak szeroko używany, dlaczego jest tak szybki. A Was zachęcam w wolnej chwili do poczytania o różnych bazach danych i ich architekturze, na codzień pracujemy z danymi, więc warto znać dokładnie jak są przechowywane. W szczególności jeśli pracujecie na chmurze, bo tam naliczana jest każda złotówka za każdy GB przetworzonych danych, więc im więcej wiecie o konkretnej bazie danych, tym łatwiej będzie napisać Wam efektywne zapytania SQLowe, żeby szybko dane wyciągnąć i przy okazji Wasza firma niezbankrutowała. 
9. Wracając do tematu: standardem rynkowym jest Redis (**RE**mote **DI**ctionary **S**ervice), o który na pewno wielu z Was słyszało. O Redisie też nie będę robił wielkiego wykładu, o architekturze Redisa poczytajcie w wolnej chwili, natomiast są dwa główne plusy Redisa, dlaczego jest on tak szeroko stosowany:
	1) Po pierwsze jest to baza key-value, a bazy key-value mają najszybszy czas Random Access. Krótko powiem: czym jest czas Random Access? Jest to czas jaki zajmuje wyciągnięcie danych o dowolnym rekordzie z bazy danych. Przy bazach np. relacyjnych, im więcej danych tym ten czas się wydłuża - indexy, klastrowanie czy partycjonowanie tabel może to przyśpieszyć, ale wciąż będzie przegrywać z bazami key-value. W przypadku bazach key-value, raz że Random Access jest najszybszy, a dwa, że czas Random Access pozostałe stały i nie zależy od ilości danych w bazie. Dla nas jest to bardzo istotne w cacheowaniu, dlatego, że nasz cache będzie rósł wraz z użyciem naszego API, a my podczas korzystania z niego będziemy sięgać po pojedynczy rekord (w naszym przypadku request) i dla tego rekordu chcemy uzyskać jaka była odpowiedź. 
	2) Po drugie: Redis to baza danych przechowująca dane w pamięci RAM, a nie na dysku. Sięgnięcie po dane, które przechowywane są w RAMie jest znacznie szybsze niż sięgniecie po dane przechowywane na dysku - HDD czy SSD. Sięgnięcie po dane przechowywane na RAMie liczone jest w nanosekundach, podczas gdy sięgnięcie podane na dysku (HDD czy SSD) liczone są w milisekundach, także mówimy tutaj o różnicy kilku rzędów wielkości - to bardzo dużo. 
Krótko mówiąc: Redis jest bardzo szybki. 
No i fakt, że jest szeroko stosowany oznacza też dla nas że mamy już gotowe bilbioteki, które pozwolą nam na prace z nim.
10. Aby ułatwić nam pracę, przygotowałem już skrypt bashowy, który lokalnie stawia nam baze Redisa. Wystarczy, że w terminalu uruchomimy:
```bash
make redis
```
Dostaliśmy informacje, że Redis działa i jest dostępny pod adresem 127.0.0.1 i porcie 6379. O IPkach i portach będę mówił później, tak jak już wspominałem. Niestety Redis nie ma żadnego GUI, więc nie podejrzmy go. Ale na pewno możemy się z nim połączyć, mamy u góry komunikat - Success - we can connect to redis on 127:0.0.1:6379.

11. Druga sprawa: aby zaoszczędzić nam czasu i aby skupić się w pełni na rzeczach związanych z API, przygotowałem krótką i prostą implementację pozwalającą nam na interakcje z Redis-em. Te implementacje znajdują się w `src/databases/redis`. 

Zacznijmy od krótkiego omówienia tej interakcji i wraz z tym będę Wam przedstawiał istotne szczegóły odnośnie Redisa. 

1. Zacznijmy od `src/databases/redis/connector` - tutaj sprawa jest bardzo prosta. Mamy klasę która odpowiedzialna będzie za przechowywanie połączenia z bazą redis. W pythonie jest biblioteka `redis`, która pozwala nam na interakcje z nim. Będziemy mieli atrybut `connection`, który trzyma w sobie połączenie z naszym redisem na danym






Strategie przechowywania requestów i response w Redis:
1. na pewno TTL
2. różne struktury danych w Redisie: na pewno hash vs zwykłe key:value 