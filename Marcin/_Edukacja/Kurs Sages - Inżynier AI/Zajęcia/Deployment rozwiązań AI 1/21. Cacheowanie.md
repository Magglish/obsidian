# Cacheowanie

1. Zakończyliśmy część podstawową, fundamentalną. Zaczynamy przechodzić do kwestii troche bardziej zaawansowanych, określiłbym je jako średnio zaawansowane. Zaczniemy sobie od bardzo ważnego tematu, jak cacheowanie.
2. Ale zanim to zaczniemy to najważniejsza rzecz: implementując API, moim zdaniem powinny przyświęcać Wam dwa główne cele:
	1. porstota: ogólnie API MLowe nie jest aż tak duże jak API np. serwisu typu CRUD bo obecnie mamy zdefiniowane w sumie 3 endpointy, na tylko 1 metode typu POST i to będzie w tym przypadku maksimum, więcej nie będzie. Natomiast niezależnie od ilości endpointów i reprezentacji jakie zdefiniujecie to moim zdaniem, z własnego doświadczenia, za prostotę API odpowiada w większości jakość dokumentacji. Możemy też API skomplikować samą implementacją użycia końcowek jak np. pokazywałem Wam ten sposób z jedną reprezentacją `predictions`, dla której odpowiedzi sterowaliśmy parametrami w `query`. Jeżeli ktoś jednak zdecyduje się na takie podejście, to aby było to proste w korzystaniu, ważna jest bardzo dobra, najlepiej łopatologiczna dokumentacja. Jeżeli dokumentacja będzie jasna i przejrzysta to nie będziemy mieli wątpliwości jak skorzystać z API, to API dla nas będzie po prostu proste i przyjemne. Fajne w FastAPI jest to, że dokumentacje generuje na podstawie naszego kodu. Będziemy jeszcze do niej wracać i modyfikować je ręcznie aby jeszcze lepiej je usprawnić, ale to później.
	2. szybkie: Z drugim aspektem, czyli szybkość, jest zdecydowanie trudniej. To już wymaga od nas dużej ilości pracy z naszym własnym kodem i zrozumienia jak on działa: ale też takich pojęć jak złożoność obliczeniowa - pojęcie najważniejsze w kontekście pisania szybkiego kodu, czy też wektoryzacja działań, w szczególności kiedy operujemy na wektorach. Są też, ja je określam jako "przyspieszacze kodu" czyli kompilatory JIT (Just In Time) jak np. [numba](https://numba.pydata.org/). My w tym zjeździe nie będziemy tych aspektów poruszać, bo to już bardziej tematyka zaawansowana, dotycząca zajęć związanych z optymalizacją kodu. Natomiast My w API możemy przyśpieszyć jego działanie, nie optymalizując naszego kodu. Po prostu dołączając do API możliwość cacheowania.
3. O Cacheowanie wspomniałem przy okazji omawiania założeń architektury REST. Dzisiaj zaimplementujemy to sobie w naszym API.
4. Czym jest cachoewanie? No myślę, że każdy z Was się z tym terminem spotkał, tłumacząc na polski - trzymanie w pamięci. Za każdym razem kiedy przeglądacie internet, w waszych przeglądarkach są cachoewane odpowiedzi/response zwracane przez konkretne strony.
5. Dlaczego warto cacheować? Dlatego, że to znacznie przyśpiesza, dlatego, że łatwiej jest po prostu sięgnąć do pamięci podręcznej i odczytać coś co już jest tam przechowywane niż ponownie renderować stronę i jej zawartość. 
6
   (Pokaż na przykładzie np. stackoverflow.com - z cachem i bez cachea. Jest spadek z 500 ms na jakieś 300 ms.) 
   
   Ktoś może powiedzieć, że co to te 500 ms i 300 ms. Nie odczuwalne dla użytkownika. Tak to prawda, dla nas to jest wręcz niezauważalne. Ale dla systemów, na których wdrożony jest stackoverflow z których korzysta setki tysięcy ludzi codziennie każda zaoszczędzona milisekunda to mniejsze obciążenie dla systemu. Mniejsze obciążenie systemu oznacza mniejsze zuzyćie CPU i RAM. A mniejsze zużycie CPU i RAM ostatecznie oznacza mniejsze pieniądze jakie musicie zapłacić np. jeżeli korzystacie z chmury, bo tam każda minuta wykorzystania CPU, czy RAM jest płatna.
   Pamiętajmy też o tym, że z stackoverflow mogą korzystać nie tylko poprzez renderowanie stron, ale ono ma też swoje API, z którego można wyciągnąć surowe dane o forach, treści postów, treści komentarzy i ich statystyki itd. I aplikacje, jakieś systemy zbudowane na bazie tego API też korzystają z tego, że szybciej dostaną jakąś odpowiedź z API. Innymi słowy, przyśpieszenie naszego API oznacza też to, że inne systemy, które z naszego API korzystają, też będą szybsze - korzyść jest ogromna - zarówno dla użytkoników jak i dla osób, które ten system zaimplementowały.
1. Ok wiemy czym jest cache i jaki jest z niego benefit. To teraz wróćmy do naszego API MLowego i pójdzmy dalej. Teraz pytanie powstaje - jak stworzyć pamięć podręczna dla naszego API MLowego. W przypadku API MLowego potrzebujemy pewnej bazy danych, która posłuży nam po prostu jako pamięć podręczna, w której będziemy trzymać dane. Teraz co bęðziemy trzymać za dane w przypadku API MLowego? Sprawa jest banalna. W pamięci podręcznej będziemy trzymać predykcje naszego modelu dla konkretnych requestów. 
2. Jak taki cache będzie działał w naszym serwisie? Jeżeli przyjdzie do naszego API request z danymi, dla których już wcześniej wykonaliśmy predykcję, to prostu nie będziemy ponownie uruchamiać naszego modelu aby znowu wyliczyć te same predykcje, tylko po prostu sięgniemy do naszej bazy danych, służąca jako cache, i z niej wyciągniemy już gotową odpowiedź naszego modelu i tą gotową odpowiedź zwrócimy jako response z naszego API. Jeżeli natomiast danego requesta nie będzie w naszym cacheu, to wtedy uruchomimy model, żeby nam wygenerował predykcję, zwrócimy tą predykcje oraz zapiszemy tą odpowiedź do naszego cacheu i jeżeli w przyszłości przyjdzie znowu ten sam request do naszego API, to będziemy mieli już predykcje w cacheu i z cachea ją wyciagnięmy i zwrócimy. Sprawa jest bardzo prosta.

(POKAŻ IM SLAJD)

BARDZO WAŻNE ZAPISYWANIE PREDYKCJI PO ZWRÓCENIU ODPOWIEDZI DO KLIENTA. PO TO ABY ON NIE CZEKAŁ NA TEN PROCES, BO PO CO? MAMY JUZ PREDYKCJE WIEC MU CHCEMY JA ZWRACAĆ, A SAMO ZAPISANIE DO CACHEA NIE POWINNO JUZ GO OBCHODZIC I WPŁYWAĆ NA ODPOWIEDŹ DO NIEGO.

(Do tej pory nie ma żądnych pytań?)
8. Ok to teraz trudniejsza rzecz... Wspomniałem bazie danych. Pytanie jakie można sobie zadać to jaka baza danych? Rodzajów baz danych jest wiele - PostgresSQL, Oracle, Microsoft SQL Server, MySQL, MondoDB, Cassandra, HBase... mogę wymieniać ich wiele. Którą wybrać? Decyzje o wyborze bazy danych do cachoewania powinna kierować się jedną najważniejszą zasadą: szybkość wyciągnięcia danych z bazy, a będąc precyzyjnym czas Random Access (albo Direct Access - z takim terminem można się spotkać). Słuchajcie - nie chce tutaj za dużo mówić o rodzajach baz danych, bo o samych bazach danych i architekturze ich można robić oddzielne weekendy. Dlatego od razu przejdę do tego co jest standardem rynkowym używanym do cacheowania, powiem dlaczego jest on tak szeroko używany, dlaczego jest tak szybki. A Was zachęcam w wolnej chwili do poczytania o różnych bazach danych i ich architekturze, na codzień pracujemy z danymi, więc warto znać dokładnie jak są przechowywane. W szczególności jeśli pracujecie na chmurze, bo tam naliczana jest każda złotówka za każdy GB przetworzonych danych, więc im więcej wiecie o konkretnej bazie danych, tym łatwiej będzie napisać Wam efektywne zapytania SQLowe, żeby szybko dane wyciągnąć i przy okazji Wasza firma niezbankrutowała. 
9. Wracając do tematu: standardem rynkowym jest Redis (**RE**mote **DI**ctionary **S**ervice), o który na pewno wielu z Was słyszało. O Redisie też nie będę robił wielkiego wykładu, o architekturze Redisa poczytajcie w wolnej chwili, natomiast są dwa główne plusy Redisa, dlaczego jest on tak szeroko stosowany:
	1) Po pierwsze jest to baza key-value, a bazy key-value mają najszybszy czas Random Access. Krótko powiem: czym jest czas Random Access? Jest to czas jaki zajmuje wyciągnięcie danych o dowolnym rekordzie z bazy danych. Przy bazach np. relacyjnych, im więcej danych tym ten czas się wydłuża - indexy, klastrowanie czy partycjonowanie tabel może to przyśpieszyć, ale wciąż będzie przegrywać z bazami key-value. W przypadku bazach key-value, raz że Random Access jest najszybszy, a dwa, że czas Random Access pozostałe stały i nie zależy od ilości danych w bazie. Złożoność obliczeniowa w Redisie jest O(1) - czyli generalnie ilość rekordów w cacheu nie wpływa na czas wyciągnięcia danych z Redisa. Dla nas jest to bardzo istotne w cacheowaniu, dlatego, że nasz cache będzie rósł wraz z użyciem naszego API, a my podczas korzystania z niego będziemy sięgać po pojedynczy rekord (w naszym przypadku request) i dla tego rekordu chcemy uzyskać jaka była odpowiedź. 
	2) Po drugie: Redis to baza danych przechowująca dane w pamięci RAM, a nie na dysku. Sięgnięcie po dane, które przechowywane są w RAMie jest znacznie szybsze niż sięgniecie po dane przechowywane na dysku - HDD czy SSD. Sięgnięcie po dane przechowywane na RAMie liczone jest w nanosekundach, podczas gdy sięgnięcie podane na dysku (HDD czy SSD) liczone są w milisekundach, także mówimy tutaj o różnicy kilku rzędów wielkości - to bardzo dużo. 
Krótko mówiąc: Redis jest bardzo szybki. 
No i fakt, że jest szeroko stosowany oznacza też dla nas że mamy już gotowe bilbioteki, które pozwolą nam na prace z nim.
10. Aby ułatwić nam pracę, przygotowałem już skrypt bashowy, który lokalnie stawia nam baze Redisa. Wystarczy, że w terminalu uruchomimy:
```bash
make redis
```
Dostaliśmy informacje, że Redis działa i jest dostępny pod adresem 127.0.0.1 i porcie 6379. O IPkach i portach będę mówił później, tak jak już wspominałem. Niestety Redis nie ma żadnego GUI, więc nie podejrzmy go. Ale na pewno możemy się z nim połączyć, mamy u góry komunikat - Success - we can connect to redis on 127:0.0.1:6379.

11. Druga sprawa: aby zaoszczędzić nam czasu i aby skupić się w pełni na rzeczach związanych z API, przygotowałem krótką i prostą implementację pozwalającą nam na interakcje z Redis-em. Te implementacje znajdują się w `src/databases/redis`. 

Zacznijmy od krótkiego omówienia tej interakcji i wraz z tym będę Wam przedstawiał istotne szczegóły odnośnie Redisa. 

1. Zacznijmy od `src/databases/redis/connection/connector` - tutaj sprawa jest bardzo prosta. Mamy klasę która odpowiedzialna będzie za przechowywanie połączenia z bazą redis. W pythonie jest biblioteka `redis`, która pozwala nam na interakcje z nim. Będziemy mieli atrybut `connection`, który trzyma w sobie połączenie z naszym redisem na danym hoście i porcie. I bardzo ważna rzecz, możliwość zamknięcia tego połączenia.
2. Dalej, przejdźmy sobie do `src/databases/redis/clients/base.py` - tutaj mamy implementację, która pozwala nam na interakcje z redisem, czyli zapisywaniem i wyciąganiem kluczy. Jak widać jest to abstrakcja, dlatego, że w zależności od tego czy to będzie decyzja, czy to będzie prawdopodobieńśtwo czy kategorie ryzyka - czyli różne outputy z modelu - wymagają one troche innych operacji, ale to za chwilę. 
   Inicjujemy go podając połączenie. I tak na prawdę mamy dwie metody `read` i `write` - `read` wyciąga nam dane z Redisa, a `write` zapisuje nam dane w Redisie. 
   
   Zacznijmy od omówienia `write`, bo potem omówienie `read` będzie łatwiejsze. 
   Tak jak wam mówiłem, baza Redis jest to key - value, czyli przechowujemy klucz a dla niego jakąś wartość - tak samo jak pythonowy `dict`. W przypadku klucza niestety ale Redis akceptuje tylko i wyłącznie dwa typy danych, które mogą być kluczem - jest to `string` albo `bytes`. Teraz pytanie powstaje jak możemy zapisać nasze requesty do bazy danych Redis, skoro my nie mamy tutaj stringa, tylko właśnie obiekt typu `BaseRequest`, który defacto jest strukturą danych zdefiniowaną za pomocą `pydantica`. To nie jest możliwe. 
   
   Aby wam pokazać różne możliwości wrzuce sobie przykłady do konsoli:
   
```python
import redis
from src.service.schemas.requests import DecisionRequest
import pickle

client = redis.Redis()

request = DecisionRequest(**{  
    "installment_rate_in_percentage_of_disposable_income": 0.25,  
    "age_in_years": 30,  
    "foreign_worker": "yes",  
    "present_employment_since": "unemployed",  
    "personal_status_and_sex": "male: single",  
})
```
   
   Gdybyśmy chcieli zapisać taki obiekt, to redis zwróci nam błąd. 

```python
client.set(request, 1)
```

   Więc z naszego obiektu request musimy stworzyć w jakiś sposób klucz - może to być bytes, string, inty lub floaty:

   1. Możemy zamienić nasz obiekt request na str poprzez `str(request)` i wrzucić go do bazy `client.set(str(request), 1)` i jak widzicie dostajemy `True` udało się. Możemy to sprawdzić poprzez `client.get(str(request))`
   2. Możemy zamienić nasz obiekt na `json` poprzez `request.json()` i wrzucić go do bazy `client.set(request.json(), 2)` i jak widzicie dostajemy `True` udało się. Możemy to sprawdzić poprzez `client.get(request.json())`. Ale tak na prawde jak sie przyjrzymy to ten json jest... stringiem -> `type(request.json())`
   3. Możemy też zpickleować ten obiekt `pickle.dumps(request)` w efekcie czego otrzymujemy zakodowany obiekt bytes i też nam to zadziała `client.set(pickle.dumps(request), 3)` i sprawdźmy to: jest ok `client.get(pickle.dumps(request))`.
   4. I jest jeszcze jedna metodą, żeby nie używać `str(request)` a `repr(request)` to też będzie string ale zaraz powiem jaka jest zaleta tego podejścia.

Czyli możemy nasz obiekt `request` przechować jako string, jako JSON albo jako byte poprzez pickle. Pytanie powstaje, który wariant wybrać? Tak na prawdę nie ma jednoznacznej odpowiedzi na to pytanie, bo to wszystko zależy oczywiście od tego co chcemy w tej bazie zapisać. Na pewno istotne jest to, że baza Redis, która urzęduje sobie w pamięci RAM będzie miała ograniczoną przestrzeń. Mój komputer ma 64 GB RAMu, a dysk ma 1TB. To ogromna różnica. W chmurze RAMu możecie mieć ile chcecie, ale pamiętajcie że RAM jest znacznie droższy niż dyski, zatem ze względu na to, bazy Redis mają zdecydowanie mniej przeznaczonej przestrzeni niż inne bazy danych, które operują na dyskach. Zatem można sobie zadać pytanie: możemy wybierzmy taki rodzaj klucza, który zajmuje najmniej przestrzeni:

```python
import sys
print(f'Request zapisany jako string: za pomocą str(...) = {sys.getsizeof(str(request))} bajtów')
print(f'Request zapisany jako json = {sys.getsizeof(request.json())} bajtów')
print(f'Request zapisany jako bytes = {sys.getsizeof(pickle.dumps(request))} bajtów')
print(f'Request zapisany jako string: za pomocą repr(...) = {sys.getsizeof(repr(request))} bajtów')
```

Kwestia jest następująca: skoro Redis działa na RAMIE, RAM jest ograniczony + RAM jest drogi, to najlepiej wybrać taki format, który zajmuje najmniej miejsca. Natomiast okazuje się, że Redis ma w sobie mechanizmy optymalizacji miejsca, ma swoje własne mechanizmy przechowywania danych w sposób dla niego optymalny. My możemy postarać się mu pomóc w tym troche wybierając format, który teoretycznie jest najmniejszy, ale nie zawsze gwarantuje to sukces. Z mojego punktu widzenia i doświadczenia, nie przejmowałbym się tym w jakiej formie jest klucz, bo to niepotrzebne wchodzenie w bardzo niskopoziome aspekty, które za bardzo nam nic nie dzadzą, bo tak jak mówiłem: Redis sam wie jak najlepiej to przechować. Po za tym zobaczcie, że jeżeli zapiszecie coś  do Redisa `client.set(str(request), 123)` i potem to wyciągniecie to nie otrzymujecie inta tak jak to zostało zapisane, a w bajtach. Czyli widzimy, że on wewnętrznie te dane przechowuje "po swojemu". 

Wniosek z tego jest taki: nie skupiajcie się na formacie i nie pomagajcie redisowi. On sam w sobie ma mechanizmy optymalizacji i serializacji miejsca, i zrobi to lepiej za nas. Wybierzcie ten format, który Wam w danym momencie po prostu odpowiada.

Generalnie best-practises w przechowywaniu danych w Redisie jest format `JSON`, czyli `request.json()`. Ale: 1) nie wszystko można zamienić na JSON. I w takiej sytuacji jedynym rozwiązaniem jest `pickle`-owanie. 2) Sprawa nawet jeżeli można to zamienić na JSONa, tak jak w naszym przypadku, to jednak te rozwiązanie ma pewną wadę. Skopiuje sobie przykłady, żeby Wam pokazać o co mi chodzi:

```python
from src.service.schemas.requests import DecisionRequest, ProbabilityRequest, RiskCategoryRequest

decision_request = DecisionRequest(**{    
"installment_rate_in_percentage_of_disposable_income": 0.25,    
"age_in_years": 30,    
"foreign_worker": "yes",    
"present_employment_since": "unemployed",    
"personal_status_and_sex": "male: single",  })

risk_category_request = RiskCategoryRequest(**{    
"installment_rate_in_percentage_of_disposable_income": 0.25,    
"age_in_years": 30,    
"foreign_worker": "yes",    
"present_employment_since": "unemployed",    
"personal_status_and_sex": "male: single",  })

probability_request = ProbabilityRequest(**{    
"installment_rate_in_percentage_of_disposable_income": 0.25,    
"age_in_years": 30,    
"foreign_worker": "yes",    
"present_employment_since": "unemployed",    
"personal_status_and_sex": "male: single",  })
```

Mamy teraz trzy requesty: `decision_request`, `risk_category_request` oraz `probability_request`. Czyli można to zinterpretować tak, że ktoś nasze API odpytał 3 razy, za każdym razem inny endpoint, ale z tymi samymi danymi wejściowymi. Teraz gdybyśmy chcieli zapisać odpowiedzi na te requesty w naszej bazie Redis korzystająć z klucza, który będzie json-em to problem jest taki, że te klucze są takie same. 
`decision_request.json()`, `probability_request.json()`, `decision_request.json() == probability_request.json()`. Teraz co to dla nas oznacza:

1. Jeżeli odpyta endpoint `/decisions` i dostaniemy `decision_request` i załóżmy, że nie ma go w cache, to po zwróceniu predykcji odpowiedź o decyzji zostanie zapisana w bazie -> `client.set(decision_request.json(), 'ACCEPT')` - załóżmy, że jest to accept.
2. I teraz jeżeli on odpytałby dalej `/risk_categories` to dostaniemy teraz `risk_category_request` z tymi samymi danymi i będziemy chcieli sprawdzić czy taką odpowiedź mamy już w cacheu, a naszym kluczem jest json `risk_category_request.json()`. I okaże się, że takie coś już jest w bazie i w efekcie czego otrzymamy... `client.get(risk_category_request.json())` -> ACCEPT, a nie naszą kategorię ryzyka. I to jest już błąd - zwróciliśmy inną odpowiedź niż to co API miało zwrócić. 

Ok, więc skoro nie JSON to możeby to zpicklować. To jest rozwiązanie: bo jak porównamy sobie: 

`pickle.dumps(decision_request)` z `pickle.dumps(risk_category_request)` to te wartości się od siebie różnią bo tutaj w tych stringach są podane nazwy klas i tym sie różnią, więc klucze będą inne. Przy podejściu z picklem spotkacie się z krytyką tego sposobu: wiele osób wskaże, że jest takie ryzyko, że możecie w ten sposób uruchomić jakiś kod. Ok.... ale Redis będzie albo założony przez Was, albo przez kolegę z zespołu na potrzeby tego API i będzie dostępny tylko wewnątrz waszej firmy, więc ryzyko, że ktoś wrzuci w Redisa jakiś zpickleowany kod, który ma Wam coś zepsuć jest zerowy. To już zależy od Waszego podejścia i polityki firmy. Założmy jednak, że nam to nie przeszkadza. Natomiast jest drugi argument przeciw to taki, że wersje `pickle` zmieniają się w czasie i ja się spotkałem w swoim życiu z problemami z odpicklowaniem jakiś wartości, więc jest ryzyko, że coś może być nie tak.

Jest rozwiązanie tych problemów, które mi w mojej pracy bardzo dobrze się sprawdza i rozwiązuje problem, jaki wskazałem przy JSONie i nie jest to obiekt zpicklowany, więc nie narusza zasad security itd. Sposób jest następujący - zamiast użyć `str(decision_request)` używam `repr(risk_category_request)` - `repr` to funkcja zwraca nam reprezentacje obiektu w stringu, ale w taki sposób aby można go było odtworzyć. Jeżeli porównamy sobie `repr(decision_request)` z `repr(risk_category_request)` to widzimy, że to są inne stringi i różnia się na początku nazwą klasy, więc nie powinno być tego problemu jak z `.json()`: `client.set(repr(decision_request), 'DECLINE')` i potem `client.get(repr(risk_category_request))`. I taką też implementację zastosowałem w metodzie `_create_key`: z requesta tworzy stringa używając `repr`. Dlastego też zdefiniowałem sobie `DecisionRequest`, `RiskCategoryRequest` i `ProbabilityRequest` aby właśnie móc użyć tego sposobu. 

W programowaniu często trzeba wybierać kompromisy - prostota i czytelność vs. czas działani kodu vs. zużycie pamięci. Dla mnie ten kompromis z `repr` sprawdza się bardzo dobrze i moja rekomendacja jest właśnie taka. Natomiast decyzję ostateczną, jak będą wyglądać wasze klucze w bazie Redis pozostawiam Wam, tylko miejcie proszę na uwadze te problemy i wyzwania, które wcześniej omawiałem.

Ok omówiliśmy sobie jak można zdefiniować nasz klucz `key` to teraz pytanie co z `value`:

```python
from src.service.schemas.responses import DecisionResponse  
decision_response = DecisionResponse(decision='ACCEPT')
```
Mamy nasz `decision_request` i `decision_response`. Spróbujmy zatem spróbować to zapisać w bazie Redis:

```python
client.set(repr(decision_request), decision_response)
```

Niestety ten sam problem - musimy zamienić nasz `DecisionResponse` na obiekt string, bytes, float lub int. Tutaj już nie będę się tak rozwlekał nad tym, bo sprawa jest analogiczna jak w przypadku ustawienia klucza. Ja w przypadku wartości wszystko sprowadzam do jednego typu: bytes. Dlaczego? To wynika z tego, że łatwiej się pracuje na czymś jeżeli wiesz, że przychodzi do Ciebie zawsze w jednym formacie, a nie w 4 różnych. Tutaj jak widzicie redis podpowiada, że akceptuje bytes, string, int i floaty. Ja wszystkie stringi, inti floaty zamieniam i przechowuje jako bytes. Więc mam jeden format wspólny. W dodatku jeżeli będę chciał przechowywać pythonowy dict albo tuple albo liste... cokolwiek - to będę musiał i tak to zamienić na bytes. Więc bytes pozwala na przechowanie wszystkiego i w dodatku będzie to jeden format. I tak też zdefiniowałem metody `_create_value` oraz `_create_response`, które pracują bezpośrednio tylko na `bytes`, a nie na wszystkich typach na raz `str`, `float`, i jeszcze `int` to by znacznie mogło skomplikować kod.  

Teraz to co pokazuje to klasa abstrakcyjna, więc w modułąch `decision.py`, `probability.py` oraz `risk_category.py` zdefiniowałem to jak te wartości mają być kodowane na `bytes`. Nie ma w nich wielkiej filozofii (omów krótko co tam jest).

Ok jak już wiemy jak stworzyć klucze i wartości, które możemy zapisać w Redisie. To czas z tego skorzystać. Przejdźmy do naszego `main.py`.

Zaimportujmy nasze obiekty:
```python
from src.databases.redis.connection.connector import RedisConnector  
from src.databases.redis.clients.decision import DecisionRedisClient  
from src.databases.redis.clients.risk_category import RiskCategoryRedisClient  
from src.databases.redis.clients.probability import ProbabilityRedisClient
```

I zainicjujmy je

```python
redis_connector = RedisConnector()  
decision_redis_client = DecisionRedisClient(redis_connector)  
risk_category_redis_client = RiskCategoryRedisClient(redis_connector)  
probability_redis_client = ProbabilityRedisClient(redis_connector)
```

Zmodyfikujmy sobie na początek endpoint `decisions`:

```python
@app.post("/decisions")  
async def decisions(request: DecisionRequest) -> DecisionResponse:  
    response = decision_redis_client.read(request)  
    if response is not None:  
        print("Cache hit")  
        return response  
  
    print("Cache miss")  
	decision = model.predict_decision(request.to_dataframe())[0]  
	response = DecisionResponse(decision=decision) 
    decision_redis_client.write(request, response)  
    return response
```

W pozostałych endpointach musielibyśmy ten sam kod napisać natomiast NA RAZIE zakomentujmy je. Zajmiemy sie nim wtedy kiedy bedziemy omawiać jak w API zarządzać wieloma endpointami. A na razie skupmy się na jednym endpoicie, bo i tak pozostałe będą działać praktycznie tak samo.

Sprawdźmy czy działa: `make api` i wyślijmy request `make request` i zobaczmy, że pierwszy request powinien wyprintować nam `Cache miss`, ale każdy kolejny `Cache hit`
czyli, że sięgamy cache'a.

Dobra to mamy ten cache. To teraz sprawdźmy czy gra była warta świeczki i sprawdźmy czasy odpowiedzi API z cachem i bez cache'a.
Sprawdźmy czy faktycznie mamy uzysk w czasie:

(Skopiuj ten kod żeby nie tracić czasu na pisanie go)

```python
import time  
  
start = time.time()  
response = requests.post(url=url, headers=headers, json=data)  
end = time.time()  
print(f"Elapsed = {(end - start) * 1000:.8f} ms")
```
Zmieńmy wartość w `data` w `sen_example_request.py` i wykonajmy kilka requestów i zobaczmy czasy.

Mamy różnice w 50% w przypadku tego API. Ale dla modeli tekstowych czy obrazkowych różnica będzie znacznie bardziej dostrzegalna. Dla GPT2 predykcje będa trwały kilka do kilkanaście sekund, więc Redis dla takich przypadków da naprawde ogromny uzysk.

Ok to wyrzućmy te wszystkie printy i inne niepotrzebne rzeczy.

(Powinno być)

```python
@app.post("/decisions")  
async def decisions(request: DecisionRequest) -> DecisionResponse:  
    response = decision_redis_client.read(request)  
    if response is not None:  
        return response  
  
	decision = model.predict_decision(request.to_dataframe())[0]  
	response = DecisionResponse(decision=decision) 
    decision_redis_client.write(request, response)  
    return response
```

Tak jak mówiłem, pozostałymi endpointami zajmiemy się później - i tak w ich przypadku implementacja będzie analogiczna. 

I na koniec istotne rzeczy o bazie Redis, które moim zdaniem inżynier ML powinien wiedzieć. Jak widzicie, przygotowałem prosty skrypt bashowy do postawienia Redisa - po to aby zaoszczędzić nam wszystkim czasu. Pytanie jakie możecie sobie zadać, to czy inżynier ML będzie w swojej pracy stawiał Redisa czy może on już będzie zarządzany przez jakiś inny zespół. Wiecie co, to wszystko zależy od skali, zespołów z jakimi będzie współpracować. Możecie bedzie tego Redisa sami stawiać, a może ktoś inny wam będzie go deployował. Poza tym po kolejnym zjeździe o Dockerze i potem po Kubernetesie zobaczycie, że deployowanie swoich własnych baz jak Redis jest proste i sami możecie to zrobić. Nie mniej jednak, niezależnie czy Wy sami będziecie zarządzać Redisem czy ktoś inny musicie wiedzieć dwie ważne rzeczy:

1. [Struktury danych w Redisie](https://redis.com/redis-enterprise/data-structures/): My w tej implementacji skorzystaliśmy z tej pierwszej struktury Redis String. Czyli nasz klucz jest stringiem i dla danego klucza jest jakaś wartość. Natomiast tych struktur danych, w których możemy przechowywać nasze requesty i odpowiedzi z modelu jest wiele. Wszystko to zależy od tego no z jakimi danymi pracujecie. Tak jak Wam wspomniałem wcześniej, w przypadku kluczy, czy przechować klucz jako string, a może jako int, a może jako float - to nie ma wielkiego znaczenia. Natomiast to jaką strukturę danych wykorzystacie to ma znaczenie, bardzo duże. Ja w swojej implementacji użyłem tego, dlatego, że to jest Wasz pierwszy kontakt z budową API MLowym i cacheowaniem. I jeżeli ktoś zaczyna, warto po prostu skorzystać z podstawowej struktury w redisie czyli po prostu Redis String. Natomiast jak nabierze już więcej doświadczenia i ogłady i będziecie chcieli zoptymalizować zużycie pamięci w Redisie, to strukturą która Wam w tym pomoże są Hashe - mamy hasha, sub-key (podklucz) i wartość. 
   Żebyście łatwiej to zrozumieli, pokaże Wam na naszym przykładzie:

(Tutaj trzeba zrobić obrazek w prezce)

Obecnie skorzystaliśmy ze struktury danych Redis String - czyli każdy request i response jest oddzielnym rekordem w bazie Redis. 

Rozwiązaniem naszego case'u, w którym mamy 3 różne struktury danych i jak widziliście użycie `.json()` nie odpowiada, jest skorzystanie z Hashes, bo jak możemy przeczytać w opisie:

*"Redis Hashes structure stores a set of field-value pairs designed not to take up much space, making them ideal for representing data objects."*

Natomiast wersja z Hashem jest taka, że mamy jeden hash, który w tym przypadku może być zwykłym JSONem, zawierającym dane wejściowe w requescie
A podklucze oznaczają to co to jest - czy jest to decyzja, risk_category lub probability i dla tych podkluczy mamy wartość odpowiedzi. I to jest JEDEN rekord, a nie TRZY rekordy. W dodatku struktura Hash może mieć różna długość. Czyli np. to oznacza, że ktoś dla tych danych wejściowych odpytał wszystkie 3 endpointy i te dane są w cache. A np. dla tych wartości danych wejściowych ktoś odpytał tylko 1 raz i mamy wtedy jeden podklucz. Jak ktoś dla tych wartości odpytaj inny endpoint, to wtedy tutaj się pojawi drugi podklucz. Jaki jest uzysk w pamięci? Z tego co się uda wyczytać w dokumentacji Redisa i na przykładach innych implementacji, uzysk jest około ~3 krotny. Czyli podejście z hashem zajmuje 3x mniej miejsca w bazie Redis niż podejście z Redis String, czyli innymi słowy przechowanie w cache 3 razy więcej odpowiedzi z modelu dla danych requestów. Ale tak jak mówiłem, możecie się nad tym zastanowić jak nabierzecie już wiecej doświadczenia i ogłady z Redisem i samą implementacją API. Aha i ta struktura jest oczywiście warta użycia jeśli musicie zapisać więcej odpowiedzi niż 1. 

Pokaż im przykład -> https://redis.io/docs/data-types/hashes/

W `"bike:1"` wchodzi wtedy request.json(), a pole `mapping` uzupełni się w zależności od tego własnie jaki endpoint jest odpytany.

Przykład:

```python
# ustawienie
client.hset(name=decision_request.json(), key='decision', value='ACCEPT')
client.hset(name=probability_request.json(), key='probability', value=0.23)
client.hset(name=risk_category_request.json(), key='risk_category', value='low_risk')

# odpytanie
client.hget(risk_category_request.json(), 'risk_category')
client.hget(probability_request.json(), 'probability')
client.hget(decision_request.json(), 'decision')
```

Natomiast w związku z tym, że jest to pierwszy kontakt z Redisem przez Was, chciałbym żebyśmy zostali przy strukturze Redis Strings -> bo tą strukturę wykorzystamy i przy ćwiczeniach i przy obrazkach. Natomiast te hashe, które Wam tutaj pokazałem jest przydatny tylko i wyłącznie jak macie właśnie model z wieloma endpointami tak jak tutaj, więc jest to specyficzny case. Natomiast ja chce żebyśmy skupili się na czymś bardziej generalnym, z czym najczęściej się spotkacie i co Wam się najcześciej przyda.

1. [To jak Redis zarządza rekordami w momencie kiedy kończy się pamięć](https://redis.io/docs/reference/eviction/): Redis ma ograniczoną pamięć. Pytanie powstaje: co się dzieje kiedy kończy się pamięć w Redisie? To zależy. Domyślnie jest ustawione, że jeśli pamięc w Redisie się skończy to dostaniecie błąd że już więcej nie możemy zapisać. ALE Redis pozwala Wam na ustawienie strategii poradzenia sobie z tą sytuacją. Jeżeli spojrzymy sobie do dokumentacji to mamy różne rodzaje takich strategii. (Omów krótko strategie). Teraz którą strategie wybrać? To nie jest proste pytanie. Bo to wszystko zależy od tego jak Wasze API jest używane, co zaimplementowaliście, jakie to ma przełożenie na biznes. Natomiast jeśli miałby zarekomendować dla Was jakąś strategię taką domyślną (bo domyślnie ustawione jest `noeviction` co spowoduje, że cache przestanie działać) to mi się najbardziej sprawdza `allkeys-lfu`. Natomiast można to też wybrać analitycznie: w cache istnieją takie metryki jak `Cache hit` i `Cache miss` czyli informacja o tym czy sięgneliśmy do cache'a oraz że do tego cache nie sięgneliśmy - czyli te same printy, które wcześniej zostały w kodzie zdefiniowane. Czyli analitycznie, można po prostu uruchomić API i zobaczyć po pewnym dla których strategii mamy najwyższy współczynnik `cache hit` do `cache miss` - po prostu chcemy żeby cache był jak najczęściej używany, no bo jak sami widzicie to po prostu znacznie przyśpieszy działanie Waszego API.

I to by było na tyle jeśli chodzi o temat cache'owania. Czy są jakieś pytania zanim pójdziemy dalej do kolejnego zagadnienia?