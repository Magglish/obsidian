# Różne sposoby udostępniania modeli
1. Strasznie się rozgadałem o samym API, ale to jest bardzo ważne bo na tym będziemy skupiać się na naszych zjazdach, ale pytanie jakie może powstać w Waszej głowie to czy są inne sposoby na udostępnianie naszych modeli? 
2. Zhardkowany model - to jest pewien sposób udostępnienia modelu, czyli zintegrowania naszego modelu z jakimś systemem po prostu hardkodując jego postać w systemie. Przykład z życia: w mojej pierwszej pracy w banku, gdzie budowałem modele ryzyka kredytowego, to taki model, który był modelem liniowym, konkretnie regresją logistyczną, był on "wdrażany" w innym systemie w języku C# po prostu zapisując jego postać funkcyjną i sumując iloczyny wag przez cechy i otrzymując w ten sposób predykcję. Może się tak zdarzyć, że takie "wdrażanie" modeli spotkacie, ale to raczej w jakichś firmach z archaicznymi systemami. Jakie to ma wady: przede wszystkim jesteście ograniczeni do bardzo prostych modeli, które można zapisać łatwo jako przemnożenie wag przez cechy (czyli modele liniowe) czy też stworzyć ifologie (czyli wszelkie drzewa decyzyjne), ale bardziej zaawansowane agorytmy jak sieci neuronowe odpadają. Czy to jest zły sposób? Jest takie powiedzenie - jeśli coś jest głupie, a działa to to nie jest głupie. Aczkolwiek ciężko tutaj mówić o jakimkolwiek "wdrożeniu", bo to nie będzie w żaden sposób przypominać wdrożenia jakiego będziemy się uczyć na tych 4 zajęciach.
3. Jako zapisany obiekt - Innym sposobem z jakim możecie się spotkać to po prostu zapisać model do jakiegoś obiektu i go komuś przekazać, np. kolegom z działu IT i niech to wdrażają. Ale to jest bardzo zły pomysł chociażby z prostego powodu - nie ma informacji o tym jakie biblioteki są wymagane aby uruchomić ten zpicklowany obiekt. Nawet jeśli dostarczylibyśmy razem z modelem listę pakietów, które wymagane by były do tego aby taki obiekt zpicklowany wczytać i używać, to jest ogromne ryzyko, że system w którym te bilbioteki zostaną zainstalowane może mieć konfilikty z bilbiotekami i rozwalić nam całe środowisko. Powiem szczerze, że ja się z takim sposobem nigdy nie spotkałem w rzeczywistości i całe szczęście, ale w wielu różnych dyskusjach chociażby na Reddicie można spotkać się z opisem sytuacji gdy naprawdę Data Scientist przekazywał zpicklowany model do MLowca i "masz go wdróż", albo po prostu wysyłał notebooka z odtworzeniem modelu i myślał, że to wystarczy. 
   Natomiast są pewne wyjątki - przykładem tego są strony z modelami open-source jak np. hugging face i przykładowy model [stable diffusion](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0). Ale to jest zupełnie inny case niż będziemy skupiać się w ogóle na tym kursie - tutaj mamy przykład modeli stworzony w ramach prac R&D i ich celem było 1) oczywiście użycie go w jakimś produkcie/systemie ale również 2) upowszechnienie ich, aby inni mogli z tego skorzystać - i w przypadku takiego upowszechnienia najlepszym sposobem jest po prostu podzielenie się kodem jak i stworzonym i zapisanym modelem. Natomiast My skupiamy się na budowie systemu opartego o machine learning i w takim kontekście raczej nie spotkacie się (a nawet jeśli się spotkacie, to źle że tak się robi) z udostępnianiem modelu jako wysyłanie komuś zapisanych plików.
4. Jako paczka pythonowa - Zatem można się zastanowić, czy powyższy problem możnaby jakoś poprawić? Pomysł do głowy może przyjść następujący - a co gdybyśmy sobie stworzyli paczkę pythonową z naszego projektu w którym ten model zbudowaliśmy i moglibyśmy go udostępnić. To jest jakieś rozwiązanie. (Przykładowe kody w  [stable diffusion](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)) Bo tak na prawdę instalując paczkę Pythonową istalujemy wszystkie potrzebne zalezności. W dodatku możemy zaimplementować pewne klasy, które implementują logikę działania naszych modeli. Do tego pobierzmy sobie jeszcze model w wersji zpicklowanej i powinno zadziałać. Niestety nie jest to takie kolorowe z dwóch powodów:
	1. przy wdrażaniu sieci neuronowych biblioteki pytorch i tensorflow mogą potrzebować jakichś dodatkowych komponentów w systemie (np. sterowników CUDA gdybyśmy chcieli je uruchamiać na GPU), których w paczce pythonowej po prostu nie uda się zawrzeć
	2. nie unikniemy problemu z pakietami - jeżeli zainstaluje paczke pythonową z moim modelem, to musze doinstalować wszystkie zależności. A co jeżeli np. nasz system produkcyjny wciąż działą na pythonie 2.7, a modele zostały nauczone na pythonie 3.10? Python 2.7 nie jest już wspierany przez Python Software Foundation, ale np. RedHat który tworzy płatnego Linuxa, wciąż dostarcza update'y do Pythona 2.7. Są pewne systemy, które są tak duże, że przepisanie ich na wyższą wersje Pythona to ogromne przedsięwzięcie. U mnie w pracy są takie elementy. Wiecie... tutaj nawet nie musi chodzić o Pythona 2.7, ale nawet system stworzony i działający jeszcze w Pythonie 3.7 może mieć problemy z naszym modelem stworzonym w Pythonie 3.10.
5. Jako job, offline, batchowy - Ok to może nie udostępniajmy modelu, poprzez przekazywani go komuś, tylko może udostępnijmy jego wyniki. Stwórzmy może jakiś skrypt, który będzie przez nas uruchamiany (job), który będzie uruchamiany poprzez jakiś scheduler raz dziennie/raz w tygodniu (offiline) i będzie przetwarzał jakąś porcję danych (batch) i zapisywał to na jakimś storage albo w bazie danych. Te rozwiązanie jest powszechnie stosowane i spotkacie sie z nim praktycznie wszędzie. Z tym podejściem spotkacie się często w połączeniu z serwisem API do modeli i one nawzajem się uzupełniają. Spotkacie się z takim podejściem najczęściej w dwóch wariantach:
	1. Joby, offline, batchowe - wykonują już wszystkie operacje przeliczające predykcje (np. w nocy) a podczas korzystania z API przez klientów/czy inne serwisy w naszym systemie (np. w ciągu dnia), tak na prawde API pobiera już gotowe predykcje i tylko je wysyła klientowi/serwerom, które z API korzystają. Robi się to po to aby zaoszczędzić na czasie przetwarzania zapytań przez API, ale w sytuacji gdy generowanie predykcji zajmuje naprawde sporo czasu. Dzięki temu korzystanie z  API było przyjemniejsze dla klientów czy tez po prostu szybsze dla innych serwisów w naszym systemie aby szybciej działały - nikt nie lubi czekać.
	2. Druga sytaucja dotyczy już nie konkretnie predykcji, a zmiennych. Są pewne cechy, które liczą się bardzo długo, a pewne cechy bardzo szybko. Zatem joby w offline np. w nocy przeliczają cechy, które się normalnie długą liczą, a API podczas generowania predykcji z modeli pewne cechy przelicza wewnątrz serwisu, a te cechy, które zostały wcześniej przeliczone po prostu pobiera ich wartości i na podstawie tych wszystkich cech wykonywana jest predykcja. U mnie w pracy tak jest rozwiązywana kwestia przetwarzania danych na obrazkach: codziennie rano pobieram grafiki reklam z facebooka naszych klientów, które są przetwarzanie w dwóch krokach: w 1) kroku modele sieci neuronowych tworzą cechy z obrazków, tzw. embeddingi, o których będziecie się uczyć na zajęciach z deep learningu i w 2) kroku są dwa modele, jeden który wykrywa gdzie znajduje sie tekst na obrazku i drugi zczytuje tekst z tego miejsca. Na końcu informacje w ten sposób wyciągnięte są potem zapisane do baz danych i w momencie korzystania z API są cechy, które są przetwarzane w serwisie, a cechy związane z obrazkami pobierane są z bazy dla danego przypadku - w ten sposób zaoszczędamy bardzo dużo czasu na samym procesie generowania predykcji.
6. Jako interaktywna aplikacja - Często bywa, że w ramach prototypowania naszego rozwiązania chcielibyśmy przygotować jakieś demo, w ramach którego możemy zademonstrować działanie naszego modelu. To jest często spotykana praktyka, często też jako koniec jakiegoś PoC, czy po prostu przeprowadzenia testów z potencjalnymi klientami. Często przynosi to forme dashboardu, w którym są jakieś interaktywne elementy wywołujące określone akcje i wśród nich może być coś co wykorzysta nasz model. Czasami jest tak, że cały kod modelu i jego obiekty są zintegrowane z tą aplikacją - czyli jakby w środku tej aplikacji w jej "bebechach" siedzi model. Np. takie testy robiłem w poprzednim roku gdzie chcieliśmy zobaczyć jak klienci będą korzystać z naszego modelu, zebrać od nich feedback itd. zanim zaczniemy projektować główną aplikacje. Bardzo często do takich celów wykorzystywana jest bardzo prosta biblioteka streamlit i jeżeli ktoś z Was będzie planował tworzyć takie dema działania waszych modeli, to polecam się jej przyjrzeć.
7. Jako część pipelineu - Innym sposobem udostępniania naszego modelu jest zintegrowanie go z pipelinem, który np. przetwarza dane ze źródła i zapisuje je gdzieś w bazie. Podczas tego przetwarzania można dodać krok, w którym wykonywane są predykcje naszego modelu i są one razem z danymi zapisywane. W rzeczywistości z takimi przypadkami również się spotkacie: u mnie w pracy w momencie pobierania komentarzy i postów z facebooka od razu są one dodatkowo wzbogacane o sentyment z naszych modeli MLowych. 
8. Jako serwis API - no i ostatni sposób, czyli jako serwis API o czym wcześniej mówiłem dużo. 

Podsumowując, z jakimi jeszcze innymi sposobami udostępniania modeli się spotkacie:
1. Przede wszystkim serwis API oraz joby przeliczające predykcje modeli. O jobach wspomne troche więcej przy okazji następnego zjazdu przy okazji kontenerów, ale jak sie sami przekonacie temat jest bardzo prosty i nie wymaga to oddzielnego zjazdu czy wielu godzin tłumaczenia. 
2. Jeśli spotkacie się z z interaktywnymi aplikacjami demonstrującymi działanie waszych modeli czy też pipeline'ami w ramach którego jest krok wzbogacania danych o predykcje z modeli, to raczej w pipeline czy też w aplikacji demo nie będzie kodu na Wasz model wraz ze wszystkimi bibliotekami potrzebnymi do użycia itd. bo to znacznie komplikuje te komponenty i przedewszystkim koszty uruchomienia. Jeśli z takimi rzeczami się spotkacie, albo po prostu przyjdzie Wam je tworzyć to najczęściej jest tak, że najpierw tworzony jest serwis API, a potem aplikacja demonstracyjna czy pipeline po prostu w trakcie swojego działania korzysta z tego serwisu.
3. 