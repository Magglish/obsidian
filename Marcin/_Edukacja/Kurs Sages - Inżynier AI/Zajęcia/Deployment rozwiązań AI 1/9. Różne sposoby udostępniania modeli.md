# Różne sposoby udostępniania modeli
1. Zjazd w ten weekend opiera się o budowe API do modeli - budowa API do modeli to jest preferowany sposób udostępnienia modelu, ale chciałbyn na wstępie wskazać Wam jakie są potencjalne rodzaje udostępniania modeli, jak często się z nimi spotkać i jakie są z nimi problemy:
2. Zhardkowany model - to jest pewien sposób udostępnienia modelu, czyli zintegrowania naszego modelu z jakimś systemem po prostu hardkodując jego postać w systemie. Przykład z życia: w mojej pierwszej pracy w banku, gdzie budowałem modele ryzyka kredytowego, to taki model, który był modelem liniowym, konkretnie regresją logistyczną, był on "wdrażany" w innym systemie w języku C# po prostu zapisując jego postać funkcyjną i sumując iloczyny wag przez cechy i otrzymując w ten sposób predykcję. Może się tak zdarzyć, że takie "wdrażanie" modeli spotkacie, ale to raczej w jakichś firmach z archaicznymi systemami. Jakie to ma wady: przede wszystkim jesteście ograniczeni do bardzo prostych modeli, które można zapisać łatwo jako przemnożenie wag przez cechy (czyli modele liniowe) czy też stworzyć ifologie (czyli wszelkie drzewa decyzyjne), ale bardziej zaawansowane agorytmy jak sieci neuronowe odpadają. Czy to jest zły sposób? Jeśli coś jest głupie, a działa to to nie jest głupie. Aczkolwiek ciężko tutaj mówić o jakimkolwiek "wdrożeniu", bo to nie będzie w żaden sposób przypominać wdrożenia jakiego będziemy się uczyć na tych 4 zajęciach.
4. Jako zapisany obiekt - Innym sposobem z jakim możecie się spotkać to po prostu zapisać model do jakiegoś obiektu i go komuś przekazać, np. kolegom z działu IT i niech to wdrażają. Ale to jest bardzo zły pomysł chociażby z prostego powodu - nie ma informacji o tym jakie biblioteki są wymagane aby uruchomić ten zpicklowany obiekt. Nawet jeśli dostarczylibyśmy razem z modelem listę pakietów, które wymagane by były do tego aby taki obiekt zpicklowany wczytać i używać, to jest ogromne ryzyko, że system w którym te bilbioteki zostaną zainstalowane może mieć konfilikty z bilbiotekami i rozwalić nam całe środowisko. Powiem szczerze, że ja się z takim sposobem nigdy nie spotkałem w rzeczywistości i całe szczęście, ale w wielu różnych dyskusjach chociażby na Reddicie można spotkać się z opisem sytuacji gdy naprawdę Data Scientist przekazywał zpicklowany model do MLowca i "masz go wdróż", albo po prostu wysyłał notebooka z odtworzeniem modelu i myślał, że to wystarczy. 
5. Jako paczka pythonowa - Zatem można się zastanowić, czy powyższy problem możnaby jakoś poprawić? Pomysł do głowy może przyjść następujący - a co gdybyśmy sobie stworzyli paczkę pythonową z naszego projektu w którym ten model zbudowaliśmy i moglibyśmy go udostępnić. To jest jakieś rozwiązanie. Bo tak na prawdę instalując paczkę Pythonową istalujemy wszystkie potrzebne zalezności. W dodatku możemy zaimplementować pewne klasy, które implementują logikę działa naszych modeli. Do tego pobierzmy sobie jeszcze model w wersji zpicklowanej i powinno zadziałać. Niestety nie jest to takie kolorowe z dwóch powodów:
	1. przy wdrażaniu sieci neuronowych biblioteki pytorch i tensorflow mogą potrzebować jakichś dodatkowych komponentów w systemie (np. sterowników CUDA gdybyśmy chcieli je uruchamiać na GPU), których w paczce pythonowej po prostu nie uda się zawrzeć
	2. nie unikniemy problemu z pakietami - jeżeli zainstaluje paczke pythonową z moim modelem, to musze doinstalować wszystkie zależności. A co jeżeli np. nasz system produkcyjny wciąż działą na pythonie 2.7, a modele zostały nauczone na pythonie 3.10? Python 2.7 nie jest już wspierany przez Python Software Foundation, ale np. RedHat który tworzy płatnego Linuxa, wciąż dostarcza update'y do Pythona 2.7. Są pewne systemy, które są tak duże, że przepisanie ich na wyższą wersje Pythona to ogromne przedsięwzięcie. U mnie w pracy są takie elementy. Wiecie... tutaj nawet nie musi chodzić o Pythona 2.7, ale nawet system stworzony i działający jeszcze w Pythonie 3.7 może mieć problemy z naszym modelem stworzonym w Pythonie 3.10.
6. Jako job, offline, batchowy - Ok to może nie udostępniajmy modelu, poprzez przekazywani go komuś, tylko może udostępnijmy jego wyniki. Stwórzmy może jakiś skrypt, który będzie przez nas uruchamiany (job), który będzie uruchamiany poprzez jakiś scheduler raz dziennie/raz w tygodniu (offiline) i będzie przetwarzał jakąś porcję danych (batch) i zapisywał to na jakimś storage albo w bazie danych. Te rozwiązanie jest powszechnie stosowane i spotkacie sie z nim praktycznie wszędzie. Z tym podejściem spotkacie się często w połączeniu z serwisem API do modeli i one nawzajem się uzupełniają. Spotkacie się z takim podejściem najczęściej w dwóch wariantach:
	1. Joby, offline, batchowe - wykonują już wszystkie operacje przeliczające predykcje (np. w nocy) a podczas korzystania z API przez klientów/czy inne serwisy w naszym systemie (np. w ciągu dnia), tak na prawde API pobiera już gotowe predykcje i tylko je wysyła klientowi/serwerom, które z API korzystają. Robi się to po to aby zaoszczędzić na czasie przetwarzania zapytań przez API, po to aby korzystanie z API było przyjemniejsze dla klientów/serwisów - nikt nie lubi czekać.
	2. Druga sytaucja dotyczy już nie konkretnie predykcji, a zmiennych. Są pewne cechy, które liczą się bardzo długo, a pewne cechy bardzo szybko. Zatem joby w offline np. w nocy przeliczają cechy, które się normalnie długą liczą, a API podczas generowania predykcji z modeli pewne cechy przelicza wewnątrz serwisu, a te cechy, które zostały wcześniej przeliczone po prostu pobiera ich wartości i na podstawie tych wszystkich cech wykonywana jest predykcja. U mnie w pracy tak jest rozwiązywana kwestia przetwarzania danych na obrazkach: codziennie rano pobieram grafiki reklam z facebooka naszych klientów, które są przetwarzanie w dwóch krokach: w 1) kroku modele sieci neuronowych tworzą cechy z obrazków, tzw. embeddingi, o których będziecie się uczyć na zajęciach z deep learningu i w 2) kroku są dwa modele, jeden który wykrywa gdzie znajduje sie tekst na obrazku i drugi zczytuje tekst z tego miejsca. Na końcu informacje w ten sposób wyciągnięte w ten sposób są potem zapisane do baz danych i w momencie korzystania z API są cechy, które są przetwarzanie w serwisie, a cechy związane z obrazkami pobierane są z bazy dla danego przypadku - w ten sposób zaoszczędamy bardzo dużo czasu na samym procesie generowania predykcji.
7. Jako część pipelineu - Innym sposobem udostępniania naszego modelu jest zintegrowanie go z pipelinem, który np. przetwarza dane ze źródła i zapisuje je gdzieś w bazie. Podczas tego przetwarzania można dodać krok, w którym wykonywane są predykcje naszego modelu i są one razem z danymi zapisywane. W rzeczywistości z takimi przypadkami również się spotkacie: u mnie w pracy w momencie pobierania komentarzy i postów z facebooka od razu są one dodatkowo wzbogacane o sentyment z naszych modeli MLowych. Ale jeśli spotkacie się z czymś takim, to raczej w pipeline nie będzie kodu na Wasz model + wszystkie biblioteki 
8. Jako interaktywna aplikacja - 
9. Jako serwis API - 


10. Dlaczego rozwiązanie API jest wybierane w praktycznie wszystkich przypadkach? 
	1. jest to oddzielny serwis/usługa/komponent w naszym systemie - inni developerzy czy użytkownicy API nie musza nic wiedzieć o tym jak zostało to zaimplementowane. Mają dokumentację co mają wysłać do API aby otrzymąc wyniki z modelu
	2. Jest to oddzielny serwis, zarząðzany przez Nas - wszelkie zmiany i rozwój dotyczy tylko i wyłącznie naszego serwisu. Gdyby model był zintegrowany z jakimś pipelinem/rozwiązanami (TODO: dopisz/popraw po napisaniu całego akapitu) to aktualizacja naszych rzeczy mogłaby spowodować to, że inne przestaną działać, np. aktualizacja biblioteki.
	3. Z reguły API mają konkretne cele - w naszym przypadku nasze API po prostu zwraca wyniki modelu. Wydzielenie takiego API jako mniejszy, oddzielny komponent łatwiej pozwala nam tym wszystkim zarząðzać i wprowadzać poprawki bo wprowadzamy je konkretnie do danego elementu.
	4. Oddzielny serwis/usługa w postaci API pozwala na integrowanie tego z wieloma różnymi systemami.