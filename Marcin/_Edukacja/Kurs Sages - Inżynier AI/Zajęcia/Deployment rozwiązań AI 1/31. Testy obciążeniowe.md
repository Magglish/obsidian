# Testy obciążeniowe

**DO TESTÓW TYCH BEDZIE POTRZEBNY GENRATOR PRZYKŁADOWYCH REQUESTÓW. POWINIEN BYC W KODZIE ALE GDYBY GO BRAKOWAŁO TO:**

```python
import sys  
import pydantic  
  
from src.features.categories import (  
    CategoryEnum,  
)  
from typing import TypeVar  
import random  
from src.service.schemas.requests import BaseRequest  
  
Category = TypeVar("Category", bound=CategoryEnum)  
Request = TypeVar("Request", bound=BaseRequest)  
  
  
class ExampleRequestGenerator:  
    """Generates example requests with data for given request class."""  
  
    def __init__(self, request_cls: type[Request]) -> None:  
        """  
        Initalization of example request generator.  
        Args:            request_cls: Request class for which we want to generate example requests with fake data.        """        self.request_cls = request_cls  
        self.request_fields = request_cls.__fields__  
  
    def generate(self) -> Request:  
        """  
        Generates one example request with fake data  
        Returns:            Request object.        """        generated_values = {}  
        for field_name, field in self.request_fields.items():  
            generated_values[field_name] = self._generate_value(field)  
        return self.request_cls(**generated_values)  
  
    def _generate_value(self, field: pydantic.fields.ModelField) -> str | float | int:  
        """  
        Generates value for given field.  
        Args:            field: Details about the field in request class.  
        Returns:            String, float or int - depends on the field.        """        generation_functions = {  
            pydantic.types.ConstrainedInt: self._generate_int_value,  
            pydantic.types.ConstrainedFloat: self._generate_float_value,  
            CategoryEnum: self._generate_category_value,  
        }        return next((func(field) for type_, func in generation_functions.items() if issubclass(field.type_, type_)))  
  
    @staticmethod  
    def _generate_category_value(field: pydantic.fields.ModelField) -> str:  
        """  
        Generates example category value from category field.  
        Args:            field: Details about the category field in request class.  
        Returns:            Random category        """        categories = list(field.type_.mapping().values())  
        return random.choice(categories)  
  
    def _generate_int_value(self, field: pydantic.fields.ModelField) -> int:  
        """  
        Generates example int value from integer field with respect to the min and max value constraints.  
        Args:            field: Details about the integer field in request class.  
        Returns:            Random int value        """        return random.randint(*self._check_min_max_value(field))  
  
    def _generate_float_value(self, field: pydantic.fields.ModelField) -> float:  
        """  
        Generates example float value from float field with respect to the min and max value constraints.  
        Args:            field: Details about the float field in request class.  
        Returns:            Random float value        """        return random.uniform(*self._check_min_max_value(field))  
  
    @staticmethod  
    def _check_min_max_value(field: pydantic.fields.ModelField) -> tuple[int | float, int | float]:  
        """  
        Checks min and max value in numeric fields.  
        Logic:        1. If there is no limit, set the minimum and maximum values to:            Minium = -9223372036854775807            Maximum = 9223372036854775807        Which is the `sys.maxsize` - maximum and minimum value that your computer can generate.        2. If there is a `greater/less than` and `greater/less and equal than` set up together,            we choose the `greater/less than` for min and max values.        3. In all other cases, we choose `greater/less than` or `greater/less and equal than`            (depending on which is set) for min and max values.  
        Args:            field: Details about the float field in request class.  
        Returns:            Tuple with (min, max) value        """        min_value = -sys.maxsize  
        if field.type_.ge is not None:  
            min_value = field.type_.ge  
        if field.type_.gt is not None:  
            min_value = field.type_.gt  
  
        max_value = sys.maxsize  
        if field.type_.le is not None:  
            max_value = field.type_.le  
        if field.type_.lt is not None:  
            max_value = field.type_.lt  
        return min_value, max_value
```

Okej przechodzimy do ostatnich testów testów obciążeniowych które w przypadku API MLowych są bardzo istotne, dlatego że powiedzą wam wprost to jak szybko otrzymujecie odpowiedź z API w zależności od obciążenia.

Generalnie przy testach obciążeniowych i w ogóle podczas analizy szybkości działania naszego API rozważamy dwie metryki

1. latency - Pierwsza z nich to jest latency czyli po prostu opóźnienie przy przetworzeniu pojedynczego requestu. Już to wcześniej widzieliśmy na poprzednim etapie jak kodziliśmy, że to było około dwóch milisekund.
2. throughput - Druga z nich to jest throughput Czyli po polsku można by to nazwać przepustowość,  czyli ile requestów jesteśmy w stanie przetworzyć w jakiejś jednostce czasu - minucie, sekundzie, godzinie - w zależności od tego co was interesuje Przeważnie jest to liczone jako ilość requestów na sekundę.

Oczywiście z jednej metryki można przejść na drugą bo w naszym przypadku latency jest w wysokości 2 ms oznacza 500 requestów na sekunde I vice versa. Ale generalnie patrzy się zawsze na te dwie metryki Łącznie. 

W swojej pracy na co dzień w zależności od tego gdzie będzie wdrażać API na pewno będziecie mieli jakieś wymagania co do tego w jakim czasie chcemy otrzymać odpowiedź, z wielu powodów: 

1. Wiadomo - nikt nie lubi czekać 
2. Druga sprawa jest taka że jeżeli coś działa naprawdę długo to jest to pewien sygnał może żeby zastanowić się czy nie ma w naszym kodzie jakiejś kiepskiej implementacji która ten czas niepotrzebnie wydłuża.
3. Trzecia sprawa jest taka że inne systemy korzystające z naszego API też spowolnią, jeżeli odpowiedź z naszego API trwa długo. I też API zwracające odpowiedzi po długim czasie może wpłynąć na to jak Wasz produkt oparty o modele uczenia maszynowego będą finalnie działały. Żebyście mogli to sobie łatwiej wyobrazić to podam dwa przykłady: Wyświetlanie reklam: tak naprawdę wyświetlanie reklam opiera się o system aukcyjny w którym każda reklama jest oceniana - czy wy w nią klikniecie, bądź wyświetlicie, bądź będziecie wchodzić z nią w jakąś interakcję i w takim systemie używane modele uczenia maszynowego używane są tego żeby aby ocenić jakość reklam i wybrać tą która będzie najlepsza, aby Wam pokazać. Konkurują między sobą setki reklam jak i tysiące I dla każdej z nich musi być ona predykcja, więc w takich warunkach modele muszą działać naprawdę szybko - tutaj można mówić o ograniczeniach rzędu  kilku bądź kilkunastu milisekund. U mnie w firmie mamy model, który zwraca predykcję od 15 do 30 sekund. Czy można to to przyspieszyć? Niestety nie. Jak tą sytuacja można rozwiżać? Od strony UI i interakcji użytkownika z tym modelem tak naprawdę w momencie kiedy on z niego korzysta to dostaje informację zwrotną że predykcje z modelu są przygotowywane i dostanie on powiadomienie w aplikacji o tym że jest już to gotowe po jakiś tam czasie. 

Więc generalnie badanie szybkości waszego API decyduje naprawdę o wielu aspektach i warto to sprawdzić.

Testo obciążeniowe będziemy wykonywać dzisiaj oraz na zjeździe 3. Dzisiaj chciałbym tylko żebyście zobaczyli z jakiej biblioteki możemy korzystać i jakie ona może na wygenerować wyniki i tak dalej. Natomiast te wyniki nie będą miarodajne. Dlaczego? Dlatego że pamiętajcie że mamy na razie API zdeployowane lokalnie, a system produkcyjny  ędzie inne pod względem liczby CPU i pamięci RAM, zatem tak naprawdę najlepiej testy przeprowadza się wtedy kiedy wdroży się już pewną wersję API na środowisko dev czy preprod, które powinno być kopią produkcji I na niej przeprowadzić testy obciążeniowe. Dzisiaj zrobimy te testy lokalnie żebyście zobaczyli z czego skorzystamy i nauczyli się takie testy tworzyć. Jak wyglądają te wykresy z analizy i tak dalej a na trzecim wtedy skupimy się stricte na tym żeby zrobić to “po bożemu”. 

  

Okej zobaczymy w takim razie od tworzenia tych testów. Do testów jednostkowych wykorzystamy bardzo fajną bibliotekę o nazwie locust, Która tam w sumie dostarcza wszystko co chcemy a pisanie testów obciążeniowych  z jej wykorzystaniem jest bardzo bardzo proste. 

  

Przejdźmy do `tests/stress` stwórzmy `__init__.py` oraz `test_decisions.py`

  

Zaimportujmy sobie biblioteki które będziemy potrzebować:

  

```python

from locust import HttpUser, task

  

from src.utils.generate_request import ExampleRequestGenerator

from src.service.schemas.requests import DecisionRequest

```

  

Teraz to co musimy zrobić w ramach lokusta to stworzyć sobie użytkownika naszego API i określić co ma robić w naszym przypadku po prostu ma wysłać requesta

  

```python

class ConstantUser(HttpUser):

   host = "http://0.0.0.0:8080"

```

Parametr host określający Na jakim adresie będzie nasz API działało.

  

Następny tydzień jemy sobie funkcję `post_decisions` I to co musimy w niej zrobić to po prostu napisać kod który wyśle nam danego requesta do naszego API. Przekopijmy sobie w takim razie zawartość `sent_example_requests.py` ze z folderu `scripts`.

  

```python

def post_decisions(self):

   data = {

       "installment_rate_in_percentage_of_disposable_income": 0.25,

       "age_in_years": 40,

       "foreign_worker": "yes",

       "present_employment_since": "unemployed",

       "personal_status_and_sex": "male: single",

   }

  

   headers = {

       "Content-Type": "application/json",

   }

  

   url = "http://localhost:8080/decisions"

   response = requests.post(url=url, headers=headers, json=data)

```

  

Musimy delikatnie poprawić ostatnią linijkę z wysłaniem requesta: 

  

```python

self.client.post(url=url, headers=headers, json=data)

```

  

I ostatnia rzecz to jest dodanie decoratora `@task` do naszej funkcji, Mam uruchomić właśnie tą funkcję.

  

```python

@task

def post_decisions(self):

```

  

Wróćmy jeszcze do naszego słownika `data` Bo musimy to zmienić. Gdybyśmy zostali przy tym słowniku z tymi danymi to tak naprawdę po pierwszym requeście już byśmy odczytywali dane z casha a tak naprawdę nie chcemy tego robić. Chcemy po prostu za każdym razem wysłać inny request żebyśmy sprawdzili to jak API działa w momencie kiedy zwraca cały czas predykcje. Może na chamsko zakomendować kod związany z redisem ale nie chcemy tego robić.  w tym celu skorzystamy z prostej implementacji `ExampleRequestGenerator` Który za każdym razem będzie nam generował requesta ze sztucznymi danymi:

  

```python

@task

def post_decisions(self):

   request_generator = ExampleRequestGenerator(DecisionRequest)

   data = request_generator.generate().dict()

   headers = {

       "Content-Type": "application/json",

   }

   self.client.post(url="/decisions", headers=headers, json=data)

```

Okej kod jest  gotowy to tam zostało do zrobienia to skonfigurowaniu naszego testu.  W tym celu stwórzmy `locust.conf` w tym samym folderze, Czyli plik konfiguracyjny naszych testów w którym zawrzemy takie wartości

  

pierwszy test:

  

```

locustfile = test_decisions.py

headless = true

users = 1

spawn-rate = 1

run-time = 1m

html = report_1_user.html

```

  

1. locustfile - Wskazuje na plik z naszymi testami
    
2. headless - Locus pozwala na uruchamiać testów z poziomu interfejsu graficznego więc ten parametr to wyłącza po prostu chcemy je uruchomić z poziomu terminala
    
3. users - Liczba użytkowników naszego API na początek ustawmy jeden żeby żeby tylko zobaczyć o co tutaj chodzi
    
4. spawn-rate -  spawner określa Jak często ci użytkownicy mogą się pojawiać i tylko oznacza jeden na sekundę, ale przy jednym użytkowniku to nie ma teraz znaczenia potem będziemy to zmieniać
    
5. run-time - Jak długo będę trwały testy ustawmy na jedną minutę żeby cokolwiek nam się pojawiło
    
6. html -  nazwy naszego raportu który stary wygenerowane z testem obciążeniowymi
    

  

Teraz przejdźmy sobie do terminala Wejdź mi do tego folderu z naszymi testami

  

```bash

cd tests/stress

```

  

I uruchomimy nasze testy Korzystając z komendy `locust` I następne musimy wskazać nazwę klasy czyli nasz `ConstantUser`, bo W jednym pliku może zadawać ci wiele scenariuszy użytkowania.

  

```bash

locust ConstantUser

```

  

Poczekajmy minutkę aż się testy wykonają I zobaczmy co otrzymamy. Okej to po pomyślnym przeprowadzeniu testów powinniśmy otrzymać nasz raport w tym samym folderze `report_1_user.html`,  Otwórz mi go i Zobaczmy co nam locust przygotował.

  
  

1. Mamy informacje o tym kiedy dany test został przeprowadzony,  na jaki adres wysłany był requesty i jaki był skrót używany Czyli po prostu podstawowe te dane
    
2.  dalej mamy informacje o naszych requestach które zostały wysłane Zobaczcie mamy kilkadziesiąt tysięcy w samych requestów.  informacje też o tym czy jakiś request z nich został nieprzetworzony czyli liczba faili.  informacje o średnim czasie przetwarzania requestów,  również min i Max, No i też rps czyli tylko jest Request Per Seconds.
    
3. Pod spodem mamy z kolei informacje o czasie  zwracania odpowiedzi przez nasze API wartości medialne oraz konkretnych percentyli. 
    
4. I pod spodem mamy wykresy liczba requestów wysłana na sekundę czas odpowiedzi gdzie dwie metryki są podane w sumie najważniejsze mediana oraz 95 percentyl. Oraz liczba użytkowników która odpytywała nasz API w tym przypadku cały czas był to tylko jeden użytkownik.
    

  

Na razie to wykres są płaskie dlatego że to był tylko jeden Użytkownik który odpytywał nasze API, Ale teraz trochę testy będą uwzględniały większą ilość użytkowników natomiast ten pierwszy test mówi nam już dosyć sporo.

  

Ten użytkownik którzy Został przez nas zdefiniowany ma nazwę `ConstantUser` - nie bez przyczyny.  Locus pozwala nam na definiowane różne użytkowników tak naprawdę tutaj sobie stworzyć naszego API. W tym celu korzystasz się z parametru `wait_time` w atrybutach danej klasy. Co określa ile użytkownik Czeka przed wysłaniem kolejnych requestów.  my takiego parametru nie ustawiliśmy dlatego nasz użytkownik działa tak że jak tylko otrzymujesz odpowiedź naszego API to wysyła go na request. To jest scenariusz testowy taki który nasze up jest ciągle odpytywane jest pod ciągłym obciążeniem przez jednego użytkownika.

  

Ten scenariusz testowy pozwoli nam na odpowiedzenie sobie na pytanie jaki mamy throughput na naszym API - W tym wypadku jest to około 400. A z response time Możemy czytać że 95% wynosi 3 milisekundy. Generalnie podczas sprawdzania latency Taką powszechną metryką stosowaną do oceny jest właśnie 95 percentyl - Po prostu taka jest powszechna praktyka i tak naprawdę jeżeli spotykacie się z wszelkimi  warunkami na czas przetwarzania API to przeważnie domyślnie one zawsze dotyczą 95 percentyla. Dlaczego tak jest - Po prostu stawiamy sobie jakiś margines błędu na szum, Bo te niewielkie fluktuacje w czasie przetwarzania requestów są powiązane Z wieloma rzeczami tak jak bardzo jest obciągnę komputer z którego ten request jest wysyłany, Aspekty sieciowe Może nasza sieć w bloku w którym mieszkamy jest jakoś obciążona Jest wiele rzeczy  niezależnie od nas które mogą  w czasach przetwarzania One nigdy nie będą idealne dla każdego requesta. Dlatego używam się przeważnie 95 percentyla.

  

Jak widzicie nasze API sobie poradziło Sobie z zapytaniami od jednego użytkownika to teraz Sprawdźmy w takim razie jak nasze API działa Kiedy otrzymywałby zapytania od 5 użytkowników na raz.

  

Zmieńmy sobie `locust.conf`

  

drugi test

  

```

locustfile = test_decisions.py

headless = true

users = 5

spawn-rate = 0.25

run-time = 1m

html = report_5_users.html

```

  

Ustawiamy sobie pięć użytkowników oraz Spawn rate równy 0,25 Który oznacza że co 4 sekundy będzie dołączony nowy użytkownik.  włączmy testy poczekamy minutę i Sprawdźmy Jak wygląda raport.

  

Jak widzicie nasze up już sobie gorzelać z 5 użytkownikiem i na raz otrzymujemy 1200 requestet na sekundę. I jak widać czas oczekiwania na odpowiedź to już nie 3 Mil sekundy jak wcześnie a już 7 ms Jak widać Ma to związek z kolejnymi użytkownikami, Im więcej użytkowników odpytujących równoleglash API tym te czas przetwarzania są znacznie krótsze.

  

Teraz z czym związany jest ten wzrost czasu odpowiedzi z naszego API. Dlatego że mamy je tylko jedną instancję naszego API czyli w danym momencie jesteśmy w stanie przetworzyć tylko jeden request. A w sytuacji gdy mamy pięć użytkowników wysyłających W tym samym czasie zapytania i kontynuują  ten proces to znaczy jak tylko dostaną odpowiedź to znowu wysyłają te requesta. W efekcie czego tworzy się kolejka oczekujących requestu do przetworzenia. I nasz API przetwarza je zgodnie z kolejnością przychodzących requestów. Czyli w momencie kiedy pięć użytkowników wyślę pięć requestów na raz to API przetworzy pierwszy request i zwróć odpowiedź,  potem przetworzę drugi request Zwróćcie odpowiedź trzeci czwarty i piąty i tak dalej. Czy ta osoba która wysłała Rico Jest taka ostatni to musi jak najdłużej czekać na swoją odpowiedź bo Bo przednią jeszcze cztery requesty są do przetworzenia przez nasze API. Jak w ten sposób dostanie się czas przetwarzań ze wszystkich requestów które tutaj było kilkadziesiąt tysięcy Otrzymujemy rozkład czasu przetwarzania danych requestów I w ten sposób możemy Wyznaczyć percentyle przetwarzania odpowiedzi naszego API. Czy Jak widać przy takim uciążonym nasz API działa 2,5 razy wolniej.

  

To teraz zróbmy kolejny eksperyment jeszcze na znacznie większą skalę. Zobaczmy jak zachowuje się nasze API przy stu użytkownikach 

  

trzeci test

```

locustfile = test_decisions.py

headless = true

users = 100

spawn-rate = 5

run-time = 1m

html = report_100_users.html

```

Teraz tutaj mamy 100 użytkowników i co sekundę pojawia się 5 nowych. Czyli po 20-stej sekundzie trwania testów powinno już API być odpytywane przez 100 użytkowników.

  

No jak widzę tutaj już znacznie czas oczekiwania na odpowiedź API wzrósł to kilkuset milisekund przez to użytkownikach ale tego można było się spodziewać.

  

Okej to teraz jak ten locust może nam pomóc Sytuacji gdy mamy ograniczenie czasowe.  Przejdźmy sobie do naszego kodu z implementacją `ConstantUser`  i zmodyfikujmy sobie wysyłanie posta:

  

```python

with self.client.post(url="/decisions", headers=headers, json=data, catch_response=True) as response:

   if response.elapsed.total_seconds() > 0.1:

       response.failure("Exceeded maximum response time")

   else:

       response.success()

```

  

Załóżmy że mamy po prostu teraz ograniczenie na 100 milisekund czyli requessy które  dostaną odpowiedź Po tym czasie uznawane są jako `failure` a pozostałe jako `success`.

  

Włącz mi nasze testy i zobaczmy jak będzie wyglądał wynik. 

  

No to widzicie że nasze API niestety nie poradziło sobie z naszym warunkiem na 100 milisekund można zobaczyć po wykresach  z czasem przetwarzają na którym pojawiła się czerwona linia mówiąca o failors czyli w naszym przypadku feider była zdefiniowany jako  jako request dla którego czas przetwarzania jest większość 100 milisekund. 

  

Jak sobie na zimę wykres to zobaczymy że już przy około 50 użytkownikach pewne requessy są przetwarzane już później. A przy to przy stu to już w ogóle.

  

Teraz Wyobraźmy sobie że to jest faktycznie case na naszej produkcji.  że faktycznie do naszych serwisów wysyłane ich jest tyle requestów że odpytuje go aż 100 użytkowników teraz jak moglibyśmy sobie z tym poradzić No pierwsza rzecz która nam przychodzi do głowy Oczywiście to jest zastanowienie się czy możemy przyspieszyć nasz kod. Akurat w naszym przypadku implementacja jest tak prosta że tutaj nie ma co więcej zdziałać Więc zostaje nam ostatnia rzecz która najczęściej stosowana. Po prostu w takiej sytuacji musimy zeskalować nasze rozwiązanie i nie opierać się tylko i wyłącznie o jedną instancję naszego API o więcej. Takie skalowania że rozwiązań będziemy robić biorę na trzecim zjeździe na którym poznacie Klaster zarządzany przez Kubernetesa. Generalnie można zditować sobie lokalnie API na wielu procesach ale nie chciałbym żeby się dorobili tutaj na wirtualne maszynkach bo one mają tylko dwa procesory i to dosyć mocno spowolnić działanie waszej wiem jak a może nawet najgorszym wypadku je ubije. Ja uruchomię taki Case u siebie na komputerze bo tutaj mam 24 procesory, Żebyście zobaczyli że Faktycznie zdepilowanie API na wielu różnych instancjach można w tym przypadku pomóc:

  

Teraz uruchamia API na 4 procesach 

```bash

gunicorn src.service.main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8080

```

  

I włączymy ten sam test z 100 użytkowników. 

  

To co można dostać od razu to to że througput się zwiększył znacznie. Jesteśmy w stanie przetwarzać teraz około 1300 requestów na sekundę Co bezpośrednio na pewno wpłynie na czas przetwarzania. 

  

Jak widzicie 95 marca wynosi teraz już z około 100 ms A nie około 300 jak było wcześniej, Więc jest znacznie znacznie lepiej, Ale wciąż istnieją requesty jak widzicie które wciąż są przetwarzane z czasem większym niż 100 milisekund. Ale kierunek rozwiązania jest znany prosto trzeba dostawić kolejną instancje żeby móc taki ruch obsłużyć. To jest taki przedsmak czego Co będziemy robić na zjeździe 3 to znaczy poświęcony. Będziemy te testy uruchamiać już wtedy kiedy nasze  nasze API będzie działało na klastrze na produkcji i będziemy też uczyć się wdrażania tak zwanego HorizontalPodAutoscalera Którego zadanie będzie dostawianie nowych instancji w sytuacji kiedy obserwowany jest wzmożony ruch. I wtedy będziemy widzieć jak te Response Times powinny spadać w ramach  przeprowadzonych testów. Ale to na trzecim zjeździe.

  

Słuchajcie to kończy nasze testy obciążeniowe I zarazem Modus znalazł testowaniem jak widzicie Jest tego sporo ale to ten element jest bardzo bardzo ważny dlatego chciałem go tak dokładnie omówić I będziemy jeszcze tego wracać na trzecim zjeździe.**