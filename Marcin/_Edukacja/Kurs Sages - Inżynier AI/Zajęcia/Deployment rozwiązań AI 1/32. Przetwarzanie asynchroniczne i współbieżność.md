# Przetwarzanie asynchroniczne i współbieżność

Został nam ostatni temat najtrudniejszy ze wszystkich tematów, które teraz przerabiamy dlatego że tutaj będziemy omawiać rzeczy związane z współbieżnością oraz asynchronicznością. To wam pozwoli zrozumieć co się dzieje w Fast API w momencie kiedy na przykład są tworzone Background Taski oraz Co to oznacza że fast API jest w stanie przetwarzać requesty w sposób asynchroniczny. 

Po tym module nie będziemy mieli ćwiczeń, dlatego że jest to moduł stricte teoretyczno-demonstracyjny. Na samym końcu podsumuję to wszystko co Wam zaprezentowałem w kontekście naszego API z modelem uczenia maszynowego. Ten moduł jest bardzo istotny, dlatego, że bardzo często w implementacjach API będzie spotykać się z klauzulami `async` i `await`, które ja też w naszym kodzie użyłem. Samo użycie tych klauzul nie oznacza od razu że wasz kod będzie działał w sposób asynchroniczny, bo nie każde działanie może zostać w taki sposób przetwarzane. 

To co teraz będziesz umawiać jest mocno osadzone w kontekście Fast API. Po tych zajęciach będę miał dla was “pracę domową”, polegające na tym żebyście obejrzeli sobie w wolnej chwili nagrania z dwóch konferencji, w których były poruszane tematy asynchroniczności i współbieżności ,które pomogą wam jeszcze lepiej to wszystko zrozumieć i przyswoić. 

Rozpoczniemy sobie ten moduł od tematów współbieżności. Jak się zapyta się Data Scientista o tematy współbieżności, czy też w ogóle szerzej rozumianego przetwarzania równoległemu, no to jemu od razu nasunie się pierwsza myśl do głowy o multiprocessing, czyli przetwarzaniu na wielu procesorach i to jest jak najbardziej okej dlatego że my w Data Science opieramy się głównie o przetwarzanie wieloprocesorowe. Jest to dosyć łatwe dlatego że np. scikit-learn czy inne biblioteki podczas uczenia naszych modeli dostarczają parametry związane z przetwarzaniem wieloprocesowym. Np. w scikit-learn jest taki parametr jak `n_jobs` który określa na ilu procesorach ma być wykonana pewna operacja i to znacznie przyspiesza uczenie niektórych algorytmów, które można sparaliżować, przykładem tego są lasy losowe - każde drzewo może być uczone na oddzielnym procesorze. Z kolei jak się zapyta osoby które są bardziej związane z backendem czy frontendem tam w tematach współbieżności bardziej panuje pojęcie multithreading, czyli wielowątkowości. Dlatego że tam więcej jest związanych z wysyłaniem requestów do jakichś, przesyłaniem danych do bazy danych - to są inne operacje, w ramach których lepiej jest użyć wielowątkowości. O takich rodzajach operacji powiem więcej później.

Oprócz tematów wielowątkowości oraz multiprocesowości dochodzi też temat asynchroniczności, który jest z kolei czymś innym niż ta współbieżność, o czym powiemy sobie później.

Zacznijmy od tego żeby przypomnieć sobie czym jest proces, a czym jest wątek:

1. Proces - Proces tak naprawdę to instancja danego programu - w tym przypadku naszym programem będzie API napisane w bibliotece Fast API, które uruchomiliśmy tutaj korzystając z komendy make api. Proces ma swoją własną pamięć, w której przechowywane są obiekty - w naszym przypadku jest to model i połączenia z bazami dane oraz wszystkie inne obiekty które zainicjowaliśmy sobie w kodzie. Ważne: ma też swojego własnego interpretera. Czyli dlatego możemy zrównoleglić dane wykonywanie wykonywanie w operacji w Pythonie właśnie dzięki wieloprocesowości. Natomiast w wielowątkowości już nie, o czym zaraz. Dany proces też może wywołać inne procesy. Wtedy mówimy że dany proces jest jakby procesem rodzic, a kolejne procesy które Są wywoływane przez niego są procesami dziećmi.
    
2. Wątek - Z kolei wątek jest to pewna część programu która wykonywana jest w ramach procesu. Współdzieli pamięć z innymi - wątkami to znaczy że ma dostęp do tych samych obiektów które są dostępne w tym procesie w ramach którego działa. Przy czym te obiekty które są dostępne w wątkach one nie są kopiowane do wątków tylko trzymana jest do niej referencja. Dlatego też jak się utworzy tysiąc wątków w ramach procesu to nie oznacza to, że teraz wykorzystanie pamięci zwiększy się 1000-krotnie. Nie! Każdy z tych wątków ma dostęp do tego samych obiektów tego procesu. Tutaj ważnej jest to, że 

Żebyśmy to sobie łatwiej zwizualizowali, To uruchomię teraz nasze API tak do tej pory to robiliśmy.

```bash
make api
```

A następnie spojrzymy sobie do monitoringu procesu Korzystając z `htop` :

```bash
htop
```

W filtrach znajdę nasz proces -> `python src/service/main` oraz F5 pozwoli mi na wyświetlenie listy wątków i procesów.

U góry ten nasz pierwszy rekord oznacza proces czyli w naszym wypadku jest to instancja programu naszego API. Z kolei te poniżej rzeczy które jak widzicie wychodzą od niego w takiej strukturze drzewiastej to są wątki, czyli to są pewne części programów, które wykonywane są w ramach wątków. Jest ich 42 - to są wątki, czyli pewne części programu, które FastAPI stworzył w momencie inicjalizacji naszego API. Kolumna S, która mówi o statusie, wskazuje, że te wątki mają status S, czyli sleep. Po prostu wątki rozpoczęły swoją pracę, trwała ona dosłownie 70 milisekund i po zakończeniu przeszły w stan uśpienia. Niestety nie udało mi się dowiedzieć dlaczego FastAPI w momencie inicjalizacji tworzy tyle wątków. Ale to nie będzie dla nas istotne bo zaraz sami takie wątki będziemy tworzyć. 

Żebyście zobaczyli o co mi chodzi. Ja dodam specjalną linkę w kodzie tam gdzie zapisujemy dane do postgresa, bo w tej chwili ten zapis trwa tak szybko że nie zobaczycie o co chodzi.

**TERAZ ZMIEŃ W ZAPISANIU DO POSTGRESA W BACKGROUND TASK TĄ LINIJKĘ ABY POKAZAĆ IM JAK TO DZIAŁA**

```
while True:  
    1 + 1
```

To co ja zrobiłem to po prostu petle dodająca 1 do 1 aby wykonywane były jakieś operacje zajmujące CPU tak żebyście zobaczyli to w `htop`ie.

Uruchamiam API jeszcze raz I wyślę jednego requestów. Jak widzicie dostaliśmy odpowiedzi naszego API.  Spójrzmy na logi w naszym API widzimy że dane nie zostały zapisane jeszcze do bazy danych. Po logach może stwierdzić że kod zatrzymał się w tym momencie właśnie zapisania danych do bazy, tam gdzie dodaliśmy tą pętlę. 

Spójrzmy na `htopa`I zobaczmy co się dzieje. Pojawił się na dole jeden rekord, który korzysta z 100% CPU. To właśnie nasz background task, który powstał. Background task, który tutaj powstał jest wątkiem, czyli pewnym podprogramem naszego API. 

Teraz wyślijmy kilka requestów do API.

To co wam tutaj pokazuje, to to że dodało kolejne wątki. One są tutaj niżej widać wykorzystanie CPU to jest właśnie to są te właśnie wątki, które w tej chwili działają żeby tą pętlę przeliterować a potem zapiszą do bazy danych. Czyli jak widzicie Fast API używa wątków do tego żeby móc wykonywać funkcję zapisane w Background Taskach. Jeszcze raz przypominam - wątek jest to pewna część programu która wykonywana jest w ramach procesu. Współdzieli pamięć z innymi - wątkami to znaczy że ma dostęp do tych samych obiektów które są dostępne w tym procesie w ramach którego działa. W tym przypadku, pojedynczym wątkiem jest uruchomienie funkcji `write_to_postgres`. I ta implementacja `BackgroundTasks` jak sama nazwa wskazuje, powoduje to, że dana funkcja zostanie uruchomiona w tle. A w rzeczywistości jak widzicie, tworzony jest oddzielny wątek, który odpowiedzialny jest za to, aby wykonać konkretną operację.

Ale chciałbym teraz zwrócić waszą szczególną uwagę na jedną rzecz. Nie wiem czy to widzicie dobrze ale jest taka literka R, która się pojawia i cały czas zmienia swoje miejsce tutaj w konsoli. Czy ktoś z Was może się domyślać co to może oznaczać? Wspominałem o tym, że Python jest jednowątkowy.

Literka R oznacza, który wątek, czyli w tym naszym konkretnym przypadku, który zapis do bazy, jako Background Tassk, ma teraz dostęp do Pythonowego Interpretera. Czyli to jest to co słyszycie o Pythonie, że jest jednowątkowy - to teraz macie żywy przykład tego, że tak właśnie jest.

S - Oznacza Sleeping czyli spanie w tej chwili ten wątek nie jest w ogóle wykonywany, R - Oznacza Running ten konkretny wątek jest w tej chwili wykonywany. Jak widzicie ta R-ka przeskakuje pomiędzy wątkami i raz jest tak że jeden wątek jest wykonywany, a pozostałe śpią. Potem drugi wątek jest wykonywany, a pozostałe śpią i tak dalej i tak dalej. To będzie tak przeskakiwało aż wszystkie wątki zakończą swoje działanie. Można zobaczyć też, że te czasy wykonywania inkrementują się o małe wartości, więc on stara się w miarę równomiernie je zrealizować. Czym to jest spowodowane? 

Na pewno słyszeliście o tym z takim jak GIL w Pythonie - czyli Global Interpreter Lock. Wiem że wspomniany był on na poprzednich zjazdach, ale tutaj widzicie jego działanie w praktyce. Global intermeter Lock powoduje to że Python jest wciąż jednowątkowy - czyli w danym momencie może być wykonywany jeden wątek. Dlaczego?  Dlatego że gdyby GILa nie było, to te wątki które tu widzicie mogłoby operować na tych samych strukturach danych jednocześnie, czyli załóżmy jeden wątek usunął jakiś obiekt - na przykład model - a drugi wątek chciałby się do niego odnieść żeby wykonać operację i kod by się wywalił. Dzięki GILowi, nie mamy tych problemów i dzięki temu Python może łatwo zarządzać współdzieloną pamięcią. A będąc precyzyjnym, dzięki GILowi Python nie musi jakoś specjalnie zarząðzać pamięcią, bo ona jest zawsze dostępna tylko dla jednego wątku. Więc jest jakiś tradeoff. Ta przeskakująca literka R, którą tu widzicie oznacza to który wątek ma teraz dostęp do Interpretera. Python stara się zrealizować naszą potrzebę zrealizowania wszystkich tych wątków i co jakiś czas przekazuje tego interpretera z wątku na wątek, tak żeby wszystkie wątki się zakończyły.

Generalnie jak widzicie tych wątków można utworzyć bardzo, bardzo dużo. Wyślijmy troche tych requestów do naszego API i zobaczmy czy ma to jakiś wpływ.

Nie wiem czy widzicie ale od pewnego momentu ta odpowiedź z API jest troche wolniejsza. Zaraz powiem jak można to lepiej zrobić. 

No ma to związek z tym, że Pythonowy interpreter musi tera zadowolić wszystkich - zarówno te nasze wątki które powstały, ale też nasz główny proces, który przejmuje zadanie wykonania predykcji.

Natomiast pamięć, zobaczcie, że stoi w miejscu. To wynika z tego co mówiłęm, że apmięć jest współdzielona między wątkami. Gdyby jednak wątki musiały coś zmodyfikować jakieś dane w swoim działaniu  to nie powinniśmy mieć problemu z brakiem pamięci RAM. Jeżeli z kolei wątek modyfikuje jakiś obiekt, który jest wspólny dla wszystkich, w trakcie swojego działania to jest używana tak zwana metoda Copy-On-Write. Czyli po prostu alokowana jest dodatkowa specjalna pamięć dla tego wątku i w tej pamięci specjalnej modyfikował na obiekt dajmy na to, nasze wątki modyfikują jakieś tabelki pandasowe. Przykładem takiego obiektu był nasz `CORRELATION_ID`, który zdefiniowaliśmy jako `ContextVar`. To jest obiekt, który jest zapisany w pamięci każdego wątku, tak jak mówiłem wcześniej zrobiliśmy to specjalnie po to aby ta wartość nie przenikała do innych wątków. Czyli w tym przypadku wartość w `ContextVar` jest dla każdego wątku inna. Można też zdefiniować struktury, które są share’owane pomiędzy różnymi wątkami i do tego służą zmienna globalne.

Teraz pytanie, kiedy może pojawić się tak dużo wątków? No przede wszystkim wtedy kiedy wasze API dostaje mnóstwo requestów. Każdy request będzie oznaczał kolejny wątek  którego zadaniem będzie zapisanie danych do bazy danych. Czy istnieje ryzyko że wasze API się przez to spowolni? Niestety jest. Bo jak widzicie w tym przykładzie z tą skakającą R-ką, która oznacza który wątek ma w tej chwili interpretera,  możecie znaleźć się w takiej sytuacji w której faktycznie w wasze API działa troszkę wolniej niż wtedy kiedy ten ruch jest mniejszy,  i to może być związane właśnie z dużą ilością wątków które zostały stworzone, bo każdy wątek zajmuję jakoś tam malutką część waszego procesora. Oznacza to, że w momencie kiedy przyjdzie request do API, to nie bedzie wykorzystania 100% CPU dla tego requesta, a jakaś mniejsza jego część.

Rozwiązanie tej sytuacji jest zaprojektowanie architektury API w troszeczkę inny sposób, ale to dopiero będzie problemem dla was kiedy naprawdę będzie przychodziło duża liczba requestów, a w dodatku pracujecie przy projekcie dla którego każda milisekunda jest istotna. 

(POKAŻ IM SLAJD)

Tak naprawdę idea polega na tym, wtedy jest to że nie tworzycie wątków polegających na tym żeby zapisywać te dane do bazy, tylko wasze wątki mają zadanie tylko i wyłącznie wysłanie requesta z danymi do zupełnie innego serwisu który odpowiedzialny jest za zapisywanie tych danych do bazy danych. To najczęściej może być jakiś system kolejkowy typu PubSub, Kafka lub RabbitMQ służące na przykład do streamowania danych. Wysłanie danych do tego serwisu jest znacznie, znacznie szybsze niż zapisanie danych do bazy danych i oczekiwanie na odpowiedź zwrotną z bazy danych. I tam za tymi systemem kolejek mogło być definiowane dedykowane serwisy które biorą te dane i odpowiednie zapisujemy do bazy, ale powtarzam to jest ekstremalny case, kiedy naprawdę będziecie mieli ogromną ilość requestów i zauważycie pewne opóźnienie które właśnie może sobie być związane z tą ogromną ilością wątków i dodatkowo pracujecie w warunkach kiedy naprawdę czas odpowiedzi z API jest dla was bardzo bardzo istotny.

Drugim rozwiązaniem, który jest znacznie prostszy, ale wymaga wtedy większych zasobów może być to oczywiście zeskalowanie naszego API i uruchomienie API nie na jednym procesie, a na wielu procesach. 

Teraz pytanie czy jesteśmy w stanie uruchomić API na wielu procesach? Jak najbardziej:

```bash
gunicorn src.service.main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8080
```

**W FILTRACH WPISZ `gunicorn` **

To co zrobiłem to uruchomiłem nasze API na 4 procesach. de facto technicznie jest ich 5 bo pierwszy proces który wycina samej górze jest procesem rodzicem, on z kolei utworzył 4 procesy dzieci, którymi zarządza. (KLIKNIJ DWA RAZY NA PROCES TO SIE SCHOWA LISTA) Czyli to co mówiłem na poprzednim slajdzie - proces może utworzyć inny proces. Wtedy mówimy o procesie rodzicu i procesie dziecku. Zadanie procesu rodzica jest jedno - jeśli przyjdzie request, to ma go wysłać losowo, do któregoś z tych procesów. Czyli to już będą nasze API. Mamy tutaj API zdefiniowane na cztery procesach. Teraz przypominam - proces jest to instancja naszego kodu - ona ma swoją własną pamięć tworzy swoje własne struktury i to pamięcią dzieli się tylko i wyłącznie z wątkami które wchodzą w skład tego procesu, czyli każdy z tych białych rekordów ma wczytany model oraz te wszystkie inne obiekty które zostały stworzone. Wykorzystanie pamięci waszej w tej chwili jest czterokrotnie większe, dlatego że każdy proces musiał te wszystkie obiekty sam zainicjować dla siebie. 

To teraz znowu puśćmy sobie troche requestow i zobaczmy jak to wszystko wygląda. 

Teraz jak widzicie mamy podobną sytuację tak jak wcześniej, czyli każdy z procesów utworzył swoje własne wątki, które są związane z zapisem tych danych do bazy i znowu jak widzicie ta erka przeskakuje pomiędzy wątkami, ale każdy proces ma swój własny pythonowy interpreter. Który przeskakuje z wątku na wątek. No tak żeby móc te wszystkie nasze wątki związane z wysłaniem danych do postgresa zrealizować. 

(POKAŻ SLAZD Z MULTIPROCESSINGIEM I MULTITHREADINGIEM)

Pytanie jakie możecie sobie zadać: czy jest możliwość stworzenia więcej procesów niż waszych procesorów? Oczywiście, że tak. Wpiszscie sobie w konsoli 

```
ps aux
```

To są wszystkie procesy, które działają na Waszej maszynce. Jest kilka kilkaset albo kilkatysięcy, podczas gdy macie tylko 2 CPU zaalokowane na niej. Po prostu działa to tak że dany proces jest w tej chwili wykonywany, potem w pewnym momencie jest zatrzymywany, żeby inny proces był wykonywany i tak to się odbywa. Trochę jak z tą przeskakującą Rką, którą wcześniej widzieliście. Gdyby w danym momencie wykonywane był tylko 1 proces od początku do końca,  to tak naprawdę ruch tej myszki mojej spowodowałoby to że nic innego na tym komputerze nie było wykonywane więc nasze połączenia powinno być zatrzymane, no bo komputer wykonuje teraz co innego. Dzięki temu że  że nasze procesory są bardzo mocne i wykonują miliony operacji na sekundę to takich zatrzymań działania praktycznie Nie jesteśmy w stanie zarejestrować. Więc tak ja dysponując 24 procesorami Mógłbym uruchomić API na przykład na 200 procesach.  ale to by oznaczało to że moje procesory musiałyby przetwarzać równolegle więcej niż są w stanie więc deployowanie API na liczbie procesów większej niż liczba waszych procesorów oznacza to że Będą działać wolniej dlatego że konkurują one o zasoby między sobą. 

Właśnie background taski są świetnym przykładem żeby zwizualizować sobie jak to wszystko działa i w taki sposób wam chciałem to pokazać.

### TUTAJ SKOŃCZYŁEŚ

Teraz chciałbym pokazać Wam coś ciekawego, Traktujemy to jako wstęp do przetwarzania asynchronicznego czyli czym jest `async def` i `await`, ale ubijmy API i włączmy jeszcze raz żeby wyczyścić naszego `htop`-a. 

Zmodyfikujmy sobie funkcje `write_to_postgres` dodając do niej `async` przed `def`.

```python
async def write_to_postgres(postgres_client, request, response):
   logging.info(f"Started background task to write to Postgres for {request=} and {response=}")
   postgres_client.write(request, response)
   logging.info(f"Successfuly ended background task to write to Postgres.")
```

Wyślijmy jednego requesta. 

A potem następnego i nastepnego i nastepnego.

Okazało się że nasz request w ogóle nie zwraca odpowiedzi. Logi API wskazują na to że podpowiedź już gotowa zobaczcie mamy post 200. 

Zobaczmy htopa. 

Nie pojawił się żaden wątek. A to co powinno zwrócić uwagę to to, że teraz nasz proces ma literkę R i tam wykonywane są operacje. A nie w oddzielnych wątkach.

Dlaczego tak jest dlatego? Aby to zrozumieć musimy spojrzeć w implementację background tasks. 

(PRZEJDŹ TERAZ DO METODY add_task I POKAZ IM `__call__` w klasie  BackgroundTask)

I tutaj jest pies pogrzebany... jeżeli funkcja jest asynchroniczna, to po prostu ją uruchom, a jeżeli funkcja nie jest asynchroniczna to wtedy uruchom ją jako oddzielny wątek -  ta funkcja `run_in_threadpool` właśnie powoduje że tworzy nowy wątek, w ramach którego uruchamiana jest nasza funkcja związana z zapisywaniem danych do bazy danych, czyli to co widzieliście wcześniej - pojawiły się nowe wątki, odpowiedzialne za wysłanie tych danych do postgresa.

Dlatego kiedy będziecie definiować BackgroundTaski to powinna zapalić wam się czerwona lampka, czy operacje które wykonujemy w ramach tej funkcji są asynchroniczne - bo w tym przypadku niestety ale nie są. Co oznaczają operacje asynchroniczne? Zaraz Wam o tym powiem.

Przejdźmy teraz do drugiego przypadku.

**USUŃ async def Z TEJ FUNKCJI**

**UBIJ POPRZEZ HTOPA TWOJE API**

Pokażę wam jeszcze inny przykład. Usuńmy nasz `async` z endpointa `decisions` i Dodajmy do niego ten nasz kod odpowiedzialny za iterowanie po petli, Na samym początku zaraz po pierwszym logowaniu, że przyszedł request:

**DODAJ DO ENDPOINTA decisions**

```python
logging.info(f"Received {request=}")
while True:  
    1 + 1
```

Włączmy API i wyślijmy kilka requestów w oddzielnych terminalach.

Jak widzicie wysłałem kilka requestów, z żadnego z nich nie otrzymałem jeszcze odpowiedzi no ale to jest logiczne bo te pętla musi przejść. Ale zobaczcie proszę na logi w API, które mówią że otrzymaliśmy cztery requesty. 

Z kolei konsola tutaj z analizą procesów i wątków mówi nam właśnie że powstało kilka wątków. I one są w cudzysłowiu równoległe przetwarzane, bo tak w przeciwności nie jest jak widzicie cały czas ten interpreter realizuje inny wątek.



A teraz dodajmy spowrotem ten `async`:

```python
@app.post("/decisions")
async def decisions(
```

Ubijmy API i wykonajmy ten sam eksperyment - wyślijmy kilka requestów.

Zobaczcie tym razem dostaliśmy w logach, że tylko jeden request przyszedł, a o pozostałych ani słowa. 

I nie pojawił się żaden nowy dodatkowy wątek. Ale zwróćcie uwagę na pierwszy rekord- tutaj widzimy literę R i wykorzystanie CPU w 100%. Czyli znowu mamy sytuację w której nasze requesty przetwarzane są w głównym wątki. I widzimy, że z jakichś przyczyn nasze API jest jakoś blokowane.

Czyli widzicie, jedno małe `async` w definicji funkcji i może popsuć Wam działanie API. 

Okej to umówimy nasz proces i wyjaśnijmy sobie co oznacza ta asynchroniczność.

**PRZYWRÓC TERAZ WSZYSTKO DO STANU POPRZEDNIEGO TO NA REDISIE A NIE NA POSTGRESIE**

Żeby zacząć omawianie tematu asynchroniczności musimy zacząć od tematu tego jaki mamy rodzaje operacji. Możemy wydzielić dwa główne rodzaje

1. Operacje CPU-bounded - Czyli wszelkie operacje które wykorzystują i obciążają nasz procesor czyli wszelkie obliczenia numeryczne, uczenie algorytmów,  przetwarzanie danych - wszystko co potrzebuje naszego procesora do działania.
2. operacje IO-bounded - Czyli wszelkie operacje które zależą od systemów zewnętrznych, czyli na przykład wysłanie zapytania do API, wysłanie danych do bazy danych,  prośba o pobranie danych z bazy danych, czy odczytywanie i zapis danych z dysku. Przy tych operacjach IO-bounded tak naprawdę nasz procesor czeka aż otrzymamy jakieś dane, aż dostaniemy odpowiedź z jakimś API, w każdym razie jest on nieużywany. 

I te operacje IO-bound to operację które jako jedyne można zaprogramować w sposób asynchroniczny.

Mamy teraz zajęcia z API, więc weźmy sobie przykład wysyłania requestów do jakiegoś API. I wyobraźmy sobie że wysyłamy na raz 100 requestów. Gdybyśmy podeszli do tego w sposób synchroniczny to tak naprawdę wyglądało by to tak, że pierwszy request zostałby wysłany, Wasz komputer czekałby aż dostanie dane i dopiero później po otrzymaniu tych danych wysyła kolejnego requesta. I znowu musi czekać aż te dane przyjdą żeby móc wysłać kolejnego requesta. Jak widzicie w tym momencie są to etapy czekania, momenty w których Wasze CPU nic nie robi, no bo czeka, więc w sumie mógłby robić coś innego w tym czasie.


POWIEDZ IM KONIECZNIE ZE TEN PRZYKLAD JEST MOCNO UPROSZCZONY, TAK ZEBYSCIE ZLAPALI IDEĘ
(Omów slajd z kodem wykonywanym synchronicznie vs asynchronicznie)

To był podany przykład w przypadku requestów, ale analogicznie ten case można odnieść też również do wysłania danych do bazy danych. Jeżeli wysyłamy dane do bazy danych, to baza danych musi je przetworzyć i zwróci odpowiedź. Jeżeli wysyłamy zapytanie SQL z poziomu Pythona, które ma w sobie jakieś operacje na danych, które trwają bardzo długo to to też jest idealny case na to aby ten kod został wykonany asynchronicznie. Bo my tylko czekamy aż baza danych przetworzy nasze zapytanie i po prostu zwróci nam wyniki.

Okej to teraz z tą wiedzą, która Wam do tej pory przkeazałem chciałbym abyśmy przeszli sobie  `main.py` - prześledźmy sobie kod na nasz `decisions` i I Sprawdźmy które operacje są CPU-bounded a które są IO-bounded I zapiszmy to sobie w komentarzach obok tych kodów:

**PRZEJDŹ LINIJKA PO LINIJKE I ZAPYTAJ SIE CO UWAŻAJĄ NA TEMAT TEJ LINII KODU**

```python
@app.post("/decisions")
async def decisions(
   request: DecisionRequest,
   background_tasks: BackgroundTasks,
) -> DecisionResponse:
   logging.info(f"Received {request=}")
   response = app.state.decision_redis_client.read(request)  # IO-bounded
   if response is None:
       decision = app.state.model.predict_decision(request.to_dataframe())[0]  # CPU-bounded
       response = DecisionResponse(decision=decision)  # CPU-bounded
       background_tasks.add_task(  # IO-bounded
           func=write_to_redis,
           redis_client=app.state.decision_redis_client,
           request=request,
           response=response,
       )
   background_tasks.add_task(  # IO-bounded
       func=write_to_postgres,
       postgres_client=app.state.decision_postgres_client,
       request=request,
       response=response,
   )

   logging.info(f"Returning {response=}")
   return response
```

Dokładnie tak że jak widzicie wszelkie interakcje tutaj z bazą danych są typu IO-bounded,  A z kolei predykcja modelu jak i też generowanie odpowiedzi to są już CPU-bounded.

Teraz powstaje pytanie najważniejsze - jak sprawić, żeby nasz kod działac asynchronicznie, tak jak to było na prezentacji? 

Niestety ale musze was zmartwić. Samo dodanie `async` czy też `await`, które było wcześniej, na samym początku naszego zjazdu, jest niewystarczające aby nasz kod działać asynchronicznie. 

Żeby kod działał asynchronicznie, to musimy już skorzystać z implementacji, które w taki sposób zostały stworzone. Generalnie rzecz biorąc biblioteki np. z zapisywaniem danych do bazy danych mogą mieć u siebie już pewne implementacje, które są zaimplementowane w myśl asynchronicznego działania. 

Weźmy sobie przykład odczytu danych z Redisa. 

W przypadku redisa istnieje moduł `asyncio` w bibliotece `redis` 

```python
from redis.asyncio import POKAZ_IM
```

I tutaj wszelkie obiekty są już przystosowane pod asynchroniczne działanie.

Jeśli natomiast my sami chcielibyśmy napisać kod w sposób asynchroniczny, to musimy posłużyć się dedykowaną biblioteką do tego i w tym przypadku możemy użyć biblioteki `asyncio`.

Natomiast chce podkreślić, że my dzisiaj nie bedziemy uczyć się implementacji kodu asynchronicznego. Ten temat jest bardzo trudny moim zdaniem i generalnie wykracza poza nasz zjazd. Samej asynchroniczności myślę, że można by poświęcic cały pełny dzień, 8 godzin żeby nauczyć się programować i używać tego mądrze.

To co ja Wam chce przekazać to właśnie jak FastAPI współgra z kodem asynchronicznym i kiedy go warto używać a kiedy nie. Natomiast programowanie w sposób asynchronicznie zostawiam zainteresowanym tematem. 

Pod koniec tego modułu podsumuję Wam ten cały mój wywód i to jak on się odnosi do naszego API w który mamy nasz model MLowy.

Teraz chciałbym **zasymulować**, podkreślam **zasymulować** że mamy w środku jakiś kod który działą w sposób asynchroniczny, tak żebyście zobaczyli jaki to ma wpływ na nasze API.

Przejdźmy sobie do implementacji naszego redisa do `BaseRedisClient` I teraz żeby coś było przetwarzane w sposób asynchroniczny musimy wykonać dwie rzeczy.

Po pierwsze musimy funkcję nazwać od async:

```python
async def read(self, request: Request) -> Response | None:
```

A następnie w jej ciele musimy zidentyfikować operacje które są typu IO-bound I który powoduje to że czekamy. 

Taką operacją w przypadku redisa jest otrzymanie odpowiedzi od niego czyli nasze

```python
value = self.connector.connection.get(key)
```

Teraz to co wystarczy zrobić to postawić przed wywołaniem `await`. ALE. Możemy to zrobić tylko i wyłącznie tedy kiedy ta metoda `get` faktycznie pozwala na przetwarzane w sposób asynchroniczny. Niestety ale obecny `RedisConnector` pod sobą nie działa w sposób asynchroniczny, więc od razu Wam powiem, że to nie zadziała. API rzuci błędem.

To co ja teraz zrobię to sobie zasymuluje, że mamy jakąś asynchroniczną operację.

Zaimportuję sobie bibliotekę `asyncio` oraz `random`

```python
import asyncio
import random
```

I przed odczytaniem danych z Redisa wstawię sobie taką linijkę kodu

```python

await asyncio.sleep(random.randint(10, 20))

value = self.connector.connection.get(key)

```

Ten kod spowoduje, że nasz procesor dostanie informacje, że właśnie teraz jest operacja czekania na coś, on będzie wiedział że w sumie to może pójść teraz dalej coś innego robić, i dopiero jak ta `sleep` się wykona to procesor wróci do tego momentu w kodzie i zacznie przetwarzać dalej. W ten sposób zasymulowałem sobie że właśnie wywołanie tej metody `read` wiąże się z jakimś czekaniem. 

Teraz wracamy do naszego `main.py` I to co musimy zrobić to to, aby w czytanie z naszego redisa wywołać z klauzula `await`. Dlaczego? Dlatego że teraz ta funkcja działa w sposób asynchroniczny,  bo została zdefiniowana jako `async`. I aby móc korzystać z funkcji zdefiniowanych jako asynchroniczne czyli z klauzulą `async` musimy Użyć klauzuli `await`:

```python
@app.post("/decisions")
async def decisions(
   request: DecisionRequest,
   background_tasks: BackgroundTasks,
) -> DecisionResponse:
   logging.info(f"Received {request=}")
   response = await app.state.decision_redis_client.read(request)
```

Włączmy nasze API. Przygotuj sobie kilka terminali. Odpal htopa. Wyślij te requesty na raz. Pokaz im logi. A potem pokaz htopa ze jest 1 proces. UPEWNIJ SIĘ, ZE W ZAPISIE DO POSTGRESA NIE MA TEJ PETLI WHILE.

I co teraz widać to to że faktycznie te cztery requesty do nas przyszły I każdy z nich zawiesił się na tym etapie związanym z czytaniem redisa. Ale to nie zablokowało nam API, tak jak to było w poprzednim przypadku. Dlatego że te wszystkie requesty zatrzymały się w tym etapie redisa, czyli w momencie kiedy czekamy na odpowiedź z tej bazy danych. 

Teraz ciekawostka jest taka, że zobaczcie, że wykorzystanie procesora w naszym przypadku jest bliskie 0. Dlaczego? Dlatego, że mieliśmy operacje asynchroniczną. Nic nie musiał robić, a tak na prawdę mógł wykonać inną pracę.

I tą samą rzecz, którą zrobiłem z odczytaniem z redisa w sposób asynchroniczny można było wykonać w przypadku background które pokazywałem na początku wam - te wysyłki danych do postgresa  też musiały być zdefiniowane z klauzulą `async` i `await` ale tutaj tak jak Wam mówiłem, musieliśmy zmienić naszą implementację na tyle żeby skorzystać z takich implementacji które pozwalają na wysyłanie danych do postgresa w sposób asynchroniczny. 

To tyle jeśli chodzi o moją część demonstracyjną. To teraz najważniejsza rzecz dla Was. Po co to wszystko Wam powiedziałem?

### TUTAJ SKOŃCZYŁEŚ

Podsumowując:

1. W dokumentacji Fast API często spotkacie się ze zdaniem że jeżeli nie wiesz na czym polega kod asynchroniczny, to go nie używaj. Czyli nie używaj wszelkich `async` i `await` w swoim kodzie a FASTApi sobie z tym poradzi -> [https://fastapi.tiangolo.com/async/](https://fastapi.tiangolo.com/async/) . I to jest prawda. Innymi słowy jeżeli ten temat był dla was trudny i nie do końca jeszcze zrozumiały, to trzeba sobie wszystko przetrawić w wolnej chwili i trochę tego kodu napisać. Ale generalnie jest tak że, na początku podczas projektowania API zapomnijcie o tych `async` i o `await`-ach, a FastAPI i tak będzie działało sposób asynchroniczny - one sobie z tym poradzi.
2. Jeżeli nabierzecie już praktyki możecie wtedy spróbować zastanowić się, które operacje w naszym API można przetwarzać w sposób asynchroniczny. Jeżeli biblioteki z których korzystacie pozwalają na to, to warto to zrobić dlatego że to może mieć pozytywny wpływ na wasze API w takim sensie że będzie działało szybciej, bo w momencie czekania na coś wasz procesor będzie robił coś innego na przykład przetwarzał kolejnego requesta.
3. Natomiast z idei asynchroniczności w API MLowym rzadko skorzystacie, więc czasami gra może nie być warta świeczki. Dlatego że naszym core-owym działaniem API jest model i generowania predykcji przez niego, a operacje generowania predykcji przez niego są operacjami CPU-bounded i to ta operacja będzie wykonywała się najdłużej w implementacji waszego API. A tej operacji nie możesz przetworzyć w sposób asynchroniczny.
4. Podejście jakie stosuje się w API MLowym, jest to aby używać podejścia wieloprocesowego, a nie wielowątkowego. Dlaczego nie podejście wielowątkowe? Dlatego że tak sam widzieliście, GIL powoduje to że jest wykonywany jeden wątek w danym czasie i de facto żaden z tych wątek nie wykorzystuje w pełni waszych procesorów. Dlatego stosuje się podejście wieloprocesowe, w którym każdy proces ma swojego własnego GILa i dane operacje numeryczne mogą być wtedy wykonywane już równoległe. Każdy proces będzie miał po prostu swój własny procesor który będzie wykorzystywał w 100%. Generalnie lepiej nie przekraczać liczby procesów nie zwiększa liczbę procesorów dlatego że one będą znowu Walczyć o swoje zasoby, czyli de facto macie ten sam problem jak w podejściu wielowątkowym..
5. Dlatego przy wzmożonym ruchu w przypadku API MLowego po prostu skaluje się je do góry, to znaczy dokłada się kolejne procesy, tak żeby można było obsłużyć wzmożony ruch. I tego będziemy się uczyć na zjeździe trzecim poświęconym Kubernetesowi.

## Praca domowa

Poleć uczesnitkom aby obejrzeli dwa nagrania:
1. https://www.youtube.com/watch?v=kRy_UwUhBpo&t=1894s
2. https://www.youtube.com/watch?v=MCs5OvhV9S4

## Pytania

Pytanie: Czy istnieje limit threadów? 
Odp: Tak. Fastapi używa run_in_threadpool ze Starlette, a Starlette z kolei `anyio.to_thread.run_sync(func, *args)`. Dokumentacja mówi, że jest [max. 40 threadów](https://anyio.readthedocs.io/en/stable/threads.html#adjusting-the-default-maximum-worker-thread-count) 
