# Przetwarzanie asynchroniczne i współbieżność

Został nam ostatni temat najtrudniejszy ze wszystkich tematów, które teraz przerabiamy dlatego że tutaj będziemy omawiać rzeczy związane z współbieżnością oraz asynchronicznością. To wam pozwoli zrozumieć co się dzieje w Fast API w momencie kiedy na przykład są tworzone Background Taski oraz Co to oznacza że fast API jest w stanie przetwarzać requesty w sposób asynchroniczny. 

Po tym module nie będziemy mieli ćwiczeń, dlatego że jest to moduł stricte teoretyczno-demonstracyjny. Na samym końcu podsumuję to wszystko co Wam zaprezentowałem w kontekście naszego API z modelem uczenia maszynowego. Ten moduł jest bardzo istotny, dlatego, że bardzo często w implementacjach API będzie spotykać się z klauzulami `async` i `await`, które ja też w naszym kodzie użyłem. Samo użycie tych klauzul nie oznacza od razu że wasz kod będzie działał w sposób asynchroniczny, bo nie każde działanie może zostać w taki sposób przetwarzane. 

To co teraz będziesz umawiać jest mocno osadzone w kontekście Fast API. Po tych zajęciach będę miał dla was “pracę domową”, polegające na tym żebyście obejrzeli sobie w wolnej chwili nagrania z dwóch konferencji, w których były poruszane tematy asynchroniczności i współbieżności ,które pomogą wam jeszcze lepiej to wszystko zrozumieć i przyswoić. 

Rozpoczniemy sobie ten moduł od tematów współbieżności. Jak się zapyta się Data Scientista o tematy współbieżności, czy też w ogóle szerzej rozumianego przetwarzania równoległemu, no to jemu od razu nasunie się pierwsza myśl do głowy o multiprocessing, czyli przetwarzaniu na wielu procesorach i to jest jak najbardziej okej dlatego że my w Data Science opieramy się głównie o przetwarzanie wieloprocesorowe. Jest to dosyć łatwe dlatego że np. scikit-learn czy inne biblioteki podczas uczenia naszych modeli dostarczają parametry związane z przetwarzaniem wieloprocesowym. Np. w scikit-learn jest taki parametr jak `n_jobs` który określa na ilu procesorach ma być wykonana pewna operacja i to znacznie przyspiesza uczenie niektórych algorytmów, które można sparaliżować, przykładem tego są lasy losowe - każde drzewo może być uczone na oddzielnym procesorze. Z kolei jak się zapyta osoby które są bardziej związane z backendem czy frontendem tam w tematach współbieżności bardziej panuje pojęcie multithreading, czyli wielowątkowości. Dlatego że tam więcej jest związanych z wysyłaniem requestów do jakichś, przesyłaniem danych do bazy danych - to są inne operacje, w ramach których lepiej jest użyć wielowątkowości. O takich rodzajach operacji powiem więcej później.

Oprócz tematów wielowątkowości oraz multiprocesowości dochodzi też temat asynchroniczności, który jest z kolei czymś innym niż ta współbieżność, o czym powiemy sobie później.

Zacznijmy od tego żeby przypomnieć sobie czym jest proces, a czym jest wątek:

1. Proces - Proces tak naprawdę to instancja danego programu - w tym przypadku naszym programem będzie API napisane w bibliotece Fast API, które uruchomiliśmy tutaj korzystając z komendy make api. Proces ma swoją własną pamięć, w której przechowywane są obiekty - w naszym przypadku jest to model i połączenia z bazami dane oraz wszystkie inne obiekty które zainicjowaliśmy sobie w kodzie. Ważne: ma też swojego własnego interpretera. Czyli dlatego możemy zrównoleglić dane wykonywanie wykonywanie w operacji w Pythonie właśnie dzięki wieloprocesowości. Natomiast w wielowątkowości już nie, o czym zaraz. Dany proces też może wywołać inne procesy. Wtedy mówimy że dany proces jest jakby procesem rodzic, a kolejne procesy które Są wywoływane przez niego są procesami dziećmi.
    
2. Wątek - Z kolei wątek jest to pewna część programu która wykonywana jest w ramach procesu. Współdzieli pamięć z innymi - wątkami to znaczy że ma dostęp do tych samych obiektów które są dostępne w tym procesie w ramach którego działa. Przy czym te obiekty które są dostępne w wątkach one nie są kopiowane do wątków tylko trzymana jest do niej referencja. Dlatego też jak się utworzy tysiąc wątków w ramach procesu to nie oznacza to, że teraz wykorzystanie pamięci zwiększy się 1000-krotnie. Nie! Każdy z tych wątków ma dostęp do tego samych obiektów tego procesu. Tutaj ważnej jest to, że 

Żebyśmy to sobie łatwiej zwizualizowali, To uruchomię teraz nasze API tak do tej pory to robiliśmy.

```bash
make api
```

A następnie spojrzymy sobie do monitoringu procesu Korzystając z `htop` :

```bash
htop
```

W filtrach znajdę nasz proces -> `python src/service/main` oraz F5 pozwoli mi na wyświetlenie listy wątków i procesów.

U góry ten nasz pierwszy rekord oznacza proces czyli w naszym wypadku jest to instancja programu naszego API. Z kolei te poniżej rzeczy które jak widzicie wychodzą od niego w takiej strukturze drzewiastej to są wątki, czyli to są pewne części programów, które wykonywane są w ramach wątków. Jest ich 42 - to są wątki, czyli pewne części programu, które FastAPI stworzył w momencie inicjalizacji naszego API. Kolumna S, która mówi o statusie, wskazuje, że te wątki mają status S, czyli sleep. Po prostu wątki rozpoczęły swoją pracę, trwała ona dosłownie 70 milisekund i po zakończeniu przeszły w stan uśpienia. Niestety nie udało mi się dowiedzieć dlaczego FastAPI w momencie inicjalizacji tworzy tyle wątków. Ale to nie będzie dla nas istotne bo zaraz sami takie wątki będziemy tworzyć. 

Żebyście zobaczyli o co mi chodzi. Ja dodam specjalną linkę w kodzie tam gdzie zapisujemy dane do postgresa, bo w tej chwili ten zapis trwa tak szybko że nie zobaczycie o co chodzi.

**TERAZ ZMIEŃ W ZAPISANIU DO POSTGRESA TĄ LINIJKĘ ABY POKAZAĆ IM JAK TO DZIAŁA**

```
while True:  
    1 + 1
```

To co ja zrobiłem to po prostu petle dodająca 1 do 1 aby wykonywane były jakieś operacje zajmujące CPU tak żebyście zobaczyli to w `htop`ie.

Uruchamiam API jeszcze raz I wyślę jednego requestów. Jak widzicie dostaliśmy odpowiedzi naszego API.  Spójrzmy na logi w naszym API widzimy że dane nie zostały zapisane jeszcze do bazy danych. Po logach może stwierdzić że kod zatrzymał się w tym momencie właśnie zapisania danych do bazy, tam gdzie dodaliśmy tą pętlę. 

Spójrzmy na `htopa`I zobaczmy co się dzieje. Pojawił się na dole jeden rekord, który korzysta z 100% CPU. To właśnie nasz background task, który powstał. Background task, który tutaj powstał jest wątkiem, czyli pewnym podprogramem naszego API. 

Teraz wyślijmy kilka requestów do API.

To co wam tutaj pokazuje, to to że dodało kolejne wątki. One są tutaj niżej widać wykorzystanie CPU to jest właśnie to są te właśnie wątki, które w tej chwili działają żeby tą pętlę przeliterować a potem zapiszą do bazy danych. Czyli jak widzicie Fast API używa wątków do tego żeby móc wykonywać funkcję zapisane w Background Taskach. Jeszcze raz przypominam - wątek jest to pewna część programu która wykonywana jest w ramach procesu. Współdzieli pamięć z innymi - wątkami to znaczy że ma dostęp do tych samych obiektów które są dostępne w tym procesie w ramach którego działa. W tym przypadku, pojedynczym wątkiem jest uruchomienie funkcji `write_to_postgres`. I ta implementacja `BackgroundTasks` jak sama nazwa wskazuje, powoduje to, że dana funkcja zostanie uruchomiona w tle. A w rzeczywistości jak widzicie, tworzony jest oddzielny wątek, który odpowiedzialny jest za to, aby wykonać konkretną operację.

Ale chciałbym teraz zwrócić waszą szczególną uwagę na jedną rzecz. Nie wiem czy to widzicie dobrze ale jest taka literka R, która się pojawia i cały czas zmienia swoje miejsce tutaj w konsoli. Czy ktoś z Was może się domyślać co to może oznaczać? Wspominałem o tym, że Python jest jednowątkowy.

Literka R oznacza, który wątek, czyli w tym naszym konkretnym przypadku, który zapis do bazy, jako Background Tassk, ma teraz dostęp do Pythonowego Interpretera. Czyli to jest to co słyszycie o Pythonie, że jest jednowątkowy - to teraz macie żywy przykład tego, że tak właśnie jest.

S - Oznacza Sleeping czyli spanie w tej chwili ten wątek nie jest w ogóle wykonywany, R - Oznacza Running ten konkretny wątek jest w tej chwili wykonywany. Jak widzicie ta R-ka przeskakuje pomiędzy wątkami i raz jest tak że jeden wątek jest wykonywany, a pozostałe śpią. Potem drugi wątek jest wykonywany, a pozostałe śpią i tak dalej i tak dalej. To będzie tak przeskakiwało aż wszystkie wątki zakończą swoje działanie. Można zobaczyć też, że te czasy wykonywania inkrementują się o małe wartości, więc on stara się w miarę równomiernie je zrealizować. Czym to jest spowodowane? 

Na pewno słyszeliście o tym z takim jak GIL w Pythonie - czyli Global Interpreter Lock. Wiem że wspomniany był on na poprzednich zjazdach, ale tutaj widzicie jego działanie w praktyce. Global intermeter Lock powoduje to że Python jest wciąż jednowątkowy - czyli w danym momencie może być wykonywany jeden wątek. Dlaczego?  Dlatego że gdyby GILa nie było, to te wątki które tu widzicie mogłoby operować na tych samych strukturach danych jednocześnie, czyli załóżmy jeden wątek usunął jakiś obiekt - na przykład model - a drugi wątek chciałby się do niego odnieść żeby wykonać operację i kod by się wywalił. Dzięki GILowi, nie mamy tych problemów i dzięki temu Python może łatwo zarządzać współdzieloną pamięcią. A będąc precyzyjnym, dzięki GILowi Python nie musi jakoś specjalnie zarząðzać pamięcią, bo ona jest zawsze dostępna tylko dla jednego wątku. Więc jest jakiś tradeoff. Ta przeskakująca literka R, którą tu widzicie oznacza to który wątek ma teraz dostęp do Interpretera. Python stara się zrealizować naszą potrzebę zrealizowania wszystkich tych wątków i co jakiś czas przekazuje tego interpretera z wątku na wątek, tak żeby wszystkie wątki się zakończyły.

Generalnie jak widzicie tych wątków można utworzyć bardzo, bardzo dużo. Wyślijmy troche tych requestów do naszego API i zobaczmy czy ma to jakiś wpływ.

Nie wiem czy widzicie ale od pewnego momentu ta odpowiedź z API jest troche wolniejsza. Zaraz powiem jak można to lepiej zrobić. 



### TUTAJ SKOŃCZYŁEŚ

No ma to związek z tym, że Pythonowy interpreter musi tera zadowolić wszystkich - zarówno te nasze wątki które powstały, ale też nasz główny proces, który przejmuje zadanie wykonania predykcji.

Natomiast pamięć, zobaczcie, że stoi w miejscu. To wynika z tego co mówiłęm, że apmięć jest współdzielona między wątkami. Gdyby jednak wątki musiały coś zmodyfikować jakieś dane w swoim działaniu  to nie powinniśmy mieć problemu z brakiem pamięci RAM. Jeżeli z kolei wątek modyfikuje jakiś obiekt w trakcie swojego działania to jest używana tak zwana metoda Copy-On-Write. Czyli po prostu alokowana jest dodatkowa specjalna pamięć dla tego wątku i w tej pamięci specjalnej modyfikował na obiekt dajmy na to, nasze wątki modyfikują jakieś tabelki pandasowe. To wtedy każdy wątek będzie miał  swoją własną pamięć i w tej pamięci teraz zmodyfikowana ramka dane zostanie przechowywana i nie będzie to miało wpływu na pozostałe wątki. Przykładem takiego obiektu był nasz `CORRELATION_ID`, który zdefiniowaliśmy jako ContextVar. To jest obiekt, który jest zapisany w pamięci każdego wątku, zrobiliśmy to specjalnie po to aby ta wartość nie przenikała do innych wątków. Można też zdefiniować struktury, które są share’owane pomiędzy różnymi wątkami i do tego służą zmienna globalne.

Teraz pytanie, kiedy może się bawię tak dużo wątków? No przede wszystkim wtedy kiedy wasze API dostaje mnóstwo requestów. Każdy request będzie oznaczał kolejny wątek  którego zadaniem będzie zapisanie danych do bazy danych? Czy istnieje ryzyko że wasze API się przez to spowolni? Niestety jest. Bo jak widzicie w tym przykładzie z tą skakającą R-ką, która oznacza który wątek ma w tej chwili interpretera,  możecie znaleźć się w takiej sytuacji w której faktycznie w wasze API działa troszkę wolniej niż wtedy kiedy ten ruch jest mniejszy,  i to może być związane właśnie z dużą ilością wątków które zostały stworzone, bo każdy wątek zajmuję jakoś tam malutką część waszego procesora. Oznacza to, że w momencie kiedy przyjdzie request do API, to nie bedzie wykorzystania 100% CPU dla tego requesta, a jakaś mniejsza jego część.

Rozwiązanie tej sytuacji jest zaprojektowanie architektury API w troszeczkę inny sposób, ale to dopiero będzie problemem dla was kiedy naprawdę będzie przychodziło duża liczba requestów, a w dodatku pracujecie przy projekcie dla którego każda milisekunda jest istotna. Pokażę wam obrazek jak to się powinno zaimportować.

Tak naprawdę idea polega na tym, wtedy jest to że nie tworzycie wątków polegających na tym żeby zapisywać te dane do bazy, tylko wasze wątki mają zadanie tylko i wyłącznie wysłanie requesta z danymi do zupełnie innego serwisu który odpowiedzialny jest za zapisywanie tych danych do bazy danych. To najczęściej może być jakiś system kolejkowy typu PubSub, Kafka lub RabbitMQ służące na przykład do streamowania danych. Wysłanie danych do tego serwisu jest znacznie, znacznie szybsze niż zapisanie danych do bazy danych i oczekiwanie na odpowiedź zwrotną z bazy danych. I tam za tymi systemem kolejek mogło być definiowane dedykowane serwisy które biorą te dane i odpowiednie zapisujemy do bazy, ale powtarzam to jest ekstremalny case, kiedy naprawdę będziecie mieli ogromną ilość requestów i zauważycie pewne opóźnienie które właśnie może sobie być związane z tą ogromną ilością wątków i dodatkowo pracujecie w warunkach kiedy naprawdę czas odpowiedzi z API jest dla was bardzo bardzo istotny.

Drugim rozwiązaniem, który jest znacznie prostszy, ale wymaga wtedy większych zasobów może być to oczywiście zeskalowanie naszego API i uruchomienie API nie na jednym procesie, a na wielu procesach. 

Teraz pytanie czy jesteśmy w stanie uruchomić API na wielu procesach? Jak najbardziej:

```bash
gunicorn src.service.main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8080
```

**W FILTRACH WPISZ `gunicorn` **

To co zrobiłem to uruchomiłem nasze API na 4 procesach. de facto technicznie jest ich 5 bo pierwszy proces który wycina samej górze jest procesem rodzicem, on z kolei utworzył 4 procesy dzieci, którymi zarządza. Zadanie procesu rodzica jest jedno - jeśli przyjdzie request, to ma go wysłać losowo, do któregoś z tych procesów. Czyli to już będą nasze API. Mamy tutaj API zdefiniowane na cztery procesach. Teraz przypominam - proces jest to instancja naszego kodu - ona ma swoją własną pamięć tworzy swoje własne struktury i to pamięcią dzieli się tylko i wyłącznie z wątkami które wchodzą w skład tego procesu, czyli każdy z tych białych rekordów ma wczytany model oraz te wszystkie inne obiekty które zostały stworzone. Wykorzystanie pamięci waszej w tej chwili jest czterokrotnie większe, dlatego że każdy proces musiał te wszystkie obiekty sam zainicjować dla siebie. 

Teraz puśćmy kilka requestów żeby zobaczyć co nam tutaj powstanie. 

Teraz jak widzicie mamy podobną sytuację tak jak wcześniej, czyli każdy z procesów utworzył swoje własne wątki, które są związane z zapisem tych danych do bazy i znowu jak widzicie ta erka przeskakuje pomiędzy wątkami, ale każdy proces ma swój własny pythonowy interpreter. Który przeskakuje z wątku na wątek. No tak żeby móc te wszystkie nasze wątki związane z wysłaniem danych do postgresa zrealizować. 

Czy macie już jakieś pytania może do tego? Czy ta idea procesu i wątku jest dla was mniej więcej jasna? 

Pytanie jakie możecie sobie zadać: czy jest możliwość stworzenia więcej procesów niż waszych procesorów? Oczywiście, że tak. Wpiszscie sobie w konsoli 

```
ps aux
```

To są wszystkie procesy, które działają na Waszej maszynce. Jest kilka kilkaset albo kilkatysięcy, podczas gdy macie tylko 2 CPU zaalokowane na niej. Po prostu działa to tak że dany proces jest w tej chwili wykonywany, potem w pewnym momencie jest zatrzymywany, żeby inny proces był wykonywany i tak to się odbywa. Trochę jak z tą przeskakującą Rką, którą wcześniej widzieliście. Gdyby w danym momencie wykonywane był tylko 1 proces od początku do końca,  to tak naprawdę ruch tej myszki mojej spowodowałoby to że nic innego na tym komputerze nie było wykonywane więc nasze połączenia powinno być zatrzymane, no bo komputer wykonuje teraz co innego. Dzięki temu że  że nasze procesory są bardzo mocne i wykonują miliony operacji na sekundę to takich zatrzymań działania praktycznie Nie jesteśmy w stanie zarejestrować. Więc tak ja dysponując 24 procesorami Mógłbym uruchomić API na przykład na 200 procesach.  ale to by oznaczało to że moje procesory musiałyby przetwarzać równolegle więcej niż są w stanie więc deployowanie API na liczbie procesów większej niż liczba waszych procesorów oznacza to że Będą działać wolniej dlatego że konkurują one o zasoby między sobą. 

Właśnie background taski są świetnym przykładem żeby zwizualizować sobie jak to wszystko działa i w taki sposób wam chciałem to pokazać.

Teraz chciałbym pokazać Wam coś ciekawego, Traktujemy to jako wstęp do przetwarzania asynchronicznego czyli czym jest `async def` i `await`, ale ubijmy API i włączmy jeszcze raz żeby wyczyścić naszego `htop`-a. 

Zmodyfikujmy sobie funkcje `write_to_postgres` dodając do niej `async` przed `def`.

```python
async def write_to_postgres(postgres_client, request, response):
   logging.info(f"Started background task to write to Postgres for {request=} and {response=}")
   postgres_client.write(request, response)
   logging.info(f"Successfuly ended background task to write to Postgres.")
```

Okazało się że nasz request w ogóle nie wraca spowiedzi. Logi API wskazują na to że podpowiedź już gotowa zobaczcie mamy post 200. 

Wyślemy jeszcze kilka requestów w innych terminalach.

Widać nic się nie pojawia tutaj w konsoli. W dodatku żaden wątek nie powstał. 

Nasze Logi w API pokazują cały czas to poprzednią sytuację z pierwszego requesta, czyli mamy tutaj  odpowiedź 200, a widać że cały czas ten kod związany z zapisaniem danych do bazy jest przetwarzany. 

Teraz dlaczego, tak się stało - bardzo ważna rzecz zanim zaczniemy omawiać kod asynchroniczny. Jak widzicie tutaj: Nie dostaliśmy odpowiedzi z API i w konsoli związanej tutaj z analizą procesów i wątków nie pojawił się żaden nowy wątek Dlatego że kod asynchroniczny jest uruchamiany na tym samym głównym wątku na którym zwracane są odpowiedzi z API. Czyli dopóki ta operacje nie zakończy się, dopóty nie zostanie zwrócony wynik z API. 

Dlaczego tak jest dlatego? Dlatego, że tak zostało zdefiniowane to w FastAPI. Wejdźmy sobie w implementację BackgroundTasks - jeżeli funkcja jest asynchroniczna, to po prostu ją uruchom, a jeżeli funkcja nie jest asynchroniczna to wtedy uruchom ją jako oddzielny wątek -  ta funkcja `run_in_threadpool` właśnie powoduje że tworzy nowy wątek, w ramach którego uruchamiana jest nasza funkcja związana z zapisywaniem danych do bazy danych, czyli to co widzieliście wcześniej - pojawiły się nowe wątki, odpowiedzialne za wysłanie tych danych do postgresa.

Dlatego kiedy będziecie definiować BackgroundTaski korzystając z `async def` musicie zastanowić się czy operacje, które są w nim mogą być przetwarzane w sposób asynchroniczny. Zaraz Wam powiem co to znaczy.

**USUŃ async def Z TEJ FUNKCJI**

Pokażę wam jeszcze inny przykład. Usuńmy nasz `async` z endpointa `decisions` i Dodajmy do niego ten nasz kod odpowiedzialny za iterowanie po petli, Na samym początku zaraz po pierwszym logowaniu, że przyszedł request:

**DODAJ DO ENDPOINTA decisions**

```python
logging.info(f"Received {request=}")
for _ in range(int(1e11)):
   pass
```

Włączmy API i wyślijmy kilka requestów.

Jak widzicie wysłałem kilka requestów, z żadnego z nich nie otrzymałem jeszcze odpowiedzi no ale to jest logiczne bo te pętla musi przejść. Ale zobaczcie proszę na logi w API, które mówią że otrzymaliśmy cztery requesty. Z kolei konsola tutaj z analizą procesów i wątków mówi nam właśnie że powstały cztery wątki. I one są w cudzysłowiu równoległe przetwarzane, bo tak w przeciwności nie jest jak widzicie cały czas ten interpreter realizuje inny wątek. Ale jak widać są cztery wątki czyli nasze cztery questy są przetwarzane.

Teraz zmodyfikujmy delikatnie nasz endpoint I wróćmy do `async`:

```python
@app.post("/decisions")
async def decisions(
```

Ubijmy API i wykonajmy ten sam eksperyment - wyślijmy kilka requestów.

Zobaczcie tym razem dostaliśmy w logach, że tylko jeden request przyszedł, a o pozostałych ani słowa. I nie pojawił się żaden nowy dodatkowy wątek. Ale zwróćcie uwagę na pierwszy rekord- tutaj widzimy literę R i wykorzystanie CPU w 100%. Czyli nasze requessy przeznaczone są teraz w naszym głównym wątku. 

Okej to umówimy nasz proces i wyjaśnijmy sobie co oznacza ta asynchroniczność.

**WYCZYŚĆ WSZYSTKIE ZMIANY BO POTEM BEDZIESZ ROBIL TO NA REDISIE A NIE NA POSTGRESIE**

Żeby zacząć omawianie tematu asynchroniczności musimy zacząć od tematu tego jaki mamy rodzaje operacji. Możemy wydzielić dwa główne rodzaje

1. Operacje CPU-bounded - Czyli wszelkie operacje które wykorzystują i obciążają nasz procesor czyli wszelkie obliczenia numeryczne, uczenie algorytmów,  przetwarzanie danych - wszystko co potrzebuje naszego procesora do działania.
2. operacje IO-bounded - Czyli wszelkie operacje które zależą od systemów zewnętrznych, czyli na przykład wysłanie zapytania do API, wysłanie danych do bazy danych,  prośba o pobranie danych z bazy danych, czy odczytywanie i zapis danych z dysku. Przy tych operacjach IO-bounded tak naprawdę nasz procesor czeka aż otrzymamy jakieś dane, aż dostaniemy odpowiedź z jakimś API, w każdym razie jest on nieużywany. 

I te operacje IO-bound to operację które jako jedyne można zaprogramować w sposób asynchroniczny.

Mamy teraz zajęcia z API, więc weźmy sobie przykład wysyłania requestów do jakiegoś API. I wyobraźmy sobie że wysyłamy na raz 100 requestów. Gdybyśmy podeszli do tego w sposób synchroniczny to tak naprawdę wyglądało by to tak, że pierwszy request zostałby wysłany, Wasz komputer czekałby aż dostanie dane i dopiero później po otrzymaniu tych danych wysyła kolejnego requesta. I znowu musi czekać aż te dane przyjdą żeby móc wysłać kolejnego requesta. Jak widzicie w tym momencie są to etapy czekania, momenty w których Wasze CPU nic nie robi, no bo czeka, więc w sumie mógłby robić coś innego w tym czasie.

W takich case’ach używa się przetwarzania asynchronicznego: Które wygląda w następujący sposób. Znowu załóżmy że wysyłamy 100 requestów na raz. Wysyłamy pierwszego requesta, wasze CPU widzi że nie dostało żadnych danych i że musi poczekać,  ale ono nie czeka, tylko wysyła drugiego requesta, znowu patrzy czy dostał dane, nie dostał to idzie dalej, wysyła trzeciego requesta, czwartego itd. A potem co chwila sprawdza czy już przyszły jakieś dane dla konkretnych requestów. I załóżmy że przyszły dane dla pierwszego requesta, więc on wraca do tego kodu i np. dalej przetwarza tego requesta zgodnie z naszą implementacją, załóżmy, że potem przyszły dane dla 5-go requesta - też je przetworzy,  potem dla drugiego i tak dalej i tak dalej. Innymi słowy mógł on przerwać czekanie na odpowiedzi z danego requesta i po prostu pójść do następnego, żeby zobaczyć czy może tam są już dane które może przetworzyć. Jeśli tam nie ma to idzie do następnego. To był podany przykład w przypadku requestów, ale analogicznie ten case można odnieść też również do wysłania danych do bazy danych. Przecież my też czekamy aż dostaniemy odpowiedź z naszej bazy, że te dane zostały porównanie zapisane. Tak samo jak wysyłamy prośbę do bazy danych o pobranie jakiś danych, no to najpierw sql-ka musi zostać przetworzona na silniku bazodanowym i dopiero jej wyniki są nam przesyłane przez sieć do naszego Pythona. To też jest moment kiedy wasze CPU czeka na te dane, więc w tym czasie może robić coś innego. Więc jeżeli macie kod który wykonuje takie operacje to jest to idealny case żeby zrobić to w sposób asynchroniczny. 

Okej to teraz zróbmy tak - przejdziemy sobie do `main.py` - prześledźmy sobie kod na nasz `decisions` i I Sprawdźmy które operacje są CP-bounded a które są IO-bounded I zapiszmy to sobie w komentarzach obok tych kodów:

**PRZEJDŹ LINIJKA PO LINIJKE I ZAPYTAJ SIE CO UWAŻAJĄ NA TEMAT TEJ LINII KODU**

```python
@app.post("/decisions")
async def decisions(
   request: DecisionRequest,
   background_tasks: BackgroundTasks,
) -> DecisionResponse:
   logging.info(f"Received {request=}")
   response = app.state.decision_redis_client.read(request)  # IO-bounded
   if response is None:
       decision = app.state.model.predict_decision(request.to_dataframe())[0]  # CPU-bounded
       response = DecisionResponse(decision=decision)  # CPU-bounded
       background_tasks.add_task(  # IO-bounded
           func=write_to_redis,
           redis_client=app.state.decision_redis_client,
           request=request,
           response=response,
       )
   background_tasks.add_task(  # IO-bounded
       func=write_to_postgres,
       postgres_client=app.state.decision_postgres_client,
       request=request,
       response=response,
   )

   logging.info(f"Returning {response=}")
   return response
```

Dokładnie tak że jak widzicie wszelkie interakcje tutaj z bazą danych są typu IO-bounded,  A z kolei predykcja modelu jak i też generowanie odpowiedzi to są już CPU-bounded.

Na demonstracji weźmy sobie ten case na samym początku, czyli załóżmy że odczytanie Z baz Redis trwa od 5 do 20 sekund. 

Teraz przechodzimy sobie do implementacji naszego redisa do `BaseRedisClient` I teraz żeby coś było przetwarzane w sposób asynchroniczny musimy wykonać dwie rzeczy.

Po pierwsze musimy funkcję nazwać od async:

```python
async def read(self, request: Request) -> Response | None:
```

A następnie w jej ciele musimy zidentyfikować operacje które są typu IO-bound I który powoduje to że czekamy. 

Taką operacją w przypadku redisa jest otrzymanie odpowiedzi od niego czyli nasze

```python
value = self.connector.connection.get(key)
```

Teraz to co wystarczy zrobić to postawić przed wywołaniem `await`. ALE. Możemy to zrobić tylko i wyłącznie tedy kiedy ta metoda `get` faktycznie pozwala na przetwarzane w sposób asynchroniczny. 

Dlatego też jeżeli chcemy korzystać z asynchronicznego kodu musimy skorzystać z tych implementacji które pozwalają na takie programowanie. Tak się składa że Biblioteka `redis` ma moduł `asyncio`. Ja obecnie skorzystałem z kodu `redis`, który pozwala tylko na działanie synchroniczne. 

**WPISZ from redis.asyncio. I POKAZ IM CO TAM JEST**

To jak widzicie dostarczam tej wiele różnych jeszcze pod modułów a w nich są konkretne implementacje dzięki którym możemy używać tych implementacji w sposób asynchroniczny, czyli korzystając z `await`. Ja teraz tego nie będę robił, bo to wymagałoby dosyć sporo zmian w moim kodzie. Natomiast zasymuluje tą sytuację w troszeczkę inny sposób.

Zaimportuję sobie bibliotekę `asyncio` oraz `random`

```python
import asyncio
import random
```

I przed odczytaniem danych z Redisa wstawię sobie taką linijkę kodu

```python

await asyncio.sleep(random.randint(5, 20))

value = self.connector.connection.get(key)

```

Biblioteka `asyncio`, która właśnie służy do tworzenia kodu przetwarzanego w sposób asynchroniczny, dostarcza takiej funkcji jak`sleep`, którą  wywołuje się z klauzulą `await`.  To spowoduje, że nasz procesor dostanie informacje, że właśnie teraz jest operacja czekania na coś, on będzie wiedział że w sumie to może pójść teraz dalej coś innego robić, i dopiero jak ta `sleep` się wykona to procesor wróci do tego momentu w kodzie i zacznie przetwarzać dalej. W ten sposób zasymulowałem sobie że właśnie wywołanie tej metody `read` wiąże się z jakimś czekaniem. 

Teraz wracamy do naszego `main.py` I to co musimy zrobić to to, aby w czytanie z naszego redisa wywołać z klauzula `await`. Dlaczego? Dlatego że teraz ta funkcja działa w sposób asynchroniczny,  bo została zdefiniowana jako `async`. I aby móc korzystać z funkcji zdefiniowanych jako asynchroniczne czyli z klauzulą `async` musimy Użyć klauzuli `await`:

```python
@app.post("/decisions")
async def decisions(
   request: DecisionRequest,
   background_tasks: BackgroundTasks,
) -> DecisionResponse:
   logging.info(f"Received {request=}")
   response = await app.state.decision_redis_client.read(request)
```

Włączmy nasze API i wyślijmy kilka requestów.

I co teraz widać to to że faktycznie te cztery requesty do nas przyszły I każdy z nich zawiesił się na tym etapie związanym z czytaniem redisa. Ale to nie zablokowało nam API, tak jak to było w poprzednim przypadku. Dlatego że te wszystkie requesty zatrzymały się w tym etapie redisa, czyli w momencie kiedy czekamy na odpowiedź z tej bazy danych. A skoro czekamy ,to nasz procesor może zrobić coś innego. I jeżeli w sytuacji czekania przyjdzie nowy request, to nasz proceso po prostu zacznie to przetwarzać, aż do momentu kiedy znowu trafi się na jakiś element czekania - czyli tutaj w tym przypadku będzie to znowu Redis. Dlatego mogę spokojnie wysyłać requesty do tak zaimplementowanego API i każdy z nich będzie przetwarzany w momencie w sposób asynchroniczny.

Jak widzicie  w terminalu związanym z  procesami i wątkami, nie pojawił się żaden nowy wątek. Oznacza to że wszystkie te operacje są wykonywane na tym samym głównym wątku. 

I te same rzeczy można było wykonać w przypadku background które pokazywałem na początku wam - te wysyłki danych do postgresa  też musiały być zdefiniowane z klauzulą `async` i `await` i przy okazji musieliśmy też zmienić naszą implementację na tyle żeby skorzystać z takich implementacji które pozwalają na wysyłanie danych do postgresa w sposób asynchroniczny. 

Czy macie jakieś pytanie co do tej części?

Domyślam się że pewne rzeczy tutaj są dla was trudne, dlatego teraz chciałbym podsumować tą całą wiedzę, którą tutaj wam przekazałem i odnieś się do kontekstu API z modelem uczenia maszynowego, bo to jest bardzo ważne. 

Podsumowując:

1. W dokumentacji Fast API często spotkacie się ze zdaniem że jeżeli nie wiesz na czym polega kod asynchroniczny, to go nie używaj. Czyli nie używaj wszelkich `async` i `await` w swoim kodzie a FASTApi sobie z tym poradzi -> [https://fastapi.tiangolo.com/async/](https://fastapi.tiangolo.com/async/) . I to jest prawda. Innymi słowy jeżeli ten temat był dla was trudny i nie do końca jeszcze zrozumiały, to trzeba sobie wszystko przetrawić w wolnej chwili i trochę tego kodu napisać. Ale generalnie jest tak że, na początku podczas projektowania API zapomnijcie o tych `async` i o `await`-ach, a FastAPI i tak będzie działało sposób asynchroniczny - one sobie z tym poradzi.
2. Jeżeli nabierzecie już praktyki możecie wtedy spróbować zastanowić się, które operacje w naszym API można przetwarzać w sposób asynchroniczny. Jeżeli biblioteki z których korzystacie pozwalają na to, to warto to zrobić dlatego że to może mieć pozytywny wpływ na wasze API w takim sensie że będzie działało szybciej, bo w momencie czekania na coś wasz procesor będzie robił coś innego na przykład przetwarzał kolejnego requesta.
3. Natomiast z idei asynchroniczności w API MLowym rzadko skorzystacie, więc czasami gra może nie być warta świeczki. Dlatego że naszym core-owym działaniem API jest model i generowania predykcji przez niego, a operacje generowania predykcji przez niego są operacjami CPU-bounded i to ta operacja będzie wykonywała się najdłużej w implementacji waszego API. A tej operacji nie możesz przetworzyć w sposób asynchroniczny.
4. Podejście jakie stosuje się w API MLowym, jest to aby używać podejścia wieloprocesowego, a nie wielowątkowego. Dlaczego nie podejście wielowątkowe? Dlatego że tak sam widzieliście, GIL powoduje to że jest wykonywany jeden wątek w danym czasie i de facto żaden z tych wątek nie wykorzystuje w pełni waszych procesorów. Dlatego stosuje się podejście wieloprocesowe, w którym każdy proces ma swojego własnego GILa i dane operacje numeryczne mogą być wtedy wykonywane już równoległe. Każdy proces będzie miał po prostu swój własny procesor który będzie wykorzystywał w 100%. Generalnie lepiej nie przekraczać liczby procesów nie zwiększa liczbę procesorów dlatego że one będą znowu Walczyć o swoje zasoby, czyli de facto macie ten sam problem jak w podejściu wielowątkowym..
5. Dlatego przy wzmożonym ruchu w przypadku API MLowego po prostu skaluje się je do góry, to znaczy dokłada się kolejne procesy, tak żeby można było obsłużyć wzmożony ruch. I tego będziemy się uczyć na zjeździe trzecim poświęconym Kubernetesowi.

## Praca domowa

Poleć uczesnitkom aby obejrzeli dwa nagrania:
1. https://www.youtube.com/watch?v=kRy_UwUhBpo&t=1894s
2. https://www.youtube.com/watch?v=MCs5OvhV9S4

## Pytania

Pytanie: Czy istnieje limit threadów? 
Odp: Tak. Fastapi używa run_in_threadpool ze Starlette, a Starlette z kolei `anyio.to_thread.run_sync(func, *args)`. Dokumentacja mówi, że jest [max. 40 threadów](https://anyio.readthedocs.io/en/stable/threads.html#adjusting-the-default-maximum-worker-thread-count) 
