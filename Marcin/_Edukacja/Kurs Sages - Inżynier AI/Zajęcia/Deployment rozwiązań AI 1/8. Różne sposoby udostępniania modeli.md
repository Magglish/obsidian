# Różne sposoby udostępniania modeli
1. Zjazd w ten weekend opiera się o budowe API do modeli - budowa API do modeli to jest preferowany sposób udostępnienia modelu, ale chciałbyn na wstępie wskazać Wam jakie są potencjalne rodzaje udostępniania modeli, jak często się z nimi spotkać i jakie są z nimi problemy:
2. Zhardkowany model - to jest pewien sposób udostępnienia modelu, czyli zintegrowania naszego modelu z jakimś systemem po prostu hardkodując jego postać w systemie. Przykład z życia: w mojej pierwszej pracy w banku, gdzie budowałem modele ryzyka kredytowego, to taki model, który był modelem liniowym, konkretnie regresją logistyczną, był on "wdrażany" w innym systemie w języku C# po prostu zapisując jego postać funkcyjną i sumując iloczyny wag przez cechy i otrzymując w ten sposób predykcję. Może się tak zdarzyć, że takie "wdrażanie" modeli spotkacie, ale to raczej w jakichś firmach z archaicznymi systemami. Jakie to ma wady: przede wszystkim jesteście ograniczeni do bardzo prostych modeli, które można zapisać łatwo jako przemnożenie wag przez cechy (czyli modele liniowe) czy też stworzyć ifologie (czyli wszelkie drzewa decyzyjne), ale bardziej zaawansowane agorytmy jak sieci neuronowe odpadają. Czy to jest zły sposób? Jeśli coś jest głupie, a działa to to nie jest głupie. Aczkolwiek ciężko tutaj mówić o jakimkolwiek "wdrożeniu", bo to nie będzie w żaden sposób przypominać wdrożenia jakiego będziemy się uczyć na tych 4 zajęciach.
4. Jako zapisany obiekt - Innym sposobem z jakim możecie się spotkać to po prostu zapisać model do jakiegoś obiektu i go komuś przekazać, np. kolegom z działu IT i niech to wdrażają. Ale to jest bardzo zły pomysł chociażby z prostego powodu - nie ma informacji o tym jakie biblioteki są wymagane aby uruchomić ten zpicklowany obiekt. Nawet jeśli dostarczylibyśmy razem z modelem listę pakietów, które wymagane by były do tego aby taki obiekt zpicklowany wczytać i używać, to jest ogromne ryzyko, że system w którym te bilbioteki zostaną zainstalowane może mieć konfilikty z bilbiotekami i rozwalić nam całe środowisko. Powiem szczerze, że ja się z takim sposobem nigdy nie spotkałem w rzeczywistości i całe szczęście, ale w wielu różnych dyskusjach chociażby na Reddicie można spotkać się z opisem sytuacji gdy naprawdę Data Scientist przekazywał zpicklowany model do MLowca i "masz go wdróż", albo po prostu wysyłał notebooka z odtworzeniem modelu i myślał, że to wystarczy. 
5. Jako paczka pythonowa - Zatem można się zastanowić, czy powyższy problem możnaby jakoś poprawić? Pomysł do głowy może przyjść następujący - a co gdybyśmy sobie stworzyli paczkę pythonową z naszego projektu w którym ten model zbudowaliśmy i moglibyśmy go udostępnić. To jest jakieś rozwiązanie. Bo tak na prawdę instalując paczkę Pythonową istalujemy wszystkie potrzebne zalezności. W dodatku możemy zaimplementować pewne klasy, które implementują logikę działa naszych modeli. Do tego pobierzmy sobie jeszcze model w wersji zpicklowanej i powinno zadziałać. Niestety nie jest to takie kolorowe z dwóch powodów:
	1. przy wdrażaniu sieci neuronowych biblioteki pytorch i tensorflow mogą potrzebować jakichś dodatkowych komponentów w systemie (np. sterowników CUDA gdybyśmy chcieli je uruchamiać na GPU), których w paczce pythonowej po prostu nie uda się zawrzeć
	2. nie unikniemy problemu z pakietami - jeżeli zainstaluje paczke pythonową z moim modelem, to musze doinstalować wszystkie zależności. A co jeżeli np. nasz system produkcyjny wciąż działą na pythonie 2.7, a modele zostały nauczone na pythonie 3.10? Python 2.7 nie jest już wspierany przez Python Software Foundation, ale np. RedHat który tworzy płatnego Linuxa, wciąż dostarcza update'y do Pythona 2.7. Są pewne systemy, które są tak duże, że przepisanie ich na wyższą wersje Pythona to ogromne przedsięwzięcie. U mnie w pracy są takie elementy. Wiecie... tutaj nawet nie musi chodzić o Pythona 2.7, ale nawet system stworzony i działający jeszcze w Pythonie 3.7 może mieć problemy z naszym modelem stworzonym w Pythonie 3.10.
6. Jako job, offline, batchowy - Ok to może nie udostępniajmy modelu, poprzez przekazywani go komuś, tylko może udostępnijmy jego wyniki. Stwórzmy może jakiś skrypt, który będzie przez nas uruchamiany (job), który będzie uruchamiany poprzez jakiś scheduler raz dziennie/raz w tygodniu (offiline) i będzie przetwarzał jakąś porcję danych (batch) i zapisywał to na jakimś storage albo w bazie danych. Te rozwiązanie jest powszechnie stosowane i spotkacie sie z nim praktycznie wszędzie. Z tym podejściem spotkacie się często w połączeniu z serwisem API do modeli i one nawzajem się uzupełniają. Spotkacie się z takim podejściem najczęściej w dwóch wariantach:
	1. Joby, offline, batchowe - wykonują już wszystkie operacje przeliczające predykcje (np. w nocy) a podczas korzystania z API przez klientów/czy inne serwisy w naszym systemie (np. w ciągu dnia), tak na prawde API pobiera już gotowe predykcje i tylko je wysyła klientowi/serwerom, które z API korzystają. Robi się to po to aby zaoszczędzić na czasie przetwarzania zapytań przez API, po to aby korzystanie z API było przyjemniejsze dla klientów/serwisów - nikt nie lubic
7. Jako część pipelineu - 
8. Jako interaktywna aplikacja - 
9. Jako serwis API - 


10. Dlaczego rozwiązanie API jest wybierane w praktycznie wszystkich przypadkach? 
	1. jest to oddzielny serwis/usługa/komponent w naszym systemie - inni developerzy czy użytkownicy API nie musza nic wiedzieć o tym jak zostało to zaimplementowane. Mają dokumentację co mają wysłać do API aby otrzymąc wyniki z modelu
	2. Jest to oddzielny serwis, zarząðzany przez Nas - wszelkie zmiany i rozwój dotyczy tylko i wyłącznie naszego serwisu. Gdyby model był zintegrowany z jakimś pipelinem/rozwiązanami (TODO: dopisz/popraw po napisaniu całego akapitu) to aktualizacja naszych rzeczy mogłaby spowodować to, że inne przestaną działać, np. aktualizacja biblioteki.
	3. Z reguły API mają konkretne cele - w naszym przypadku nasze API po prostu zwraca wyniki modelu. Wydzielenie takiego API jako mniejszy, oddzielny komponent łatwiej pozwala nam tym wszystkim zarząðzać i wprowadzać poprawki bo wprowadzamy je konkretnie do danego elementu.
	4. Oddzielny serwis/usługa w postaci API pozwala na integrowanie tego z wieloma różnymi systemami.