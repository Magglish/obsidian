# Budowa pierwszego, prostego API

## Punkt startu

```python
from fastapi import FastAPI  
from fastapi.requests import Request  
from fastapi.responses import JSONResponse, PlainTextResponse  
  
import uvicorn  
  
import pandas as pd  
from pathlib import Path  
  
from src.models.credit_score import CreditScoringModel  
  
  
app = FastAPI()  
  
model = CreditScoringModel(path=Path("models/classifier.pkl"))  
  
  
@app.post("/predict_decision")  
async def predict_decision(request: Request) -> JSONResponse:  
    data = await request.json()  
    features = pd.DataFrame({key: [value] for key, value in data.items()})  
    decision = model.predict_decision(features)[0]  
    return JSONResponse(decision.name)  
  
  
@app.get("/")  
def welcome_page() -> PlainTextResponse:  
    return PlainTextResponse("OK")  
  
  
if __name__ == "__main__":  
    uvicorn.run(  
        app=app,  
        host="0.0.0.0",  
        port=8080,  
        workers=1,  
        reload=False,  
    )
```
**Okej jesteśmy w naszym punkcie startu W  w naszym głównym skrypcie w którym będziemy definiować co logikę te działania naszego API,  natomiast to co tutaj Widzicie To generalnie pierwsza jak najprostsza implementacja API która już działa i możemy do niej wysyłać nasze żądania żeby model wydał decyzję o nasze wniosku kredytowym.

  

 tak naprawdę cała korowa logika naszego API mieści się dosłownie w czterech linijkach.  i to jest  jedna z zalet Fast API po prostu te implementacje mogą być naprawdę Fast.  Jednakże Oczywiście to nie jest finalna wersja Jaka powinna trafić na produkcję,  bo w tej implementacji brakuje wiele jeszcze istotnych Rzeczy które będziemy z biegiem czasu dodawać.  ale jak sami zobaczycie Fast API implementacja innych skomplikowane rzeczy też jest  stosunkowo prosta i też stosunkowo mało zajmuje kodu jeśli chodzi o linijki kodu więc generalnie naprawdę  implementacja APi w Fast API jest bardzo przyjemna.

  

Ale zacznijmy sobie od początku.  jak widzicie Mamy tutaj taki obiekt jak zainstalowana instancja Fast API to jest jakby core’owy  obiekt w naszym kodzie i on zawsze powinien być na samym początku dlatego  że do niego będziemy się cały czas odwoływać w dalszym kodzie. 

  

 tak Tu widzicie mamy pewne funkcje które zdefiniowałem jako predict_decision oraz welcome_page,  które zostały udekorowane takimi rzeczami jak post I get o których zaraz sobie powiem więcej co to znaczy.  Krótko mówiąc to powoduje to że jeżeli wyślemy żądania dalszego API właśnie na te końcówki które to jest za określone jak w Play decision czy też ten Slash Zaraz zobaczymy jak to działa,  to dana funkcja zostanie wywołana i  to co zdefiniujemy sobie w returnie po prostu trafi do nas z powrotem.

  

Czyli widzimy że na przykład predestiction mamy tutaj jakiś obiekt request czyli obiekt opisujący to jakie dla nas przychodzą.

 one są odpowiednio zmieniane na ficzery przekazywane dalej do modelu model wykonuje predykcję decyzji i tą decyzję zwracamy na wyjściu.  ten kod Będziemy zaraz Wre fakturować bo jest brzydki ale  pozwala on już zaimplementować podstawą logikę działania naszego API.

  

 tutaj jedno słowo komentarza bo możliwe że po raz pierwszy spotykacie się z takimi określeniami jak `async` oraz `await`.  na razie ich tłumaczyć nie będę to jest w ogóle przedmiot naszego tematu na samym końcu naszego zjazdu najbardziej zaawansowanego czyli różnice pomiędzy przetwarzaniem asynchronicznym,  wielowątkowym czy wieloprocesorowym,  więc na razie zignorujcie to Jeżeli będzie potrzeba żeby dodać taką klauzulę `async` lub `await`  to wam o tym powiem ale na razie  pomijamy to i nie przejmujmy się tym.

  

na końcu mamy też  kawałek kodu który jest standardem w pythonie Czyli co ma się zadziać w momencie kiedy ten dany skrypt uruchomimy  z poziomu terminala i w tym przypadku używamy  takiej biblioteki uvicorn która będzie krótko mówiąc  odpowiedzialna za to że my nasze API zostało uruchomione jako pewien proces działający na naszej maszynce  i ten uwikor odpowiedzialny jest też za to żeby dane API było dostępne na konkretnym hoście czy na porcie  z Pewną ilością workerów.  generalnie my ten  temat uvicorn poruszymy znacznie później naszym zjeździe,  teraz nie będziemy tego omawiać po prostu jest to kawałek kodu który  musi zostać umieszczony tak żeby nasze API po prostu się uruchomiło.  Czyli zobaczcie on w argumencie App przyjmuje instancję  naszego Fast API i po prostu to uruchamia.  na razie tyle o uvicornie.

  

Zanim uruchomimy sobie nasze API zobaczymy w ogóle czy to działa i co Fast API nam dostarcza tak Out of the box to zobaczmy sobie jeszcze na tą implementację klasy CreditScoringModelu  tak żebyśmy mieli  odpowiedni obraz to co dokładnie implementujemy i jak to działa. 

  

Tak wymówiłem pisałem repozytorium z myślą o maksymalnej prostocie tak żeby się nie musieli tutaj spędzać nie wiadomo ile czasu analizując kod,  więc  szybciutko sobie spojrzymy na to jak ta informacja wygląda.  generalnie w wdrożeniach modeli uczeni maszynowego,  bardzo często spotkacie się z podejściem w którym definiowane są pewne abstrakcje,  odpowiednie klasy  które po prostu  oddzielają jakby logikę biznesową działania modelu od samego modelu. i tutaj mam taki przykład implementacji modelu scoringowego który po prostu akceptuje to jaki model chcemy sobie załadować i jaka jest ten decyzja  threshold definiująca prawdopodobieństwo powyżej którego ten wniosek że to po prostu nie przejdzie.

  

Metaload model po prostu model wczytuje z danej lokalizacji jak widzicie Type hinty wskazują na to że dostaniemy po prostu Regresję Logistyczną. Ale generalnie tutaj mógłby wylądować jakikolwiek Inny algorytm który  poznaliście wcześniej  i który został nauczony na tych samych danych,  Natomiast ja dla prostoty po prostu Założyłem że dysponujemy modelem regresji logistycznej. 

  

Abstrakcja którą to jest zdefiniowałem po prostu dostarcza trzech metod takich jak

1.  możliwość predykcji prawdopodobieństwa dla naszego wniosku kredytowego
    
2.  możliwość predykcji samej decyzji czy jest ona pozytywna czy negatywna
    
3.  i predykcji tak zwanej kategorii ryzyka.  dla informacyjnie na przykład  w bankowości bardzo często się taką  kategoria ryzyka wyznacza oprócz samej decyzji o akcepcie czy nie tektura jednoznacznie określa też również marży i prowizje dla banku.  im klient jest w bardziej ryzykownej kategorii,  tym więcej musimy na nim zarobić  aby  finalnie wyjść z zyskiem przy danym poziomie ryzyka.
    

  

Każda z tych metod zwraca , prawdopodobieństwa jako float, decyzji jako obiekt Decision czy kategorii ryzyka jako RiskCategory, Dlatego że zakładamy że argument features będzie naszym pd.DataFrame’m  w którym po prostu może być więcej niż jeden przypadek.

  

Natomiast od razu wam mówię jedną rzecz ciekawą dlatego że obecna implementacja tej tego obiektu i tych metod pozwala na właśnie wykonanie predykcji dla nieograniczonej ilości rekordów jakie podamy tutaj jako features. Natomiast przeważnie serwisy API które będziecie tworzyć  dla swoich modeli przeważnie mają logikę taką że przyjmują jeden przypadek do oceny. 

  

Batch processing, czyli możliwość przetwarzania wielu requestów na raz jako batch/jako grupa requestów, a nie każdy odzielny, jest generalnie trudne do implementacji. Dlatego, że na produkcji pojedyncze requesty będą najczęściej dotyczyć pojedynczyk przypadków do oceny przez model, więc będzie musieli stworzyć pewną implementację kolejkowanie i zbierania requestow i dopiero jak sie zbierze odpowiednia ilosc do wpada dalej do modelu, a potem trzeba response rozeslas odpowiednio do klientow/serverow. - to jest temat bardzo zaawansowany i dopiero przy dobrej znajomości FastAPI można to zaimplementować z sukcesem. My tego na tym zjeździe robić nie będziemy. Ale w rzeczywistości najczęściej spotkacie się w takiej sytuacji, że jeden request oznacza jeden przypadek do przetworzenia i to w większości przypadków wystarczy.
    
2. Druga sprawa jest taka że FastAPI,  co wam pokażę na samym końcu,  potrafi  przetwarzać wiele requestów na raz wysłanych do niego. Więc po prostu zamiast wysłać 1  prośbe  o wykonaniu 10 decyzji przez nasz model dla 10 wniosków po prostu możemy wysłać 10 próśb/requestów każda z nich z 1 danymi o jednym wniosku I Fast API sobie z tym świetnie poradzi.
    
3. Przyspieszanie działania naszego API w przypadku kiedy mamy naprawdę dużo requestów też poznacie na zjeździe 3 poświeconym Kubernetesowi, Bo generalnie jeżeli Czujemy że nasze API działa wolno i myślimy o przetwarzaniu baczowym czyli właśnie tych wielu wniosków kretowych na raz,  to ja zawsze rekomenduję prostą rzecz Przecież zawsze można zeskalować ilość instancji do góry i nie mieć jednej instancji z API tylko miec więcej. Ale dobra, bo zaczynam odbiegać od tematu.
     

Krótko mówiąc: większość API projektowanego do modeli uczenia maszynowego będzie miało przetwarzanie jednego requestu z jednym przypadkiem i to w zupełności wystarczy. Natomiast przetwarzanie batchowe jest już trudne i na pierwszy kontakt z implementacją API do modelu byłoby moim zdaniem zbyt trudne. A jest wiele innych tematów, które warto znać na początek.

(Potencjalne rozwiązanie: [Middleware](https://stackoverflow.com/questions/73164286/fastapi-group-multiple-requests-and-batch-execute))

  

Spójrzmy sobie jeszcze na outputy z naszego modelu.

  

To są po prostu proste enumy które definiują nasze outputy z modelu, Czyli mamy decyzję która jest po prostu dwoma stanami Akcept i Decline.

  

 oraz troszkę bardziej zaawansowany enum który określa kategoria ryzyka,  który ma w sobie tuple tak naprawdę czyli treść Tej kategorii oraz thresholdy dla każdej kategorii.

  

I najważniejsze metoda w tym enomie to jest dodatkowy konstruktor `from_probability` który po prostu Na podstawie samej wartości prawdopodobieństwa tworzy nam kategorie ryzyka do której wniosek należy.  te enumy które tutaj stworzyłem oczywiście jest to dobra generalnie  dobra praktyka i z enumami spotykacie się wszędzie,  natomiast Jak się niedługo przekonacie Fast API skorzysta z faktu że te wyniki z modelu są enumami,  dlatego że na podstawie  ich implementacji Fast API wygeneruje automatycznie dokumentację. 

  

Okej czyli mamy pokrótce omówioną implementację naszego modelu jest ona myślę bardzo prosta chyba że macie jakieś pytania do tego momentu?

  

Dobra to słuchajcie teraz włączmy sobie nasze API poprzez:

  

```bash

export PYTHONPATH=`pwd`

make api

```

  

A w drugim terminalu po prostu wyślijmy sobie requesta

  

```bash

make request

``` 

  

Okej widzimy że API działa dostaliśmy odpowiedź Decline.  wrzućmy sobie jeszcze okiem na `scripts/send_example_request.py`.

  

Co sprawa jest prosta określamy jakie dane chcemy wysłać naszego API po to aby uzyskać odpowiedź zwrotną czyli w tym przypadku będzie to nasza decyzja No jak widzicie tutaj mamy dane odnośnie z naszego wniosku kredytowego. 

  

Tutaj oczywiście rzecz Myślę że jasne dla wszystkich,  że do API przychodzi to co faktycznie wchodzi w skład naszego modelu czyli w ramach wysyłanych zapytań do API muszą być takie dane jakie stanowią input do naszego modelu. 

  

 Natomiast w praktyce oczywiście do waszego API mogą być wysłane złe dane i pytanie Jak sobie z tym poradzić. Albo dane poprawne ale niekompletne.  to Będzie temat naszych poprawek API niedługo.

  

 następny mam taki obiekt jak `headers`  o czym będziemy mówić później ale krótko mówiąc są to pewne metadane które określają dodatkowe informacje o naszych danych które wysyłam do API.  w tym W przypadku można się po prostu łatwo domyślić że dane przychodzą w formacie JSON. I generalnie rzecz biorąc  w przypadku API REST-owym, które uczymy się tutaj definiować. Przeważnie dane będą wchodzić w formacie JSONowym. A jak się sami przekonacie przypadku serwisów napisanych Fast API jest to wymagane aby były w formacie JSONowym bo inaczej nie zostano przetworzone.

  

Aczkolwiek jest pewien serwis który będzie wyjątkiem o tej reguły i zobaczycie to w  w przypadku implementacji API dla modelu YOLO,  dlatego że tam jednym z ćwiczeń będzie zaimplementowanie API w taki sposób aby przyjęło obrazek w postaci streamu bajtów.  i wtedy dane wysłane do opinii nie będą w postaci Jasona tylko będzie to stream Bytów.

  

Dalej oczywiście sprawa prosta określamy nasz URL.  tutaj lokal host mówi konkretnie o o naszym localnym komputerze, Magiczna wartość 8080 o tym Będę mówił w odpowiednim momencie.  i końcówka czyli nasz endpoint Pretty decision który sobie zdefiniowaliśmy w main.py. 

  

Uzyskam odpowiedź i wypisujemy sobie nasz status Code o których teraz powiem oraz formatujemy sobie odpowiedź Jasona żeby zobaczyć co otrzymaliśmy ostatecznie.  i  widzimy że dlatego przypadku po prostu decyzja jest negatywna. 

  

To co jeszcze chciałbym żebyście zrobili To wróćmy sobie do tego terminala w którym te API uruchomiliśmy i zobaczysz demo takie informacje że nasze API działa na tym adresie jest on możliwy do kliknięcia.  i Kliknijmy sobie w nie. 

  

Jak widzicie przywitałem do tej okienko z napisem Okej czyli to co tutaj sobie nie widzieliśmy w funkcji Welcome Page.  ale oprócz tego jest jeszcze jedna rzecz którą chciałbym teraz pokazać czyli Wejdźmy sobie w endpoint `/docs`. Jak widzicie mamy tutaj  powoli tworzącą się dla nas dokumentację na podstawie tej implementacji otwartej tworzycie.  na  na razie ona jest strasznie uboga I generalnie nic nie nie wynika ale to dlatego że nasza implementacja jest też uboga.  jak ktoś nie lubi swaggera  to endpoint `/redoc`  i tam jest już dokumentacja w redoku.  ja na przykład bardziej preferują redoca niż swaggera ale to jest indywidualna preferencja. Także Dodaj dokumentacji będziemy zaglądać co jakiś czas wraz z kolejnymi prezentacjami tak żebyście widzieli jak Fast API otworzy na podstawie waszego kodu. 

  

I tak Dokumentacja jest bardzo ważna dlatego że ona będzie stanowić pierwszy element w który każdy będzie zaglądał Aby użyć waszego API Więc ona musi być dobra i dopieszczona więc będę wam pokazywał Jak pisać odpowiednie implementacje tak żeby Fast API spokojnie renderowało to dokumentację w zrozumiały sposób  dla pozostałych użytkowników. 

  

I jeszcze jedna bardzo ważna rzecz to taka że w Data Science jesteśmy przyzwyczajeni do tego  aby No pracować w notebooka. One są bardzo wygodne Ja też jej używam do tego żeby sobie właśnie analizować dane albo eksperymentować z pierwszym implementacjami dlatego że dostajemy tam wyniki w komórkach można je łatwo podejrzeć,  i ten no ta przyjemność wskazania jest zupełnie inna niż takie odpalanie skryptu w terminalu.

Natomiast tutaj nie będziemy pracować notebooka No nie da rady napisać kodu Fast API który uruchomi się w notebooku niestety.  więc pytanie jak można Mieć chociaż takie na taką namiastkę pracę w notebooku także można było sobie zobaczyć wartości w odpowiednich zmiennych,  pobawić tymi obiektami tak żeby ta implementacja ostatecznie działała.  przy kwestii tego z pomocą przychodzi wam debugger, a w szczególności PyCharmowy debugger.  i chciałem wam pokazać jak tego użyć. 

  

Oznaczmy sobie tą linijkę z obiektiem `decision`,  a następnie  następnie obok `if __name__`  wybierzmy `Modify Run Configuration` i w Working Directory usuńmy `src/service`, żeby na końcu ścieżki został tylko nasz główny folder!

  

Następnie wracamy, odpalamy `Debug main` i w innym terminalu wysyłam sobie requesta poprzez `make request` i trafiamy na ten breakpoint ustawiony i możemy teraz z poziomu konsoli bawić się tymi obiektami i danymi, które przyszły do naszego API i w ten sposób mieć jakie troche podobne doświadczenie jak w Notebookach, ale wiadomo, ze to nie to. Ja debugera czesto używam własnie po to aby developować kod na API, widze co przychodzi, i w jakiej strukturze i wtedy zdecydowanie łatwiej jest napisać poprawną implementację. 

  

Jak chcemy Zakończyć to koniecznie kliknijcie tutaj w ten czerwony przycisk `Stop main`  a potem jeszcze raz, Żeby zabić w ogóle ten cały proces ten mam pewność że nasz API już nie działa.

  

UWAGA JEST BUG W NAJNOWSZEJ WERSJI PYCHARMA

Jest tam Workaround podany

https://youtrack.jetbrains.com/issue/IDEA-331676/Debugger-fails-with-FastAPI-uvloop-TypeError-Task-object-is-not-callable

  

Okej to był taki wstęp jeśli chodzi w ogóle o tą prostą implementację to widzicie Czy macie jakieś pytania do części zanim zaczniemy dalsze pracę nad naszym API?**