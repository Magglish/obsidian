# 37. Kwestie sieciowe i komunikacja z serwisem

Natomiast to jest idealny Case Żeby porozmawiać sobie o zagadnieniach sieciowych to znaczy,  jak to jest że nasz request z komputera trafił do tej maszynki,  trafił do naszego kontenera,  a  potem trafił do naszego API i następnie ta odpowiedź wraca do nas i Widzimy ją tutaj w konsoli. 

  

(ZRÓBMY TO NA JEDNYM SLAJDZIE)

  

Jak spojrzymy są na slajd to jesteśmy tutaj, Wysyłamy sobie requesta czyli nasze żądanie z danymi do naszego API i otrzymał odpowiedź.  natomiast droga którą musi pokonać request jest bardzo długa zanim w ogóle trafi do nas. 

  

Zaczniemy sobie od naszych elementarnych rzeczy. Przy maszynce widzicie takie informacje dwie jak internal IP lub external IP. Na pewno każdy z was słyszał o IP,  Więc bez rozwodzenia się w szczegóły  IP to po prostu jest identyfikator danego zasobu w sieci. Nasze VMki mają takie IP jak widzicie, I generalnie wskazano na te ipki możemy wysłać żądanie Właśnie do nich.  Przy czym jeżeli spróbujemy wysłać żądanie do ipka Internal  to oni dostaniemy odpowiedzi,  a jeżeli wyślemy IPka do external to wtedy dostaniemy.  Powiem o tym więcej później. Generalnie każdy zasób w sieci,  czy mówimy tutaj o VMce,  czy odpytujemy zwykłą stronę internetową w przeglądarce, Każdy ten zasób w sieci ma pod sobą jakieś IP,  które jednoznacznie identyfikuje  zasob w sieci.  jeżeli pytam jakkolwiek stronę internetową na przykład wp.pl  to też wp.pl ma IPka. 

  

(JEŚLI BRAKUJE NA VM TO DOINSTALUJ `sudo apt-get install dnsutils`)

  

Znajdźmy sobie w takim razie ipka strony wp.pl.  w tym przypadku możemy posłużyć się komenda `dig`

  

```bash

dig wp.pl

```

  

Ale jak widzicie dostajecie bardzo mnóstwo różnych informacji  które Na ten moment w ogóle nie chcę wam potrzebne jest opcja skrócona która zaraz wam tylko ipika

  

```bash

dig +short wp.pl

```

  

W efekcie otrzymujemy IPka.  i skorzystać  z niego żeby odpytać wp.pl.

  

Niestety nie udaje się to zrobić dlatego że część stron  internetowych jest hostowana w taki sposób że nie pozwalają oni na odpytywanie ich bezpośrednio przez IP więc w tym przypadku się tego nie uda.  ale na przykład Google czy przeglądarka google czy przeglądarka duckduckgo na to pozwala więc spróbujmy to zrobić:

  

```bash

dig +short google.com

```

  

Sprawdźmy googlea.  widzimy że działa,  oraz sprawdźmy naszego duckduckgo

  

```bash

dig +short duckduckgo.com

```

  

też widzimy że działa. 

  

google oraz duckduckgo pozwala na odpytanie ich strony bezpośrednio przez IPka.

  

Jest nawet taka strona: [https://www.whatsmydns.net/](https://www.whatsmydns.net/) gdzie możemy zobaczyć IPki danej domeny, np. dla google.com mamy różne IPki w różnych lokalizacjach. Wpisujemy sobie `google.com` i zostawiamy tutaj literkę `A`. Widać, że w zależności od lokalizacji mamy różne IPki dla strony `google.com`, tzn. że dana strona jest hostowana na wielu różnych serverach w różnych lokalizacjach. Możemy sobie sprawdzić jakiś inny IP-ik np. z USA i sprawdzić czy uda się odpytać.  Powinno się udać. Ok jest.

  

Okej dobra. Więc podsumowując IP jest identyfikatorem zasobu w sieci. 

  

 Czyli widzimy mamy IPki naszych maszynek wirtualnych, Które jednoznacznie identyfikują je w sieci. Natomiast widzimy dwa Internal oraz External. Internal IP  jest to IP który identyfikuje naszą maszynkę wirtualną ale w naszej sieci googlowskiej,  i to oznacza że tylko i wyłącznie inne serwisy które działają w naszej chmurze,  w naszym projekcie googlowskim  będą w stanie komunikować się z tą maszynką wirtualną Korzystając z tego IP.  i ten IP jest widoczny tylko i wyłącznie dla właśnie usług,  serwisów,  i innych rzeczy wdrożonych na naszą chmurę. I generalnie jest możliwość  stworzenia wirtualnej maszynki która ma tylko i wyłącznie ten IP wewnętrzny. I  w sytuacji kiedy chcemy żeby  nasze modele były tylko i wyłącznie osiągalne w naszej wewnętrznej sieci, w chmurze, to podczas tworzenia  to  wskazujemy. Natomiast my tutaj nasze potrzeby Chcieliśmy żeby  nasza VMka była osiągalna z zewnątrz tak żeby każdy z nas mógł ją odpytać. Czyli generalnie  wewnętrzętrzny i zewnętrzny. Czyli my musimy skorzystać z iplika zewnętrznego,  bo on jest dla nas osiągalny. Natomiast serwisy które są u nas na chmurze  mogą skorzystać z IPka wewnętrznego jaki i zewnętrznego. Przy czym lepiej oczywiście żeby korzystały one z IP wewnętrznego,  dlatego że ipic zewnętrzny może w każdej chwili zniknąć jeśli tego chcemy albo zmienić się,  w sytuacji kiedy na przykład nasza maszynka wirtualna się zrestartuje.  albo wyłączymy ją na jakiś czas potem do niej wrócimy i IP też się zmieni. Dla zainteresowanych,  technika mapowania  IP wewnętrznego na ip-a zewnętrzne  nazywa się NAT czyli network adrress translation. Dla zainteresowanych polecam więcej poczytać sobie po prostu o tym czym jest NAT,  My natomiast wchodzić nie będziemy w szczegóły teoretyczne zajmiemy się tą stroną bardziej funkcjonalną i wdrożeniową. 

  

No dobrze mamy w takim razie identyfikator w sieci.  ale na naszej maszynce No może działać wiele różnych modeli,  i teraz jak odróżnić że nasz request wpada właśnie do naszego modelu konkretnego a nie innego który może działać na tej maszynce.  i tutaj wchodzi pojęcie portu które w przypadku IPka  jest zawarta na samym końcu po dwukropku. Na przykład mamy to w naszej zmiennej `url` w `send_example_request.py`. Port to z kolei nic innego jak mapowanie konkretnego procesu który działa w naszym systemie.  czyli oprócz wskazania ipk który określa zasób w sieci musimy też wskazać port który mówi nam o tym do którego procesu działającego w naszym systemie ma ten request przyjść.  o jaki procesach mówimy: Możemy sobie łatwo sprawdzić naszą komendą:

  

```bash

ps aux

```

  

Która pokazuje nam wszystkie procesy działające na naszej maszynce.  i teraz zobaczcie.  Ja uruchomiłem sobie ten kontener  i kontener to nic innego jak działający proces w naszym systemie. Możemy go sobie odszukać:

  

```bash

ps aux | grep "docker run"

```

  

Czyli port  jest po prostu mapowaniem do naszego procesu.  czyli wysyłając dane  do konkretnego IPka  i wskazując konkretny port  po prostu wysyłamy dane do tej maszynki wirtualnej I te dane dalej zostaną przekazane do konkretnego procesu który działa u nas w systemie. W tym przypadku te dane trafia do naszego kontenera.  A nasz kontener  otrzyma te dane,  wyślę je do API,  Apis wróci odpowiedź i dalej będzie  to przekierowane z powrotem aż trafi do klienta/serwisu, który wysłał zapytanie.

  

Dlatego też  podczas uruchomienia naszego kontenera  wymagane  było określenie  portu  Korzystając z `-p`. 

  

Przy czym wartości jakie przekazywaliśmy były `-p 80:8080`

  

Czyli w sumie mamy dwie wartości i `80` i `8080`. Pierwsza wartość przed dwukropkiem,  oznacza Port na naszej maszynce wirtualnej.  A z kolei druga wartość po dwukropku  oznacza port w kontenerze. Dlaczego tak jest? Dlatego że w naszym kontenerze mogą działać różne procesy, nie tylko samo api Dlatego też wewnątrz kontenera trzeba też wskazać port  który  mapuje nam o który proces w kontenerze chodzi.  dlatego uruchamiamy nasz kontener z `-p 80:8080`,  Co oznacza że request który przyjdzie do naszej maszynki  do portu 80 zostanie przekierowany do kontenera,  a dalej do procesu w kontenerze  który jest zmapowany na porcie 8080,  a w naszym przypadku będzie to nasze API. Skąd kontener wie że chodzi o nasze API. To wynika z dwóch rzeczy,  w Dockerfile  mamy  instrukcję `EXPOSE 8080`,  Która właśnie oznacza że port 8080 jest otwarty to znaczy że dane mogą do niego przychodzić.  Bez tej instrukcji  oznacza to że ten port nie jest otwarty więc Gdybyśmy wysyłali te dane do kontenera na porcie 8080 który nie ma `EXPOSE`  to nic mi nie dotarło. Ta instrukcja mówi coś w stylu: Pozwalamy żeby dane docierały do kontenera na port 8080.  a teraz Spójrzmy na nasz kod w `src/service/main.py` w ifie `if __name__ == "__main__":` - Mamy tej funkcje która uruchamia nasze API  i zwróćcie uwage na  argument port.  to oznacza że uruchamiamy nasz API jako proces który będzie przyjmował dane na porcie 8080.  zatem dane przekazane do kontenera na port 8080  spowodują że te dane trafią do naszego API,  dlatego że w  argumentach tej funkcji  która jest odpowiedzialna za uruchomienie naszego API  wskazaliśmy port 8080.  czyli nasze API oczekuje danych które będą przechodzić na port 8080. 

  

Czy to jest w miarę jasne, czy jeszcze raz wytłumaczyć?

  

Teraz pytanie czemu port 8080 i czym możemy zdefiniować sobie inny port? Oczywiście możemy sobie zdefiniować jakikolwiek port inny,  może to być 8081. 9090, 8888. Macie pełną dowolność, z pewnymi wyjątkami o których zaraz powiem. Można to być port 5432 który na przykład postgres ma,  Redis otwiera port 6379. To który wy port otworzycię to zależy w zupełności od was. Natomiast są pewne restrykcje co do portów bądź też może takie standardy rynkowe.  zakres liczbowy od portów to jest od 0 do 65535.  przy czym w zakresie od 0 do 1023 Są porty zarezerwowane  dla pewnych aplikacji.  zarezerwowane to można duże słowo bo oczywiście można otworzyć nasze  API na porcie 22, nie ma z tym żadnego problemu,  Natomiast lepiej tego nie robić  bo Są pewne Aplikacje które domyślnie  otwierają swoje porty na konkretnych liczbach i lepiej  ich nie zajmować przez nasze aplikacje.  dalej są porty od 1024 do 49151 Które spokojnie możemy okupować przez nasze  serwisy,  możemy sobie wybrać dowolną wartość. A porty 49152 od 65535 są dynamicznie alokowane przez działanie naszych aplikacji. Generalnie dobrą praktyką jest poruszać się w zakresie liczbowym w zakresie 1024 do 49151. Dlaczego My wybraliśmy 8080. To jest po prostu standard rynkowy,  pierwsza myśl jaka przychodzi jeśli Pomyślimy o porcie dla naszej aplikacji tudzież API. Jest to wręcz taka domyślna wartość dla  określenia  portu dla naszej aplikacji.  natomiast jak powiedziałem możecie sobie tutaj ustawić dowolny jaki chcecie,  przy czym trzymać się w tym zakresie. Natomiast ja jednak polecam ten port 8080 pod warunkiem że w kontenerze jest tylko 1 proces który będzie korzystał z tego portu. Dlatego że na przykład  jest wiele różnych serwisów  z których na co dzień będzie korzystać i  i te serwisy mają domyślnie ustawione fakt że wasze API będzie na porcie 8080. Defaultowa wartość w prawie wszystkich usługach/serwisach. Więc jeżeli wasze API też będzie otwarte na porcie 8080  to macie mniej rzeczy do konfigurowania żeby wasze API działało. Nie bo polecam wam standardowo używać tego portu. W przypadku  jeżeli zmienimy jakiś port na inną to trzeba prześledzić  to jak ta komunikacja idzie krok po kroku  i upewnić się że faktycznie  we wszystkich krokach  pośredniczącyh  pomiędzy  serwerem/klientem,  który wysyła do naszego API requesta  ten nasz port customowy który wybraliśmy jest faktycznie dostępny.  zaraz o tym powiem więcej,  jak bedziemy sobie śledzić ruch naszego requesta.

  

Natomiast my tutaj jak widzicie Przy wdrażaniu rozwiązania na VMce w chmurze my jednak skorzystaliśmy z portu 80 na maszynce wirtualnej, a nie 8080. Ma to związek z tym że firewall googlowski po prostu otwiera ruch na porcie 80 domyślnie, a nie na 8080 w momencie kiedy zdeployujemy VMkę z domyślnymi ustawieniami. Możemy oczywiście odblokować firewalla tak żeby port 8080 był dostępny, Ale nie chciałem tego wprowadzać dodatkowych ustawień bo też uznałem sobie ciekawy case żeby zmienić sobie port. 

  

Teraz Dlaczego Google postawił na port 80 a nie na port 8080. Tak jak wspomniałem porty od 0 do 1023  są już w cudzysłowiu “zarezerwowane” przez pewne usługi. Port 22 o którym wcześniej wspomniałem,  jest na przykład portem  na którym domyślnie  działa nasze SSH, I dzięki któremu mogliśmy się połączyć z naszą VMka. Więc Gdybyśmy nasze API  wdrożyli na port 22,  a potem próbowali połączyć się poprzez ssh z naszą VMką,  No to  dane  nie trafiałyby do  procesu związanego z połączeniem się z wiemką tylko do naszego API. Więc generalnie Lepiej nie ruszać tych portów pomiędzy 0 a 1023  bo na tym działają pewne krytyczne usługi  i Możecie w ten sposób popsuć sobie komunikację z waszymi wirtualnie maszynkami na których są wasze modele.

  

Sposród tych portów od 0 do 1024, są dwa istotne porty które żeby żebyście zapamiętali. To jest właśnie port 80 oraz port 443, Które zostały  zarezerwowane w cudzysłowiu Dla aplikacji z którymi możemy się połączyć Korzystając z protokołu HTTP i HTTPS. Czyli wracając do naszego poprzedniego zjazdu, my  wysyłając requesty do naszego API Korzystamy z protokołu HTTP. I w momencie tworzenia wirtualnej maszynki ja powiedziałem do Googla że chce żeby on  pozwolił mi na ruch z zewnątrz do tej VMki  i będę komunikował się z nią poprzez protokół http. Tak ustawiłem podczas tworzenia VMki.  i w ten sposób Google podczas tworzenia tej wirtualnej maszynki,  i tak samo będzie w przypadku innych chmur,  odblokowujemy port 80. Co oznacza odblokowanie? Po prostu firewall Google’wski nie bęðzie blokował requestów, które przychodzą na ten port.  Czyli wszystko co wyśle do ipka mojej wirtualnej maszynki  na port 80  zostanie później przekierowane  do procesu  w tej wirtualnej maszynce,  który jest powiązany z tym portem.  A co zostało powiązane z tym portem? Nasz kontener z naszym API.  więc  request został przekierowany dalej do naszego kontenera.  w naszym kontenerze mamy otwarty port 8080  który akceptuje dane  przesyłane do niego.  no i na tym porcie 8080  działa nasze API,  które stworzyliśmy.  dostało dane,  zwróciło predykcję  i następnie z powrotem te dane są wysyłane do nas  tą samą ścieżką którą ten request przyszedł.

  

Ale oprócz tego jest jeszcze port 443 który mówi o komunikacji po https,  temat https jest bardziej zaawansowanym tematem  i po prostu  trzeba mieć certyfikat,  najlepiej jeszcze zarejestrowano domena i tak dalej.  My postaramy się to zrobić dopiero na zjeździe  3-cim poświęconym Kubernetesowi.  Na tych naszych VMkach np. ruch na VMce na porcie 443 jest zablokowane,  bo w momencie tworzenia VMki po prostu  nie chciałem żeby ten port był otwarty. Gdybyśmy chcieli otworzyć inny port na VMce, np. właśnie ten 8080 No to musielibyśmy sami  zmodyfikować opcję firewalla w Google żeby nam pozwolił na ruch  do tej VMki na konkretnym porcie, ale my nie Będziemy się tym zajmować, to będzie robił za nas Kubernetes w przyszłym zjeździe.

  

Czy IP i port jest w miarę jasny dla Was?

  

Teraz pytanie jakie można sobie zadać, czy można jakoś sprawdzić na VMce czy są otwarte porty?

  

Zobaczmy google’a

  

```bash

dig +short google.com

```

  

Następnie użyjmey netcat’a, kolejne narzędzie do analizy sieciowej.

  

```bash

nc -zvw5 <<IP_GOOGLE>> 443

```

  

Jest otwarty port do komunikacji po HTTPS.

`-z` oznacza, że nie tworzył połączenia a po prostu zwrócił informacje czy takie połączenie jest mozliwe do nawiązania

`-v` od `verbose` żeby wypisał informacje

`w5` poczekaj maksymalnie 5 sekund na nawiązanie połączenia.

  

```bash

nc -zvw5 <<IP_GOOGLE>> 80

```

  

Jest otwarty port do komunikacji po HTTP

  

```bash

nc -zvw5 <<IP_GOOGLE>> 8080

```

  

Ale na przykład port 8080 jest niedostępny.

  

Sprawdźmy czy port na naszej VMce jest otwarty:

  

Czy 80 jest:

  

```bash

nc -zvw5 <<IP_VMKI_NA_GCPIE>> 80

```

  

Czy 443 jest:

  

```bash

nc -zvw5 <<IP_VMKI_NA_GCPIE>> 443

```

  

Czy 8080 jest:

  

```bash

nc -zvw5 <<IP_VMKI_NA_GCPIE>> 8080

```

  

Jeszcze jedna rzecz: jeżeli wpiszemy sobie w adres strony IP GOOGLEa albo sam adres google.com to zobaczcie, ze nie wpisuje tego z portem. Dlatego że domyślnie przeglądarka Stara się odpytać po porcie 443 czyli po protokole HTTPS aby nawiązać bezpieczne połączenie. Jeżeli go nie ma to próbuje na porcie 80, ale wtedy dostajecie informacje, że nie może nawiązać bezpiecznego połączenia i wyskakuje Wam taki czerwony komunikat, że strona jest niebezpieczna. Jeżeli chcemy sprawdzić konkretny port to wystarczy go w adresie strony Po prostu wskazać. 

  

Okej to w takim razie wiemy czym jest IP  i wiemy czym jest port.  to teraz przejdźmy dalej to znaczy jak wygląda droga requesta od klienta/serwisu  do naszego API  które Siedzi sobie w kontenerze,  a ten kontener jest na maszynce wirtualnej. 

  

Zobaczcie mamy tutaj na początku nas bądź jakiś server który będzie korzystał z naszego API, A po prawej mamy nasze API. Czy komunikacja To znaczy ta Droga  którą przemierza nasz request jest taka bezpośrednia, tzn. prosto od naszego laptopa do naszego API. Oczywiście odpowiedź brzmi nie,  proces jest znacznie bardziej skomplikowany i dłuższy.  Droga którą request musi przemierzyć jest naprawdę dosyć długa.  i możemy ją sobie mniej więcej sprawdzić jak ona wygląda,  za pomocą dedykowany do tego komendy `traceroute`.

  

Zobaczmy w takim razie jak wygląda droga requesta od nas do google.com.

  

```bash

traceroute google.com

```

  

`traceroute`  Po prostu wysyła requesta z jakąś przykładową paczką danych tylko po to żeby zobaczyć jak wygląda właśnie ścieżka komunikacji  pomiędzy nami a destylacją.  i zobaczcie proszę tutaj tak naprawdę mamy osiem kroków.  pierwszy to jesteśmy my ósemka to jest nasz cel.  czyli w sumie request musiał przejść  przez 6  różnych systemów zanim dotrze do naszego celu. Mam informację na przykład właśnie o tym jaki czas mniej więcej zajął na dotarcie do tego celu w mili sekundach,  ipki tych systemów, czasami też hostnames. Trzy gwiazdki z kolei oznaczają to że no nie mam informacji -  powodem może być to że te miejsce jest na tyle obciążone że po prostu nie dostaliśmy informacji zwrotnej albo  te miejsce ze sobą tak skonfigurowane żeby nie odpowiadać  na dane wysłane przez komendę `traceroute` np. ze względów bezpieczeństwa. 

  

Zobaczmy sobie jak wygląda ścieżka wysyłając requesta do naszej VMki

  

```bash

traceroute <<IP_NASZEJ_VMKI>>

```

  

Wygląda podobnie,  jest wiele różnych punktów przez które nasz request przechodzi.  My oczywiście nie dostaniemy informacji dokładnie z komendy `traceroute` czym to  dokładnie jest, Ale jest ona bardzo pomocna w diagnostyce czy faktycznie ten request przechodzi przez konkretne ipki  o których wiemy że ma przechodzić.  teraz czym te punkty mogą być tak naprawdę.

  

 Wróćmy sobie do naszego slajdu i generalnie  droga od naszego laptopa do naszego API na chmurze może wyglądać następujący sposób:

  

1.  przede wszystkim  laptop połączony jest do routera naszego domowego wi-fi,  może by też podłączony do  WiFi w naszej firmie,  na ten reques wychodzi od laptopa do routera. 
    
2.  zaraz oczywiście W zależności od tego gdzie pracujecie możecie spotkać się z pierwszym punktem o nazwie forward proxy,  czym jest proxy? Krótko mówiąc - to po prostu pośrednik - w naszym przypadku pośrednik w komunikacji pomiędzy nami a naszym celem -  na przykład to może być jakaś usługa zainstalowana,  skonfigurowana  w waszej pracy  która po prostu zbiera requesty od wszystkich  komputerów sieci wewnętrznej  i dalej przekazuje je  do celu.  jest to stworzone ze względów przedewszystkim bezpieczeństwa, Po to aby oczywiście łatwo nie udało nam się zidentyfikować konkretnego użytkownika z którego przychodzi request tak żeby nie był on podatny na jakiś atak,  a po drugie takie forward Proxy ma też w sobie zaimplementowanych Firewalla,  jakieś inne reguły  wykluczające na przykład nie możecie wejść na jakąś stronę, nie możecie pobrać pakietów do Pythona, bo ruch sieciowy jest blokowany,  to ma wiele różnych celów. 
    
3. Następny jak spojrzymy sobie na naszego traseruta to czasami są informacje podane że to trafia do jakiś tam systemów wewnętrznych dostarczenia internetu UPC Orange cokolwiek,  czasami ta informacja się pojawia czasami nie ale generalnie jak widzicie droga jest dosyć naprawdę Długa,  to co jest po środku może być naprawdę bardzo długie skomplikowane i o tym będziecie mało wiedzieć  Nie wnikamy w to. 
    
4. Natomiast jak już dojdzie ten request do chmury googlowskiej,  to tam też jest szereg oczywiście różnych  po drodze Systemów które monitorują nasz ruch,  odpowiednio przekierowują i tak dalej,  ta cała sieć może być o tyle bardzo skomplikowana.  natomiast na  w końcu ścieżki jest bardzo ważny element który  też będziemy definiować w przyszłym zjeździe posiedzeniu Kubernetesowy,  jest to tak zwane Reverse Proxy. Na początku mieliśmy Forward Proxy,  a teraz mamy Reverse Proxy, Generalnie usługa/serwis Który ma zadanie przyjąć requestry przychodzące do naszych serwisów,  a potem odpowiednio je dystrybuować do konkretnych celów.  podobnie jak przy forward proxy,   cele są podobne -  kwestie bezpieczeńśtwa  bo możemy sobie tam ustawić  wszelkie reguły,  firewalle Oraz cele tak zwanego load balancingu Czyli odpowiedniego oddestruowania ruchu sieciowego do naszych serwisów. czyli o samym Load Balancingu  będziemy mówić więcej na następnym zjeździe. My takie Reverse Proxy  będziemy sobie budować  na gcp na przyszłym zjeździe.  część rzeczy stworzy nam gcp już sam z siebie automatycznie,  ale omówimy sobie dokładnie jak to wygląda.
    

  

Teraz dlaczego wam pokazuje tą ścieżkę i omawiam te rzeczy?  przede wszystkim z dwóch powodów:

1.  w trakcie wdrażania naszych modeli uprzejmoszonego będziecie również współpracować z innymi ludźmi na przykład innymi DevOpsami, System Administratorami Którzy wspierać was będą w kwestiach sieciowych.  my jesteśmy specjalistami z zagadnieniach uczeniach maszynowego, My nie będziemy ekspertami w zagadnieniach sieciowych.  Tym bardziej że to co tutaj wam przedstawiłem jest dosłownie małym wycinkiem tego wszystkiego.  ale warto znać takie podstawy dlatego że po prostu to że wasze API nie będzie działało może nie wynikać z tego że coś jest złego w kodzie,  albo coś źle wdrożyliście,  tylko np.  Firewall jest tak ustawiony żeby blokować ruch do waszego ip-a.  może ruch do waszego e-pika jest odblokowany,  ale jest zablokowany ruch do portu. A może wy stworzyliście reversproxy na konkretnym ipku,  ale jednak request idzie inaczej -  na przykład ścieżka którą widzice w `traceroute`, Może wam podpowiedzieć czy tak faktycznie jest.  na przykład  traceruta do naszego API zobaczyć czy po drodze jest IP naszego rivesproxy.  albo zalogować się na serwis,  z serwisu wysłać requesta  gdziekolwiek i zobaczyć czy przychodzi przez forward proxy. Takie proste sprawdzenia sieciowe Wy możecie wykonać z tego własnego punktu i już zobaczyć jakiś problem,  jeżeli macie do sezonów wiedzę to może to sobie poprawić bo to może wynikać może w ustawieniach waszego proxy, Ale dzięki temu Macie też podstawę do rozmawiania z innymi osobami odpowiedzialnymi za kwestie sieciowe. A kiedy ja wdrażam modele to jestem We współpracy z moim kolegą który jest Administratorem SYstemów, Po to aby np. zarejestrował IPki  na naszej domenie, Tak żeby nasze serwisy nie były odpytywane w ipikach tylko miały konkretne  nazwę strony,  i np. wysłał certyfikaty TLS żebym mógł komunikować się ze swoim API po HTTPS. Generalnie nie czuję się ekspertem w tych dziedzinach,  ale podstawowe  zrozumienie tych zagadnień sieciowych pozwoli wam po prostu łatwiej szybciej jest debugować  i rozmawiać z innymi DevOpsami czy System Administratorami  którzy są już ekspertami w tych kwestiach i po prostu was wspomogą.  nikt z was nie będzie unicornem, i ekspertem w każdej dziedzinie, Ale warto znać przynajmniej podstawy  pewnych rzeczy,  naprawdę będzie łątwiej się Wam te modele wdrażać.
    
2. Też o tym że podczas wdrażania modeli na chmurę Jesteście zdani na infrastrukturę sieciową danego dostarczyciela  i np. Google udostępnia taką macierz usług, które działają obecnie [https://status.cloud.google.com/](https://status.cloud.google.com/)  więc to że coś nie działa,  nie zawsze może być  waszą winą.  bądźcie tego świadomi jeżeli niestety ale czasami usługi Google mogą  się  popsuć. Przykład z życia -  w kiwetniu/maju 2023 Był pożar w serwerowni googlowskiej w Paryżu. Wszyscy którzy mieli ustawione usługi w Paryżu po prostu nie działały,  ale to się odbiło w ogóle na całości chmurzy GCP bo nawet nasze klastry które działały w Belgii po prostu nie przyjmowały ruchu.  odpytywanie serwisu Po ipkach wewnętrznych było możliwe,  ale wszelkie load balancery które przyjmowały ruch z zewnątrz i potem przekierowywały one po naszych usługach nie działały.  wszystko się świeciło na zielono na tej stronie ale wciąż ruch nie przychodził. Czekałem aż to samo się naprawi,  ale widziałem że to dalej nie działa. Ostatecznie rozwiązaniem było tylko i wyłącznie postawienie klastrów na nowo,  nie udało mi się zidentyfikować przyczyny, bo to było wewnątrz googlea, ale odtworzenie klastrów pomogło. Także pamiętajcie że część rzeczy jest po prostu niezależnych od was  podczas wdrażania.
    
3.  Trzecia rzecz istotna  to  wspominałem o tym przy okazji API.  że jednym z kluczowych metryk przy wydarzeniu API do modeli uczeń maszynowego i jest czas odpowiedzi.  my ją sobie  przyspieszaliśmy w ten sposób że mamy po prostu cachea postawonego na redisie, Co jak widzieliście to Znacznie potrafi nam przyśpieszyć odpowiedź z API.  ale zwróćcie uwagę na `traceroute`,  widać że każde przejście przez punkt jest obarczony jakimś opóźnieniem.  wiadomo że gdyby reques leciał bezpośrednio to byłoby to najszybsza droga,  ale tak nie jest,  i generalnie Im więcej mamy takich punktów  przez które przechodzi No generalnie tym dłużej musimy czekać na odpowiedź z API. Za dużo tutaj zrobić nie możemy, Ale możemy wybrać lokalizację naszych usług i to w dużej mierze zmniejszy ilość tych kroków pośredniczących pomiędzy klientem/serwerem który wysyła request a naszym API. Dlatego podczas wydarzenia uderzenia modeli uczeń maszynowego możecie sobie zadać pytanie Gdzie stoją inne usługi z których nasze systemy korzystają?  najlepiej zdefiniować nasze wiemki klastry na które będą działać nasze modele No w tych lokalizacjach w których działają serwisy dlatego że czas  może być najkrótszy. 
    

  

Spójrzcie proszę o tą grafikę.  to co to widzicie jest to macierz czasu  opóźnienia sieciowego w sieci googlowskiej pomiędzy pewnymi lokalizacjami.  mamy europe-central2 oznacza to Warszawę,  europe-west1 to belgia, us-central1 to chyba Iowa. 

  

 zwróćcie uwagę na to Jakie są różnice w opóźnieniach sieciowych.  jeżeli macie usługi w tej samej lokalizacji to to jest mniej niż 1 milisekunde. Nawet jeżeli mamy usługi  na tym samym kontynencie,  ci na przykład porównanie Warszawy z belgią  to zobaczysz to jest 20 mil sekund.  20 razy większe opóźnienie sieciowe.  z kolei Warszawa versus Ameryka,  100 razy większe opóźnienie.  dlatego zawsze zwracajcie uwagę na to gdzie stoją wasze usługi w jakiej lokalizacji oraz oczywiście to czy dyplorujecie modele w takich warunkach w których każda milisekunda jest istotna. Jeżeli dyplorujecie w dole w warunkach w których No nie musicie aż tak bardzo na to zwracać uwagę,  żeby zobaczyć to są czasy urzędów milisekund,  więc Załóżmy że dla API postawionego w Warszawie odpowiedź zostanie zwrócona w  sekundę,  A dla API  postawionego w stanach w 1 sekunde i 100 milisekund,  więc generalnie te stomiry sekund może być dla niektórych nie odczuwalne i akceptowalne.  wtedy na przykład gdybym decyzyjnym jest bardziej cena jaką trzeba zapłacić za diploy naszych modeli No bo ona się różni w zależności od lokalizacji.  ale jeżeli będzie pracować w warunkach gdzie każda mieli sekunda jest istotna,  to oprócz oczywiście napisania kodu najszybszy sposób jaki potraficie,  oprócz dodania cacheowania waszych requestów i response i musicie zwrócić uwagę na lokalizację w której deployujecie usług I najlepiej żeby wasze modele po prostu były w tej samej lokalizacji w której są inne usługi,  które będą z tego modelu korzystać. 

  

Okej w takim razie wróćmy do naszego  slajdu. Także to jeszcze raz Podsumowując zaczynamy od lewej strony Wysyłam request z naszego komputera/bądź może tobyć jakiś server/serwis/usługa cokolwiek, Widzicie droga requesta jest może być bardzo Zawiła i warto zwrócić uwagę,  i o zasadzie trafia ona do naszego API. To co wam Umówiłem teraz to był taki wstęp do zagadnień sieciowych, będziemy do nich wracać na następnym zjeździe poświęconym Kubernetesowi. Moim zdaniem jest to takie minimum które na pewno warto znać i dzięki temu wdrożenie tych modeli i po prostu będzie dla was przyjemniejsze bo takie względem że będziecie rozumieli że te zagadneinia sieciowe są bardzo istotne I trzeba je mieć na względzie podczas wdrażania modeli, ale najczęściej w tych zagadnieniach sieciowych będziecie wspomagani przez inne osoby które są ekspertami w tej dziedzinie.

  

Okej To wróćmy sobie do naszej VMki. Jak widzicie, zdeployowanie takiego modelu Korzystając z kontenerów jest bardzo proste. Możemy sobie odpytać nasze VMke właśnie Korzystając z tego IP. Teraz pytanie jakie se można zadać to czy faktycznie modele są wdrażane w taki sposób jak teraz zrobiliśmy. Odpowiesz mi to zależy.  generalnie podczas wytwarzania różnego rodzaju Software to jak sami widzicie na co, pracujemy w podejściu iteracyjnym - abstrahując czy pracujemy w scrumie agilu i innych takich pojęciach, Chodzi mi o to że w momencie kiedy dostarczamy coś nowego warto robić to sposób literacyjny,  od pierwszych  prostych prototypów  po coraz to bardziej skomplikowane modele, Tak samo jak budujemy aplikacje frondendową, Też jest jakieś  w jakiejś pierwszej wersji,  a potem kolejne wersje rozwijają i dodają kolejne funkcjonalności. Tak samo  proces tworzenia może być procesem migracyjnym nie trzeba od razu budować najbardziej skomplikowanej architektury przygotowane na każdy Edge Case.  można zacząć od czegoś bardzo prostego co pozwoli wam dostarczyć coś szybko na tyle aby dalsze przez zespoły, backend frontend, mogły powoli integrować się z naszym API. Nie musimy czekać aż Zbudujemy idealny model pytanie jeszcze to oczywiście oznacza idealny model ale generalnie lepiej wdrożyć już na produkcję coś co przynosi jakiś uzysk pieniądze,  i potem dopiero rozwijać kolejne wersje.   ale nawet jeżeli nie mamy jeszcze nawet tej pierwszej wersji która przynosi jakieś pieniądze i potrzebujemy jeszcze prac eksperymentalnych dalszych analiz żeby nasz model był lepszy, To nic stoi na przeszkodzie, aby spróbować już aby wdrożyć pierwszą wersję jakąkolwiek modelu, Wraz z pierwszą wersją API  tak aby można było już integrować się z waszym rozwiązaniem. W ten sposób uzyskujecie bardzo cenny feedback czy to od różnych zespołów które korzystają z waszych API co warto zmienić w tym API,  może coś jest niejasne w dokumentacji, Może wyjdą jakieś pierwsze Buggy bo API jest odpytywane jakimiś edege caseami. Wasze API może być już odpytywane z taką częstotliwością  z Zakopane ceny requestem by to przechodzić na co dzień.  wiesz na przykład takie wczesne wdrożenie API do waszego modelu  może odpowiedzieć wam na Na istotne pytanie Jak wasz API będzie działało pod obciążeniem. Może wiemka na której teraz postawiliście sobie API jest wystarczająca i nie potrzebuje większej infrastruktury,  a może jedna wielka w ogóle sobie nie radzi z tym wszystkim trzeba by było dostawić więcej instancji. Co tam chcę powiedzieć że podczas wdrażania modeli, w ramach np. proof of concept, czy może macie model już wypracowany ale po prostu chcecie przenieść go na chmurę, Warto takie PoC realizować też w kwestiach wdrożeniowych. I na przykład,  Zamiast stawiania sobie dużej infrastruktury  pod modele  na samym początku,  można  Na początek zacząć właśnie tak by teraz robiliśmy  czyli zdeployowaliśmy sobie API na VMce, znając tylko i wyłącznie to jak te API stworzyć w FastAPI, oraz jak zbudować sobie kontener. Jeżeli myślicie o tym żeby postawić sobie taką wjemkę żeby jakieś inne serwisy wewnętrzne działające na Google cloudzie mogły z niej korzystać to jest to jak najbardziej dobre rozwiązanie, szybkie, tanie i w miare bezpieczne, Bo  komunikujemy się po wewnętrznej sieci googlowskiej która Jak wiecie ma w sobie pewne zabezpieczenia już zaimplementowane.  więc z poziomu widzenia Inżyniera Uczenia Maszynowego I do realizacji PoCa czy testów  może być jak najbardziej okej.  Gorzej jeśli Chcielibyśmy to wdrożyć API  której chcemy Żeby było osiągalne z zewnątrz tak teraz zrobiliśmy czyli z laptopa wysłałam requesta do naszej VMki.  tutaj jest znacznie więcej pracy  bo my Musimy dodatkowo zabezpieczyć  na przykład  musimy sami zadbać o to żeby komunikacja po protokole HTTPS  się odbywała. Generalnie jeśli  myślimy o postawieniu sobie pierwszego modelu i API do niego który jest osiągalny z zewnątrz lepiej będzie skorzystać z innych usług niż budować to tak jak na VMce. Każda chmura ma dedykowane komponent do tego żeby deployować modele uczenia maszynowego. Jest są one na tyle rozwinięte,  np. na GCP jest to Vertex AI, Które zarówno działają dobrze w ramach PoCów,   Ale również dalej kiedy nasze modele są używane na szeroką skalę. To ma też plus taki że nawet jeżeli w ramach PoCa ta usługa wam się spodoba, spełnia wasze oczekiwania to można przy niej zostać praktycznie cały czas. Plusem jest też to jak sami zaraz się przekonacie że tam dosłownie nie trzeba być specem od infrastruktury żeby zdefiniować sobie na jakich zasobach ma działać nasz model i jak się ma skalować i tak dalej generalnie jest to bardzo wiele friendly i można dużo rzeczy skomplikowanych ustawić bardzo szybko bardzo prosto. Natomiast też Ma to swoją istotną wadę.  wadą jest to oczywiście że jest to mniej konfigurowane i gdybyśmy deployowali to własnoręcznie. A druga rzecz która jest istotna również to są oczywiście koszty,  usługi serwer less z racji tego że one pozwalają wam na diplorowanie pewnych usług po prostu klikając,  oczywiście  serwery Są pod spodem,  ale one  konfiguruj o wszystkie serwery już za was.  więc my nie musicie się przejmować architekturą i zarządzaniem nią to leży po stronie usługi,  ale to się wiąże ze znacznie większymi kosztami.  dlatego w procesie w do żadnej modeli będziecie musieli cały czas ważyć koszt Jaki potrzebny jest to żeby działała ta usługa  versus poziom skomplikowania. To zobaczmy sobie w takim razie Jaki jest poziom skomplikowania wdrożenia modelu na chmurę Korzystając z dedykowanej usługi do tego Vertex AI. 

  

Jest sobie taka zakładka numerteksie o nazwie deploy and use. Skorzystamy na początku z Model Registry,  aby zarejestrować sobie nasz model a tak naprawdę techniczny że biorąc będziemy wgrywać nasz kontener z API. 

  

 wybieramy sobie i Przejdźmy sobie na region europe-central2 czyli Warsaw.  następnie u góry mamy takie coś jak import.

  

1. Klikamy sobie Import as new model
    
2.  Dodajmy nazwę niech to  będzie po prostu  Pierwsza litera imienia potem nazwisko i na koniec dodać `live-coding` 
    
3. Następnie Przechodzimy sobie do model settings  ustawiamy sobie u góry Import an existing custom container
    
4. Wyszukujemy nasz kontener w Artifact Registry - Jutro prośba do was wybierzmy tą wersję kontenera która nie wymagała redisa i postgresa.
    
5. Teraz zwróćcie uwagę w tym okienku Command, możemy sobie nadpisać na przykład ENTRYPOINT. 
    
6. Można też wskazać sobie  lokalizację gdzie nasz model jest  przechowywany i wtedy ta lokalizacja jest podpinana do kontenera jako Volume.
    
7.  następnie możemy sprzedać argumenty,  w W tym przypadku wszystko co wpiszemy zostanie podane jako CMD
    
8.  możemy sobie zmienić na środowiskowe
    

 więc generalnie jak widzicie  ta usługa pozwalam wam na uruchomienie kontenera Dosłownie tak jakbyśmy robili to z poziomu terminala i korzystając z `docker run`

  

Chciałbym natomiast żebyśmy ustawili sobie dwie istotne rzeczy żeby nam ten model zadziałał.

  

1. Po pierwsze prediction root wskazać na `/decisions`
    
2. po drugie Chciałbym żebyście  health route ustawili na `/`
    

  

Port 8080 zostawiamy Zobaczcie to jest przykład tego co mówiłem wcześniej że ten port jest domyślnie ustawiany w różnych usługach.  i to widzimy tego przykład domyślnie 8080. 

  

Predict schemata pomijamy.

  

I klikamy Import.

  

Czekamy sobie chwilę aż ten nasz model zostanie zaimportowany do vertexa

  

 po tym jak już jest zaimportowany  to  musimy stworzyć dla niego  usługę która będzie przyjmować od naszej requesty. 

  

 Po lewej stronie Przechodzimy sobie do Online predictions. Znowu Proszę was wybranie lokalizacji Warszawa  i następnie po prostu Create.

  

I to znowu was proszę o nazwę  tak jak wcześniej  czyli pierwsza litera imienia potem nazwisko i na koncu sufix `-live-coding`

  

 zakładka Akces mówi nam o  o dostępie do naszego API  mamy Wersja standardową czyli w tej sytuacji  model będzie dostępny dla wszystkich,  ale też macie opcje Private Na przykład w sytuacji kiedy wasze serwisy są odbywane tylko i wyłącznie przez serwis inny wdżona na chmurze googlowskiej, I na przykład tej sytuacji możemy wskazać skonfigurowaną przez nas sieć. Ale jest to rzecz bardziej zaawansowana.  ustawmy sobie na standard .

  

Następnie model settings  tutaj ustawiamy sobie model I znajdujemy ten nasz model.

W wersji  Ustawcie najnowszą. Lepsza z kolei możemy sobie ustawić ile ma być naszych  compute nodes czyli w tym przypadku wirtualnych maszynek. Nadal trzeba jeszcze ustawić sobie typ maszynki. I wybierzcie prosze `n1-standard-2`. Może sobie nawet podpiąc kartę graficzną. Później można wyłączyć logowanie,  Elementy związane z wyjaśnialnością waszych modeli. Jestem w zakładkach z monitoringu modelu ale tutaj nic nie ustawiajcie.  i na końcu Create. 

  

Jak widzicie mamy tutaj bardzo dużo opcji,  a ich konfigurowanie jest bardzo proste  generalnie nie wymaga to wielkiej wiedzy od was.  dlatego użycie takich dedykowanych usług do wdrożenia modeli Jest bardzo atrakcyjne bo zajmuje wam to bardzo mało czasu,  nie wymaga od was specjalistycznej wiedzy i jak widzicie opcje jest naprawdę sporo i nawet są  takie rzeczy jak wyjaśnialność czy monitoring,  bo jest to dedykowana usługa,  więc  tak naprawdę Jeżeli użyjecie tego do PoC to można  dalej przy tej usłudze zostać i po prostu z niej korzystać nawet jak nasz model będzie używany na większą skalę i będzie do tego większe pieniądze.  Tym bardziej że operujemy tutaj na dedykowanej usłudze vartex typowe gcp które właśnie skupia się  wokół uczenia modeli przygotowania danych i ich wdrażania.  wszystko w jednym miejscu. Natomiast ja powiedziałem to wszystko kosztuje pieniądze,  i warto też jednak wiedzieć Moim zdaniem co pod spodem On tworzy Jak działa.

  

Natomiast w następnym zjeździe Będziemy uczyć się Kubernetes a nie tych usług serwer osoby które tutaj Widzieliście ale to wszystko wyjaśnię na zjeździe przyszłym dlaczego akurat Kubernetes.

  

 teraz Zobaczcie potem jak otworzyliśmy sobie pointa to możemy zobaczyć opcje sample request która Wyświetli nam przykładowe zapytanie które możemy wysłać do naszego API.  bo niestety  Werteks działa tak że on tworzy własny API pod spodem i model który będzie osiągalny,  będzie osiągalny pod endpointem o nazwie predict.

  

Jak sobie spojrzymy  co tutaj nam proponuje to jest opcja Rest i to się składa z 5 kroków i zakłada użycie curl’a do tego żeby odpytać nasze API.

  

Jest też tylko zakładka z pythonem, ale on zakłada użycie dedykowanej biblioteki od googlea. 

  

Słuchajcie nie będziemy wysyłać naszej requestów do tego API,  od razu powiem że nie zadziała bo trzeba  przygotować schemat i butów Czyli to co powinniśmy wcześniej w predict schemata ale nie chciałem tego robić No bo to trzeba przygotować w jamlu i jest trochę pracy.  Natomiast co powinno być zainteresujące to to że zobaczcie  My już w tej chwili nie mamy ipika tylko mamy Konkretny adres strony naszego API 

  

https://europe-central2-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/europe-central2/endpoints/${ENDPOINT_ID}:predict

  

Tutaj trzeba wstawić Project idpoint ID żeby wysłać requesta do naszego API. Natomiast Zobaczcie że przede wszystkim mamy https Czyli już tutaj komunikacja jest  nawiązywana w sposób bezpieczny jest szyfrowana. Mamy  jakąś konkretną nazwę strony do naszego API. Więc widzicie ta usługa zrobiła dla nas już sporo rzeczy. Więc jakby ci takie dedykowane usługi są naprawdę wygodne.

  

Czyli podsumowując to co teraz chciała przekazać,  Wróćmy sobie do tego slajdu z naszych komunikacją.  Powtórze się jeszcze raz. My nie będziemy ekspertami od spraw sieciowych ale warto jest znać podstawowe pewne pojęcia i te komendy które wam pokazałem do tego żeby zobaczyć czy faktycznie riques przechodzi do waszego API.   w zakresach sieciowych  będziecie pracować z innymi osobami w firmie głównie mam na myśli DevOpsów czy System Administratorów, Którzy w tych sprawach sieciowe was wesprą,  ale warto znać postawy. No No i ostateczna że to taka że wiedza o API i wiedzę o kontenerach już jest wystarczająca żeby wdrożyć pierwszy modele korzystają z dedykowanych usług do tego stworzonych,  które pozwalją Wam to wszystko dosłownie wyklikać. My natomiast na następnym zjeździe Będziemy uczyć się wdrażania   modeli na klastry obliczeniowe,  znaczy zestaw w pewnych maszynek które są specjalnie przygotowane dla naszych modeli  które są zarządzane przez Kubernetesa. Jak się sami przekonacie, kubernetes jest wszechobecny nawet jeżeli korzystacie z usług serwer less takich jak tutaj Vertex ri czy jakakolwiek inna usługa serwer less na innych może,  to pod spodem działa dla was jakaś infrastruktura i praktycznie w każdym przypadku  pod spodem używany jest Kubernetes tak żeby tym zarządzać, Więc generalnie Zamość u bratesa pozwoli wam na zwolnienie tego co dzieje się w bebechach tym wszystkich usług I będziecie wstanie podjąc decyzję Czy na przykład skorzystać z takiej usługi jak ta żeby szybko coś wdrożyć,  czy Może poświęcić więcej pracy na wdrożenie czegoś ręcznie na kuberdesa ale Zysk jest taki że możecie zmodyfikować Tam wszystko co chcecie  ale o tym zaraz zjeździe. 

  

 chciałbym żebyśmy teraz ćwiczeń a potem ćwiczeniach czas na mini projekt na koniec naszego zjazdu.

  
  

**