# 33. Poetry i grupowanie pakietów

**

Okej przechodzimy do następnego tematu. Do tej pory na naszym pierwszym zjeździe korzystaliśmy sobie z `venv`, `pip` i pliku `requirements.txt`,  Natomiast teraz w naszym repozytorium pojawił się trzy nowe pliki,  takie jak `poetry.lock`, `poetry.toml`, `pyproject.toml`,   więc czas teraz rozwikłać zagadkę.

  

Te plik mi trochę Widzicie to pliki wykorzystywane przez pakiet managera o nazwie poetry,  który zyskuje coraz większą popularność  i świetnie sprawdza się na produkcji  a A tym bardziej sprawdza  się w  codziennej pracy Data Scientistów i Machine Learning Engineer,  ponieważ posiada jeden feature które żaden inny pakiet manager nie ma  i w naszym kontekście jest to moim zdaniem Game Changer  i ja nie wyobrażam sobie osobiście  zarządzania repozytorium i wdrażania rozwiązań bezpośrednio,  tym bardziej jeśli  na danym tematem pracuje kilka  osób o różnych profilach  i Trzeba tym odpowiednio zarządzać.

  

Ale zacznijmy od początku.  na pewno każdy z was słyszał o podstawowym `venv`,  który zużywa się do zarządzania wirtualnymi środowiskami.  nie będziemy tłumaczyć czym są wirtualne środowiska bo to dokładnie sami wiecie. `venv` jest szeroko stosowany i generalnie do bardzo prostych projektów jak najbardziej się sprawdza. Natomiast jest nim problem,  a dokładnie nie może z samym venv  bo on po prostu zarządza aż mi się środowiskiem  wirtualnym  tak żeby oddzielić je od innych środowisk wirtualnych  i Przede wszystkim od  środowiska bazowego na którym działa nasz system operacyjny. natomiast problem jest z samym `pip`-em.  mianowicie pip kiepska sobie radzi z  ogarnięciem w zależności pomiędzy różnymi pakietami.  to znaczy że Wyobraźcie sobie że macie dwa pakiety,  Pierwszy z nich  aby prawidłowo działał wymaga pakietu X w wersji 1.0.  a drugi pakiet który instalujecie wymaga tego samego pakietu  ale w wersji 2.0.  a teraz Co się stanie jeśli najpierw zainstalujemy pakiet pierwszy,  a potem pakiet drugi. Pip podczas instalacji pierwszego pakietu zobaczył że potrzebuje X w wersji 1.0.  i go zainstaluję.  kiedy skończę instalację pierwszego pakietu,  zacznie  instalować drugi pakiet  i  zobaczę że drugi pakiet potrzebuję X w wersji 2.0.  i co zrobi pip?  zainstaluje X w wersji 2.0,  nie obchodzi go to  że już w środowisku jest jakiś pakiet który potrzebuje X 1.0. W efekcie czego wasze środowisko po prostu się popsuje.  ten pierwszy pakiet po prostu nie zadziała,  albo zadziała w taki sposób który nie chcecie.  dlatego w ekosystemie Pythona jest wiele różnych pakiet managerów,  jest ich kilka ja wam powiem że nie mam doświadczenia tych wszystkich,  po prostu jest ich sporo natomiast są dwa które zasługują na szczególną uwagę i komentarz.  jest jeszcze druga kwestia  w której brakuje w `venv`. Spójrzmy sobie na nasze pakiety, które używamy w `pyproject.toml` - mamy tez `requirements.txt`, ale do `pyproject.toml` bedzie łatwiej spojrzeć.  zobaczcie,  my na produkcję wdrażamy API,  więc Pytanie brzmi czy podczas przyjmowania requestów,  wykonywania predykcji,  i Zwracanie predykcji z modelu do klienta który wysłał requesta, Potrzebuję na przykład `matplotlib`,  czy też `seaborn`,  albo `notebooka`.  Są też takie biblioteki które odpowiedzialne są za pilnowanie jakości kodu  jak na przykład `mypy`, `pylint`, `flake8`.  one są potrzebne w momencie developmentu waszych modeli i rozwijania naszych kodów ale one w ogóle są potrzebne w momencie działania naszego API.  więc Pytanie brzmi czy możemy to jakoś zaadresować?

Bardzo często spotykanym rozwiązaniem jest to żeby rozdzielić requirementsy jakby na dwa pliki. Czyli zobaczę mielibyśmy sobie `requirements_dev.txt` i np. `requirements_prod.txt`  i w jednym pliku byśmy umieszczali te biblioteki które są potrzebne w trakcie developmentu  a w drugim  pliku te biblioteki używane podczas działania na produkcji.  i znowu w małej skali może to zadziałać ale generalnie im większe,  im wiecej osób pracuje  tym ciężej jest zarządzać dwoma plikami które definiują środowisko.  A pytanie powstaje A co jeżeli chcielibyśmy mieć więcej takich grup,  utrzymywanie 3-4-5 plików to już jest masakra.

  

Zatem mamy dwa problemy:

  

1.  Po pierwsze, `pip`  ma problem z analizą zależności pomiędzy pakietami przez co może wam bardzo łatwo popsuć środowisko
    
2.  po drugie,  potrzebujemy czegoś aby nasze pakiety rozdzielić na grupy na przykład związane właśnie z rozwijaniem naszego API,  i związane z działaniem na produkcji.  teraz dlaczego jest istotne takie rozdzielanie,  dlatego że obrazy w waszej kontenerów będą  i zajmowały znacznie mniej miejsca wtedy kiedy będą posiadać tylko biblioteki potrzebne do działania na produkcji,  to jest logiczne.  a Im mniejsze rozmiary kontenera, tym mniej pieniędzy zapłacicie za storage i transfer,  więc warto się nad tym powiedzieć.
    

  

Tak mówiłem package managerów jest kilka, Ale są w sumie dwa takie generalne,  bardziej popularne które wyglądają skomentowania i zaproponowania , który użyć. Zacznijmy od pierwszego  o którym Wielu z was na pewno słyszało,  bo jest on omawiany  na każdym tutorialu,  w momencie kiedy instalacji biblioteki to oprócz pipa pojawia się opcja zainstalowania korzystając z condy, czy też pałna nazwa anaconda. Conda jest bardzo popularna w świecie Data Science i generalnie została zbudowana właśnie w tym celu.  raczej Backend Developerzy pracujący w pythonie nie korzystają z condy, bo nie mają takich potrzeb. Skąd wynika jej popularność:

  

1. Po pierwsze,  bardzo ważna Argument jest taki że Data Scientiści, Data Analyst itd.  którzy na co dzień  zajmością analiza danych i budowania właśnie modelu przebaczowego nie są też świetnymi administratorami systemów i powiem szczerze Większość z nich pracuje pewnie na Windowsie.  z kolei Anaconda jest jedynym narzędziem do zarządzania środowiskami Pythonowymi na którym działa bez problemu Python. Dla użytkowników Windowsa Anaconda jest jedynym wyborem,  No wiadomo użytkowników Windowsa jest nie widzę na świecie więc to też przekłada się najpopularność.
    
2. A nawet jeżeli Data Scientist nie będzie używał Windowsa,  i na przykład Linuxa czy Maca,  to tam problemu z pythonem już nie będzie,  ale anaconda wciąż też jest częstym wyborem, z bardzo prostej przyczyny -  instalując anaconde dostajecie bazowe środowisko od razu na starcie w którym macie zainstalowane wszystkie najważniejsze biblioteki do uczenia maszynowego  i analiza danych,  nic jest to bardzo wygodne tak dlatego że nie musicie przejmować się konfiguracji środowiska.  a nawet jeżeli chcecie skonfigurować nowość środowisko to de facto można to zrobić wyklikując je z GUI Anacondy.
    
3. 3 rzeczą która  zwiększa popularność anakondy,  to oczywiście to że w momencie kiedy Czytacie jakieś tutoriale,  albo dokomunetację Jak zainstalować jakiś pakiet  to często oprócz  z opcji zainstalowania przez pipa jest też obok napisane jak to zrobić Korzystając z condy. Dlaczego jestem druga opcja,  dlatego, że Anakonda ma jakby swoje własne repozytoria pakietów  w ramach których  poprawia pewne rzeczy biblioteka.  Chociaż powiem wam szczerze że ja do tej pory nie spotkałem się z żadnymi problemami biblioteka w dostępnych w bazowym repozytorium pythona,  więc może ten problem występował wcześniej, Natomiast  wzbogaca jakieś pakiety dodatkowymi zależnościami,  które według anakondy poprawiają korzystanie z tego pakietu. I w dodatku też konta ma taką zaletę że na przykład  jeżeli  jakiś  pakiet pythonowy Wymaga jakiejś biblioteki systemowej do prawidłowego działania  to ją zainstaluje dodatkowo  do waszego środowiska. 
    

  

Więc generalnie  właśnie popularność anakondy składa się Właśnie tych rzeczy:

1.  jest to naturalne wybór kiedy pracujecie się na Windowsie
    
2.  nie chce wam się konfigurować środowisko,  po prostu chcecie  od razu zacząć pracę.  dlatego anaconda jest tak rekomendowana osobom początkującym  w świecie  Data Science  dlatego że cała ta warstwa konfigurowania pakietów  i środowiska jest po prostu schowana przed wami.
    

  

I anaconda świetnie rozwiązuje problem pierwszy który mieliśmy, To znaczy patrzy na zależności pomiędzy pakietami,  więc w sytuacji kiedy  chcecie zainstalować nowy pakiet ale jego zależności   i są wymagania  są sprzeczne z wymaganiami pakietów które już macie w środowisku  to on tego nie zainstaluje w ogóle  po to aby wam nie popsuć środowiska.  to jest bardzo duży plus. 

  

Natomiast niestety Anaconda nie adresuje drugiego problemu czyli  zarządzania środowiskiem w taki sposób żeby Wydzielić sobie pakiety które działa na produkcji  i pakiety które potrzebne są w trakcie developmentu. Jest jeszcze kilka innych wad, o których zaraz powiem.

  

Natomiast musimy oddzielić sobie development od produkcji,  i tutaj są troszeczkę inne wymagania.  w trakcie dewelopmentu my Mówiąc krótko chcemy się tym i danymi pobawić,  to budować różne modele, zweryfikować kilka hipotez  generalnie  to nie jest etap  w którym myślisz się o  tym żeby to później wdrożyć, więc  na pewne rzeczy może nie zwracać uwagi  albo zwraca się ale znacznie większym stopniu.  Natomiast ja bym chciał wam powiedzieć od razu że  takie podejście jest złe to znaczy,  najpierw Zbudujmy model najlepszy a potem dopiero pomyślmy o tym jak to wdrożyć na produkcję. Moim zdaniem w trakcie projektowania rozwiązania powinniście zakładać o tym że ona wejdzie na produkcję  i wybierać taki zestaw narzędzi który jednocześnie  sprawdzi się w trakcie developmentu  jak i podczas wdrażania na produkcji. Bo jeżeli rozdzielicie to myślenie,  i o wdrożeniu na produkcję będzie myśleć później,  to  Gwarantuję wam to że  wdrożenie może być przez to znacznie trudniejsze niż gdybyście myśleli o tym od samego początku. Dlatego że w trakcie developmentu możecie używać narzędzi które w trakcie developmentu są jak najbardziej okej  ale później okazuje się problem z ich wykorzystaniem na produkcji. Na produkcji trzeba będzie dostosować do waszego use case’a dlatego też Generalnie polecam wybierać takie narzędzia które pozwalają wam  na dużą możliwość konfigurowania  pod siebie.  jeśli chodzi o package managery, i zarządzanie środowiskiem waszej pracy To obecnie Moim zdaniem najlepszy jest `poetry`,  który świetnie rozwiązuje dwa problemy po których wcześniej mówiłem -  czyli respektowanie zalezności pomiędzy pakietami oraz  fakt wydzielania sobie  podśrodowisk  w ramach jednego danego środowiska,  co  żaden inny package manager nie jest w stanie uzyskać. 

  

Ale może zaczniemy od tego żeby powiedzieć jakie są wady Anacondy  w przypadku wdrażąnia jej na produkcję:

  

1. Generalnie musicie wiedzieć o tym  że Anaconda wprowadziła licencję i jeżeli spojrzymy sobie na  ich blog post który  poruszał ten temat bo wokół tego było bardzo duże poruszenie [https://www.anaconda.com/blog/is-conda-free](https://www.anaconda.com/blog/is-conda-free) To jest to taki wykres który niejako podsumowuje tą rozmowę  i wynika z niej to że firmy powyżej 200 osób pracowników  najprawdopodobniej  będą musiały wykupić licencję enterprise. W ich Terms of Services jest też temu dział poświęcony [https://legal.anaconda.com/policies/en/#purchased-vs-free-offerings](https://legal.anaconda.com/policies/en/#purchased-vs-free-offerings) . Jesteś dyskusja na reddicie  w której głos zabierał jeden z twórców Anacondy  ale niestety ludziom nie udało się uzyskać odpowiedzi  Jak traktują tych pracowników.  w sensie jeżeli Pracujecie w firmie która ma 1000 ludzi, a z Anacondy korzystają tylko dwie osoby, to musicie kupić licencję czy nie? [https://www.reddit.com/r/Python/comments/iqsk3y/anaconda_is_not_free_for_commercial_use_anymore/](https://www.reddit.com/r/Python/comments/iqsk3y/anaconda_is_not_free_for_commercial_use_anymore/) Ja mam podeślę te wszystkie materiały żebyście  mogli sami sobie w wolnej chwili je przeczytać  i Jeżeli planujecie używać anacondę w firmie  albo nawet teraz jej używacie To Miejcie ten aspekt na względzie i dowiedzcie się czy przypadkiem nie musicie kupić licencję Enterprise.  Ja jestem wielkim zwolennikiem Open Source I generalnie nie używam condy właśnie z tego powodu, Tym bardziej że wiem że na rynku są lepsze rozwiązania, całkowicie za darmo.
    
2. Druga wada jest taka że niestety ale conda to daje od siebie pewne dodatkowe zależności do pakietów  uważając,  że warto jest dodać bo w jakiś sposób ulepsza waszą pracę. Bardzo dobrym przykładem jest numpy, 
    

  

POKAZ IM NA DWÓCH ODDZIELNYCH SRODOWISKACH Z conda vs venv

  

`conda install -c conda-forge numpy` vs `pip install numpy`

  

 który instalując dostaniecie tylko numpy, ale jeżeli numpy zainstalujecie conda To oprócz numpy dostaniecie na przykład biblioteka MKL oraz intel-openmp [https://pypi.org/project/intel-openmp/](https://pypi.org/project/intel-openmp/) , która doinstalowywuje jakieś zależności pod Windowsa! a przecież pracujemy na Math Kernel Library jest to Z kolei biblioteka która przyspiesza operacja matematyczne ale tylko i wyłącznie na procesorach Intel.  fajnie,  ale co jeżeli ja tego nie chce na produkcji. To jest troszeczkę ten sam Case jak wam mówiłem przy różnicy pomiędzy obrazami bazowymi w Pythona.  jest ten obraz  w którym ma bardzo dużo w sobie i waży 900  megabajtów,  a jest obraz Slim który  ma tylko i wyłącznie 130 MB  i Jeżeli coś potrzebujesz to sobie instalujesz.  łatwiej jest coś doinstalować  i niż coś odinstalować, Więc generalnie to że Konda dodatkową instaluje wam pakiety  które w rzeczywistości mogą być wogóle Wam nie potrzebne  jest dla mnie Dużym minusem.  ja wolę sobie doinstalować ten pakiet  MKL  lub intel-openmp  wtedy kiedy wiem że mój klaster ma maszynki z procesorami intel albo wiem, że pracuje na windowsie. Teraz   osadzając to w naszym kontekście redukcji rozmiarów kontenerów,  `conda` nam w tym przeszkadza,  bo doinstalowywuje wam za dużo rzeczy niż jest wymagane. 

  

3. zaletą condy i jednocześnie wadą  jest to że jeżeli instalujecie jakiś pakiet pythonowy  który wymaga pewnych dodatkowych plików systemowych żeby działał poprawnie, To conda go doinstaluje dla was. Przykładem tego są biblioteki PyTorch oraz Tensorflow ale w wersji działającej na GPU. Swego czasu bardzo dużo osób polecało conda właśnie ze względu na to że bez problemu jesteście w stanie zainstalować biblioteki PyTorch i Tensorflow W wersji takiej w jakiej mogą korzystać z GPU  bez żadnych problemów.  swego czasu było to prawdą  w szczególności na Windowsie -  to była jedyna możliwość żeby PyTorch i Tensorflow  działały poprawnie  z GPU. Też było to radość w przypadku systemów Linux,  ale jest to przeszłość  dlatego, Wcześniej faktycznie trzeba było trochę się namęczyć  aby poprawnie zainstalować sterowniki CUDA  w systemach LInuxowych,  więc żeby zaoszczędzić  sobie trudu  to można było użyć condy,  ale generalnie dzisiaj już nie jest żaden problem.  tak naprawdę obecny wystarcza zainstalować trzy pakiety systemowe do linuxa, Dokumentacja jest już prosta i są to jednolinijkowe komendy. Więc generalnie można sobie teraz z tym poradzić nie używając condy. Natomiast kontakt cały czas Może to za nas zrobić.  i gdzie jest problem w tym?  problemem tym jest taki że kąta jest w stanie śledzić zależności pomiędzy pakietami pojedynowymi,  ale nie radzi sobie sam pomiędzy pakietami systemowymi  które doinsterowuje sama z siebie,  więc im większe  środowisko to po prostu możecie mieć potem problem z jego konfiguracją  i naprawianiem bugów. A łatwiej jest naprawić  problemy z pakietami systemowymi wtedy kiedy je Instalujemy ręcznie sami,  niż gdy instaluje to ktoś za nas w tym przypadku conda. 
    
4. No i czwarta wada jest związana z rozmiarem samej condy. Rozmiar nie ma znaczenia jeśli sobie rozwijamy nasze modele lokalne  ale teraz cały czas rozmawiamy o rozmiarach naszych kontenerów i tutaj to ma znaczenie. Anakonda przechodzi w dwóch wersjach,  tej bazowej dużej  i też w wersji mini,  ale miniconda zajmuje więcej miejsca niż wszystkie inne alternatywy które mamy dostępne.
    

  

 Dlatego też ja ze względu na te wady  nie rekomenduje używania condy na produkcji. Składaj to co polecam to `poetry` do którego teraz sobie przejdziemy.


Krótko mówiąc: conda jest super dla osób:
1. Mniej doświadczonych i obytych w tematach zarządzania środowiskiem - idealny wybór dla początkujacych Data Scientistów
2. Jedyne rozwiązanie działające na Windowsie
3. Conda od razu instaluje Wam wiele bibliotek pythonowych do mla i w ogóle od razu przechodzicie do pracy.

Natomis


Dlaczego poetry?

  

1. Poetry wymaga już pewnej konfiguracji,  więc nie jest taki łatwy jak anaconda, Ale generalnie jak już raz się skonfiguruję środowisko pod poetry to potem po prostu kopiuje się do innych projektów.  więc generalnie wystarczy operację włożyć tylko raz ogarnięcie sobie plików konfiguracyjnego poetry  i sprawa załatwiona. Później tylko pewne opcje trzeba tweakować do naszego use-casea ale to nie są duże zmiany. 
    

poetry używa trzech plików:

1.  Pierwszy z nich to `poetry.toml` -  który z kolei mówi nam o tym w jaki sposób ma powstać na środowisko. Nie będziemy przechodzić przez wszystkie opcje  bo my teraz istotne,  Ale chciałbym żebyście sobie w wolnej chwili Po zjeździe sprawdzili to Jakie są możliwości kontry.  ale to co można zauważyć to to właśnie na przykład  uruchomienie pewnych nowych rzeczy eksperymentalnych które poetry rozwija  cały czas  z nowymi wersjami. Ustawienia odnośnie samego installera czyli widzimy że ma działać równolegle `parallel = true` I wtedy on jest w stanie instalować 10 pakietów na raz to znaczy przyspiesza proces powstawania środowiska.  
    Potem mamy na przykład sekcję `[virtualenvs]`, Która wprost określa to jak nasze środowisko ma powstać. 
    
2. Drugim plikiem  który jest odpowiednikiem `requirements.txt`  jest `poetry.lock`.  Wejdźmy do niego i odszukajmy sobie CTRL + F wpis `name = "pandas"`. Czyli tutaj mamy serce poświęconą `pandas`-owi,  i jak możecie zauważyć ten plik `poetry.lock`  już jest znacznie bardziej zaawansowany niż nasze `requirements.txt`. Jest informacja o wersji,  opis,  czy za pakiet jest opcjonalny,  Jaka musi być wersja Pythona  żeby go zainstalować. Przechowa oraz nawet informacja o konkretnych plikach które trzeba użyć do instalacji pakietów i hashy. Potem niżej mamy informacje w zależnościach  i widzimy że Panda jest wymaga czterech pakietów o konkretnych wersjach.  i na dole na przykład też mamy informacje o tym jakie dodatkowe wtyczki można zainstalować do naszego pandasa  i wydatku w jakich wersjach One mogą wejść. Teraz Spójrzmy proszę na przykład na wymaganie odnośnie `numpy`,  który mówi, że `numpy` musi być w wersji co najmniej takiej. Teraz spróbujmy sobie zainstalować jakieś starego `numpy`:
    

  

```bash

poetry add numpy==1.20.0

```

  

To po prostu zostaniesz informacje że to nie jest możliwe do osiągnięcia dlatego że jakieś inne pakiet wymaga numpy w zupełnie innej wersji. I tak samo będzie przypadku u innych pakietów które będzie doinstalować sobie do środowiska,  jeżeli zobaczy on że doinstalowywanie nowego pakietu mogłoby popsuć wasze środowisko  to wam na to nie pozwoli. I  jak widzicie w treści błędu nie ma żadnego workaround albo propozycji jak to rozwiązać - ty sam musisz jakoś to rozwiązać. 

  

I trzecim plikiem najważniejszym nad którym najwięcej czasu się spędza to `pyproject.toml`.

  

Teraz czym jest `pyproject.toml` -  to nie jest twór powstały w wyniku używania `poetry`. `pyproject.toml` to plik, który powstał zgodnie z PEPami [https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/](https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/) , PEP to dla przypomnienia Python Enhancement Proposal, czyli propozycji jak można usprawnić Pythona. Generalnie standardem teraz jest używanie `pyproject.toml`, Do tego aby skonfigurować w nim to jak powinien nasz pakiet zostać zbudowany i opublikowany.  czyli docelowo powstał w celach  dzielenia się naszym kodem na nasz rozwiązanie jako pewien pakiet overse dostępny na przykład w repozytorium PyPI. Wcześniej używane był szeroko plik `setup.py`, W tym celu ale teraz rekomendowane jest aby używać `pyproject.toml`. I w związku z tym poetry wykorzystuje `pyproject.toml` po to aby przechowywać tam informacje o naszym środowisku. Zobaczmy co tam jest.  Generalnie rzecz biorąc  widzimy trzy sekcje:

  

1. `[tool.poetry]` - To serce związane z metodami o naszym pakiecie Gdybyśmy chcieli  z naszego właśnie kodu źródłowego zrobić pakiet tylko opublikować.
    
2. `[build-system]` - później jest druga sekcja  który określa jakiś silnik może złączyć do zbudowania pakietu.  Generalnie my nie budujmy pakietu który udostępnimy ludziom dlatego pominiemy sobie te sekcje,  one nie są dla nas istotne.
    
3. `[tool.poetry.dependencies]` - To co znalazłeś istotne to dalsza sekcja która mówi o  tym  jakie mamy pakiety w środowisku.  i tutaj pojawiają się wszelkie pakiety które dodajemy komendą `poetry add <<NAZWA_PAKIETU>>`, A następnie również updateowany jest plik `poetry.lock` W którym są przechowywane wszelkie metadane o zależnościach tych pakietów. Czyli generalnie `pyproject.toml`  to plik w którym my jako deweloperzy określamy dokładnie jak nasze środowisko działa  i to jest plik dla nas,  natomiast ten `poetry.lock`  to już szczegółowe informacje  dla samego `poetry`  jak powinien to zrobić sprawę zainstalować jakie zależności pomiędzy pakietami.  i tego `poety.lock` w ogóle nie dotykamy nigdy.
    

  

Czyli jak widzicie pierwsza rzecz jest taka że mamy naprawdę bardzo dużo możliwości do konfiguracji  naszego środowiska tak jak chcemy,  na początku tych opcji może być dla Was dużo ale to wystarczy poświęcić trochę czasu żeby to ogarnąć, dokumentacja poetry jest bardzo dobra I nie jest to straszne w ogóle.  A jak już raz to ogarniecie  to w 90%  używacie tego samego po prostu we wszystkich innych projektach.

  

2. Druga zaleta jest taka jak tutaj Widzieliście,  że właśnie  ma świetnego resolvera wszelkich w zależności,  więc Jeżeli chcecie tak jak środowiska pakiet który wam popsuje to bym to nie pozwoli.  i w dodatku jak widziliście w opcjach poprzednich,  poetry jest w stanie instalować kilka pakietów na raz  i też sprawdzać między nimi  dzięki temu stawianie środowiska jest naprawde szybkie

  

3.  numer to jest taka że poetry opiera się głównie o `pyproject.toml`  co zgodnie z dzisiejszymi standardami jest jakby takim głównym  plikiem definiującym właśnie  nasze środowisko,  jak się sami przekonacie  później też nas zjeździe czwartym,  to w tym pliku nie tylko będzie definicja środowiska,  ale również możemy w nim umieszczać wszystkie szczegółowe opcje  jak mają uruchamiać się pewne pakiety  nadal.  i to będzie Niezwykle istotne w momencie kiedy będziemy używać linterów  sprawdzających nasz kod,  co będziemy robić na zjeździe czwartym.  więc w jednym pliku  i będziecie mieli konfigurację wszystkich niezbędnych rzeczy których używacie w waszym repozytorium,  jest to niezwykle wygodne  i pomaga  w zarządzaniu środowiskiem.

  

4. Jeszcze jedna rzecz,  do instalowania pakietów  używamy dedykowanej komendy `poetry add <<NAZWA_PAKIETU>>`, Temat pod spodem poetry to jest ogromny kombajn  zbudowany wokół pipa.  on pod spodem uruchamia pipa,  więc nie mamy takiego problemu jak przy koncie,  że mamy dedykowane inne repozytorium  w którym przywołane są pakiety z dodatkowymi zależnościami,  tylko po prostu dostajemy to co jest w oficjalnym repozytorium PyPI. Zaimplanowaną całą logikę związaną z pilnowaniem wersji pakietów. 
    

  

5. I ostatnia rzecz czyli filtr który `poetry` ma i żaden inny package manager nie ma Co moim zdaniem jest game changerem. Tutaj mamy zestaw pakietów który  potrzebujemy żeby zarówno rozwijać nasze modele jak i działały na produkcji. Tak mu wspominałem na produkcji mamy nasz API i my nie potrzebujemy pewnych rzeczy do obsługi requestów,  `matplotlib`, `seaborn` czy np jupyter notebook Tam są w ogóle niepotrzebne  i tylko niepotrzebnie doważają nasze kontenery.
    

 podczas instalowania pakietów poetry mamy taką bardzo fajną flagę `--group` np.

  

```bash

poetry add nltk --group=dev

```

  

Co oznacza żeby zainstalować pana dk do grupy dev

  

Teraz w pliku `pyproject.toml`  pojawił się nowy wpis `[tool.poetry.group.dev.dependencies]`  który określa jakie są pakiety w grupie dev. Przesuńmy go sobie niżej żeby łatwiej go znaleźć.

  

Okej A co jeżeli chcemy jeszcze inną grupę mieć. A przekaz chcielibyśmy rozdzielić sobie pliki odpowiedzialne za jakość kodu od wszystkich pozostałych.

  

Możemy to zrobić ręcznie.  możemy sami ręcznie stworzyć sobie taką grupę zgodnie z konwencją nazewnictwa.  stwórzmy sobie grupę `linters`:

  

```toml

[tool.poetry.group.linters.dependencies]

```

  

Co mogę zrobić to mogę se przerzucić te pakiety które już mam zdefiniowane  powyżej  do tej grupy po prostu je tam kopiując 

  

```toml

[tool.poetry.group.linters.dependencies]

mypy = "^1.7.1"

pylint = "^3.0.2"

flake8 = "^6.1.0"

isort = "^5.12.0"

```

  

I tak samo mogę zrobić w przypadku naszej grupy dev, która powstała. W takim razie przerzućmy sobie do grupy `dev` te pakiety, które nie są w ogóle na produkcji potrzebne.

  

Zróbmy sobie jeszcze jedną grupę z bibliotekami do testów:

  

```toml

[tool.poetry.group.testers.dependencies]

pytest = "^7.4.2"

pytest-mock = "^3.11.1"

pytest-postgresql = "^5.0.0"

pytest-randomly = "^3.15.0"

pytest-sugar = "^0.9.7"

fakeredis = "^2.19.0"

```

  

Okej zobaczę one w takim razie cztery grupy,  tą główną grupę która będzie stanowić grupę pakietów na produkcję,  i pozostałe trzy grupy `testers`, `dev` oraz `linters`.

  

I teraz jak możemy z tego skorzystać.

  

Na samym początku naszego zjazdu jak budowaliśmy nasze środowisko  użyliśmy takiej komendy jak:

  

```bash

poetry install

``` 

  

(MOŻLIWE, ŻE TRZEBA BEDZIE PUŚCIĆ POTEM `poetry lock --no-update`)

  

Okej widzimy że tak naprawdę nie musimy uniknąć zachować, Więc ta nasza roszady która  zrobiliśmy w pliku nie wpłynęły na nasze środowisko.  Ale pytanie jak skorzystać z tych grup które sobie otworzyliśmy.  podczas rysowania środowiska czyli podczas korzystania z `poetry install` To co my możemy zrobić to wskazać grupę pakietów którą chcemy zainstalować. 

  

```bash

poetry install --only=main

```

  

Jak widzicie wskazuje tylko i wyłącznie żeby zaisntalował mi pakiety  z grupy main czyli to. Ale oprócz tego. Jeżeli puścimy ten kod to po prostu on doinstaluje nam to, co może nam potencjalnie brakować. Bo na przykład dopisałem sobie ręcznie jakiś pakiet tutaj w `pyproject.toml` I chcę żeby on jeszcze go zainstalował więc mogę wskazać po prostu  żeby zainstalował tą grupę  jeżeli są jakieś rozbieżności to on doinstaluje pakiet do naszego środowiska. Natomiast jest jeszcze jedna bardzo ciekawa flaga `--sync`. Która mówi o tym żeby zsynchronizować środowisko  i zobaczmy co teraz ci zadzieje

  

```bash

poetry install --only=main --sync

``` 

Zobaczcie że on synchronizuje na środowisko z tym co mamy zdefiniowane w tej grupie głównej.  Czyli on stworzy środowisko które ma tylko i wyłącznie te pakiety które tam określiliśmy dlatego widzicie że on usuwa część pakietów.  czyli ja jestem w stanie dostać środowisko składające się tylko wyłącznie z danej grupy pakietów. Teraz pytanie jakie możemy sobie zadać jak on respektuje wersję pakietów,  Czy jest takie ryzyko że  wersje pakietów mogą się różnić. No bo na przykład dajmy na to `pandas` wymaga `numpy` w wersji  2.0,  a pakiet który jest w grupie `dev`  wymaga wersji numpy 1.0, To instalując tylko i wyłącznie pakiety z grupy mailu  dostaniemy `numpy` 2.0, A gdybyśmy zainstalowali całość środowiska  to wtedy dostaniemy `numpy` 1.0. To nie jest prawda.  w momencie kiedy instalujecie tylko pakiety z danej grupy,  to `poetry` respektuje zależności pomiędzy wszystkimi pakietami Niezależnie od tego w jaki ona grupie są.  dlatego Maciek gwarancję że dostaniecie  te same wersję pakietów  tylko różnica będzie polegała na tym jakie pakiety macie dostępne.

  

Więc ja za pomocą tej  komendy którą teraz zrobiłem jest Zainstalowałem sobie środowisko  wręcz produkcyjną.  nie mam `matplotlib`, nie mam `pytest`, nie mam `mypy`.

  

(POKAŻ IM, WEJDŹ DO KONSOLI I ZRÓB `import matplotlib`)

  

A co jeżeli ja teraz chcę wrócić do całego środowiska czyli mieć wszystkie pakiety z wszystkich grup  to używam wtedy po prostu:

  

```bash

poetry install --sync

```

  

I on nam po prostu instaluje wszystko

  

A co jeżeli chciałbym środowisko składające się z dwóch grup?

  

```bash

poetry install --only=main,dev --sync

```

  

(POKAŻ IM, WEJDŹ DO KONSOLI I ZRÓB `import matplotlib`)

  

Mamy te pakiety, ale na przykład nie mamy nic z grupy `testers`.

  

To dlaczego jest to game changer?  na co dzień będzie pracować żeby została po kilkuosobowych i w wiele osób  będziecie rozwijać wasze modele. Po prostu wszystko to co jest wymagane do dewelopmentu  a nie jest potrzebna produkcji  wystarczy zapisać w grupie `dev`,  bo do gryfa już będzie mógł sobie tylko i wyłącznie to co jest w grupie main.

  

A co jeżeli jakiś  z będzie Potrzebne na produkcji,  albo vice versa.  żaden problem wystarczy po prostu ten wpis Skopiować do odpowiedniej grupy - i tyle. 

  

Także bazując na swoich właśnie doświadczenia  oraz na różnych forach dyskusyjnych  poświęconych właśnie wdrażaniu modeli,  `poetry` cieszy się naprawdę dużym zainteresowaniem i jest szeroko używany na produkcji, właśnie z tych wszystkich względów, które Wam teraz opisałem - i polecam go używać, bo naprawdę odmieni to Waszą pracę, a w szczególności pracę w dużym zespole. 

  

Bo w takim razie wracamy do naszego Dockerfile i dodajemy do niego poetry oraz instalujemy tylko to co potrzebujemy produkcji. Pozbywamy sie kopiowania requirementsów oraz instalowania ich pipem i wstawiamy sobie:

  

```dockerfile

…

COPY poetry.lock poetry.toml pyproject.toml ./

ENV POETRY_LOCATION=/opt/poetry

RUN python3 -m venv $POETRY_LOCATION \

    && $POETRY_LOCATION/bin/pip install poetry==1.7.0 \

    && $POETRY_LOCATION/bin/poetry install --only=main \

    && rm -rf .poetry_cache

…

```

  

1. kopiujemy wszystkie   niezbędne pliki do tego aby poetry zainstalowało na środowisko bez żadnych problemów.
    
2.  następnie ja ustawię sobie  tutaj zmienną środowiskową `POETRY_LOCATION`  którą będę potem używał w  komendzie instalującą poety
    
3. następnie mamy komendę RUN która  tworzy w środowisko wirtualne,  Instaluje  w nim poetry,  a następnie korzystając z zainstalowanego poetry w tym środowisku instaluje nasze środowisko z pakietamu tylko w grupie main.  ten sposób instalacji  jest rekomendowany w sposób instalacji poetry  zgodnie  z ich dokumentacją. 
    
4.  i na końcu mamy usunięcie Kesha.  niestety Poetry nie ma takiej flagi jak pip  więc  musimy tego cachea ręcznie usunąć.  nasz plik `poetry.toml` Wskazuje że folder z keszem będzie w tej samej lokalizacji,  i nazywać się będzie poetrykarz  i pozbywamy się go ręcznie korzystając z `rm`.
    

  

I sprawa załatwiona. 



Zbudujmy sobie nowy kontener:


I spushujmy go do repo

Czy macie jakieś pytania odnośnie `poetry`?

  
**