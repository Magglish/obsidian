# 36. Wrzucenie kontenera na VMke na chmurze



Okej Słuchajcie Przechodzimy teraz do ostatniego tematu który nie jest stricte kontenerowy,  dockerowy  a bardziej temat sieciowy, ale przy okazji sprawdzimy czy możemy wdrożyć nasz model już przy naszej obecnej wiedzy - tzn. czy wystarczy umiejętność stworzenia API i kontenera, żeby nasz model pojawił się na przysłowiowej “produkcji”. Temat sieciowy jest istotne do poruszania dlatego że  w naszym kodzie na API widzieliście tego informacje IP o wartości 0.0.0.0,  czy też nazwę `localhost` , Z kolei w Dockerfile mamy komendę EXPOSE,  a w momencie kiedy uruchamialiśmy nasz kontener to musieliśmy ustawić tego flagę jak `-p`  i wskazać na port 8080.  na koniec roku zjazdu Chciałbym porozmawiać z wami O właśnie w kwestiach sieciowych  i komunikacji pomiędzy serwisami.  będzie to zakończenie tematu kontenerów i jednocześnie  krótki wstęp do tego czym będzie zajmować się na następnym zjeździe poświęconym Kubernetesowi.

  

Ale na początek, Wróćmy sobie do naszego grafu który  prezentował proces wdrożenia z lotu ptaka.  to co było na nie pokazane to to że w pierwszym kroku budujemy sobie API która pozwala na interakcję modelem,  następnie używaliśmy kontenerów wirtualnych zasobników które mają w sobie nasz model, Naszą implementację,  wszelkie zainstalowane zależności,  Krótko mówiąc Kontener ma w sobie to wszystko co jest pod wymagane do prawidłowego działania naszego API.  i jego zaletą jest to że jest bardzo łatwo przenoszony pomiędzy różnymi  lokalizacjami  i jest on też wyizolowany,  co pozwalam na to że nie musimy się obawiać że popsuje nam coś na systemach na których  będzie on działał.  jak widzicie ten rafie my zakładamy że dalsze wdrożenie modelu będzie odbywać się na Kubernetesie  generalnie z takim case'em spotkacie się  na produkcji najczęściej, więcej o tym powiem dokładniej na następnym zjeździe. No dobrze ale ktoś może zapytanie czy można wdrożyć modele w ogóle nie korzystając z Kubernetesa? Czy można wdrożyć modele w ogóle nie korzystając z kontenerów? Odpowiedź brzmi tak ale generalnie będziemy mieli z tym duże problemy i wyzwania  z tym związane.  pracę jako będziecie musieli włożyć w zarządzanie takich workflowem będzie bardzo ogromna, ale dzisiaj spróbujemy to sobie zrobić. Czyli spróbujemy sobie wdrożyć coś ręcznie, korzystając z kontenerów i z zasobów dostępnych na chmurze.  to nie będzie oczywiście pełne wdrożenia tylko pewien przykład  jak to zrobić  i co trzeba było jeszcze dorobić żeby to miało ręce i nogi.  ale to będzie dobry przykład żeby omówić sobie też  jakie problemy są w tym związane, Jak to możemy rozwiązać,  i przy okazji bardzo mogę sobie o komunikacji między  serwisami i w ogóle ruchem sieciowym  i  też rozwiążemy sobie te nasze zagadki odnośnie IP  czy portów które w kontenerze musieliśmy sobie ustawić.

  

Na potrzeby tego ćwiczenia, Stworzyłem dla każdego z was i wirtualną maszynkę na naszej chmurze.  Spójrzmy sobie do Compute Engine -> VM instances na google cloudzie i mamy te maszynki które są określone pierwszą literą imienia i potem nazwiskiem i na końcu sufix `-vm`. Niech każdy z Was odnajdzie swoją VMkę  i po prawej stronie jest takie przycisk jak SSH. Kliknijcie w niego i wtedy otworzy sie konsola, która od razu połączy Was do maszynki wirtualnej. Jeśli pojawi się okienko z opcją “Authorize” to ją kliknijcie. Okej w ten sposób udało się połączyć z naszą wirtualną maszynką. To co my teraz będziemy robić to spróbujemy “wdrożyć” sobie w cudzysłowiu  nasz model na tą VMkę  i  Zobaczymy czy każdy z nas będzie mógł odpytać VMkę innej osoby na której jest API wysyłając requesta i dostając odpowiedź. Oczywiście w chmurze są dedykowane usługi które pozwolą wam na wdrożenie waszych modeli poprzez interfejs, pokaże Wam również to na przykładzie w Vertex AI, GCPowy moduł stricte do uczenia maszynowego. Najpierw chciałbym  żebyśmy spróbowali sobie zrobić to ręcznie niskopoziomową  i omówili sobie jeszcze co trzeba by było dorobić żeby to miało ręce i nogi, a potem na końcu spróbujemy użyć sobie dedykowanej usługi do wdrażania modeli która jest na gcp Tak żebyście mieli porównanie  on będzie takim ręcznym a używaniem usługi dedykowanej.

  

Okej to jesteśmy na naszej VMce. Zacznijmy od końca - pytanie jakie zadałem to to czy można wdrożyć model Nie korzystając z kontenerów. Oczywiście jak najbardziej.  można to zrobić tak samo jak robiliśmy to na naszym komputerze lokalnym, czyli będąc na VMce, Musielibyśmy pobrać  nasze repozytorium,  stworzyć  wirtualne środowisko, W nim zainstalować swoje biblioteki  i następnie uruchomić nasze API, tak jak  robiliśmy to lokalnie  i trzeba się upewnić że ruch sieciowy do naszej VMki jest otwarty. O tym ruchu się czasem powiem później więcej. Natomiast my nie będziemy tego robić, bo musielibyśmy skonfigurować konto gita, żeby nasze repozytorium pobrać i tak dalej, zajęłoby to troche czasu, a nie chce go tracić dlatego, że tak się obecnie nie wdraża modeli. I ten sposób nie jest jakimś rocket science bo poradzilibyście sobie z tym bez problemu. Dlaczego tak się nie robi, no zarządzanie takim sposobem wdrażania jest oczywiście możliwe, ale na prawdę pracochłonne i ma wiele wyzwań, chociażby jak zarządzać zmianami w kodzie, generalnie nikt tak nie robi. Po to właśnie są stworzone różne dedykowane rozwiązania pozwalające na przenoszenie stworzonych rozwiązań na jakiś server - w tym celu wykorzystuje się właśnie kontenery dzięki czemu można było bardzo łatwo przenosić nasze rozwiązania,  mają one w sobie gotowe środowisko z zainstalowanymi wszystkimi zależnościami I generalnie zarządzanie  wdrożeniami  opartymi o kontenery jest znacznie prostsze  i przyjemniejszy  niż tworzenie na wirtualce ręcznie wirtualnych środowisk i instalowanie pakietów. O wadach i zaletach kontenerów mówiłem sobie wcześniej.  więc na pytanie Czy można wdrażać coś bez kontenerów? Tak, Ale zarządzanie czymś takim jest bardzo trudne  i generalnie nie jest to  obecna praktyka.  i standardem rynkowym jest po prostu wykorzystywanie kontenerów, ze względu na ich zalety.

  

Pytanie  drugie jakie zadałem, To czy można wdrażać modele Nie korzystając z klastrów Kubernetes?  Oczywiście odpowiedź też brzmi tak, o Kubernetesie jeszcze nic nie wiecie  ja nie będę tutaj w żaden sposób jeszcze  go wprowadzał  bo jemu  jest poświęcony cały następny zjazd, natomiast krótko mówiąc Kubernetes służy do zarządzania infrastrukturą obliczeniową skłądającą się z wielu wirtualnych maszynek, a na tych wirtualnych maszynkach istnieje wiele serwisów działających jako oddzielne kontenery.

To co chcę powiedzieć To to że  wdrożenia modeli, czy też innych usług wciąż mogą odbywać się na wirtualnych muszelkach które są zarządzane ręcznie przez deweloperów, a nie przez dedykowaną usługę jak na przykład Kubernetes. Wciąż możecie się spotkać z takimi procesami wdrożeniowymi, gdzie po prostu zarządzanie VMkami jest robione ręcznie, bez wspomagania się jakimiś dedykowanymi narzędziami jak Kubernetes, ale pewnie takich przypadków jest teraz znacznie mniej. Najczęściej z takimi przypadkami spotkanie się pewnie na rozwiązaniach on-prem, czyli na dedykowanych serwerach które stoją w szafie obok was w firmie. Natomiast przy pracy na chmurze bardzo często spotkanie się z wykorzystywaniem dedykowanych usług do wdrażania i dzisiaj sobie taką wykorzystamy, albo po prostu stawiany jest klaster Kubernetes, bo chmura sama nim zarządza i my skupiamy się głównie na jego wykorzystaniu do naszej pracy a nie na ciągłym administrowaniu - powiem o tym więcej na następnym zjeździe. 

  

Teraz dlaczego o tym mówię? Chciałbym Wam pokazać teraz, że wiedza jak stworzyć API oraz jak zbudować kontener jest już wystarczająca do tego, żeby móc wdrażać pierwszy model na produkcję. Na początku skorzystamy sobie z rozwiązania, określę go roboczo, “niskopoziomowego” To znaczy wdrożymy sobie model na VM ręcznie  i zobaczymy czy to zadziała  a następnie omówimy sobie wady i zalet.  następnie skorzystamy z dedykowanej usługi do wydarzenia modeli na gcp  i też omówimy sobie wady i zalety takiego rozwiązania.  Oba rozwiązania są dobre ale to wszystko zależy od tego w jakim kontekście to wdrażacie i w jakim celu, na jaką skalę, jak istotny jest Wasz model i ile bęðziecie zarabiać z tego pieniędzy.

  

Zaczniemy Sobie od tego niskopoziomowego rozwiązania,  czyli wdrożymy sobie Ręcznie model na VMkę. 

  

W takim razie korzystając z naszych kontenerów z naszym API Która dostępna jest repozytorium po prostu  wdrożymy sobie to na naszą VMkę.  i korzystając z kontenerów będzie to bardzo proste:

Na początek mus

```bash
gcloud auth print-access-token | sudo docker login -u oauth2accesstoken --password-stdin europe-central2-docker.pkg.dev
```

 pobieramy sobie nasz obraz który był w naszym repozytorium z naszym api:

  

```bash

sudo docker pull <<OBRAZ_Z_REPO_Z_API>>

```

  

Następnie po prostu uruchamiamy go w trybie `--detach` i wskazując port 80:8080. Uwaga wskazujemy port 80:8080

  

```bash

sudo docker run -d -p 80:8080 --name=my-api <<OBRAZ_Z_REPO_Z_API>>

```

  

Okej. Wróćmy do widoku naszych VMek  i zwróćcie uwagę Że przy VMce mamy takie dwa adresy Internal IP oraz External IP. Kliknijmy sobie w External IP i po chwili powinien pojawić się nasz tekst z przywitaniem. Wejdźmy sobie do endpointa `/docs`. I działa.  teraz możemy sobie wrócić do naszego `send_example_request.py` I po prostu w  naszej zmiennej `url` zmienić na konkretny IP i na port 80 

  

```python

url = "[http://<<IP_VM_NA_GCPIE>>:80/decisions](http://localhost:8080/decisions)"

```

  

i wysłać naszego requesta.

  

```bash

make request

```

  

Jak widzicie działa.  Czyli w bardzo prosty sposób udało nam się wdrożyć pierwsze rozwiązanie na chmurę i tak naprawdę możemy teraz sobie odpytać różne ipki waszych maszyn i odpowiednio odpytać model każdego z was.  i może tak samo zrobić każda osoba teraz  na całym świecie Korzystając z tego IPka żeby skorzystać z Waszego API. Oczywiście to nie jest idealne rozwiązanie, powiem Wam później czego brakuje, żeby to można było nazwać produkcyjnym wdrożeniem.

**