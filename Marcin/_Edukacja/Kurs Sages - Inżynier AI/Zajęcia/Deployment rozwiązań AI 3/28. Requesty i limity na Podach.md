# Requesty i limity

  

Przechodzimy sobie teraz do 2-giej  bardzo ważne rzeczy które musi być na deploymentach  czyli temat związane z określeniem ile pamięci RAM  i CPU potrzebujemy na nasz deployment.

  

 Przejdźmy sobie do konsoli googlowskiej i zobaczmy na początek  jakie mamy maszynki czyli loauty na naszej klastrze. Wejdźmy w Cluster, następnie kliknijmy sobie na nasz klaster devowy. Następnie zakładka Nodes. 

  

Obecnie działa sobie X maszynek  w związku z naszymi dyplementami które utworzyliśmy.  i tutaj jest taka informacja o tym ile możemy zaalokować CPU oraz Memory,  ile już jest wykorzystywane czyli  czyli requested.  w sytuacji kiedy na jakiejś maszynce  wirtualnej zabraknie albo CPU albo RAM,  to kubernetes będzie starał się zostawić kolejną maszynkę wirtualną,  czyli kolejny Node,  i na ten nowy Note będzie wtedy wdrażał nasze Pody.  i będzie to robił tak długo aż powstanie tyle podów ile określiliśmy mu w Deploymencie. 

  

 Otwórzmy sobie teraz drugie okienku nasz deployment. Mamy informacje o tym dobrym momencie oraz informacje o tym ile CPU i RAMu wykorzystujemy.  na razie nie zwracajmy uwagę na wykorzystanie CPU  dlatego, że jest to wykorzystanie CPU w stanie spoczynku. Nie wiem jeszcze ile CPU będziemy potrzebować w momencie kiedy będzie przychodził ruch do naszego serwisu,  ale sprawdzimy to niedługo.  jest informacja o pamięci jaką potrzebujemy.

  

Jeżeli dojedziemy sobie na wykres to mamy też Jeszcze dwie serie  zwane `Requested` i `Limit`  które wynoszą 0. I to co będziemy teraz omawiać to właśnie umiejętne ustawienie requestów i limitów na naszych Podach.

  

 teraz musicie mieć na względzie jedną bardzo ważną rzecz,  w momencie kiedy patrzymy na Deployment to wykresy z CPU oraz Memory  to suma  wykorzystania wszystkich Podów.  natomiast to co my będziemy ustawiać za chwilę to wykorzystanie CPU oraz Memory  na jednym Podzie,  zatem te wartości które to widzimy trzeba podzielić przez liczbę Podów.  ale  prościej po prostu  wejść sobie do pojedynczego poda i zobaczyć na nim te wykresy. I to już jest jakby  informacja o naszym pojedynczym kodzie.  mówi wam o tym bo często można się po prostu pomylić  patrząc na deployment i ustawić potem  zasoby na diploymencie  ale zapominając o tym żeby podzielić się przez liczby Podów  i potem będzie problem Dla kogo renesa żeby takie Pody zdeployować  bo nagle  Podów wymagają ogromnych zasobów,  kilku czy kilkunastokrotnie większych niż w rzeczywistości.

  

Teraz Dlaczego musimy to zrobić i dlaczego jest to istotne. Generalnie  kubernetes poradzi sobie z alokowaniem podów nawet jeżeli nie ustawicie na nich wymaganych zasobów.  Wróćmy sobie do  naszego widoku maszynek I zobaczysz że mamy tutaj wolnych X CPU oraz Y Memory. I teraz  w sytuacji w której nie ustawimy tych requestów i limitów na naszych Podach to Kubernetes nie będzie wiedział generalnie ile zasobów potrzebuje żeby ten Pod działał poprawnie. I Wyobraźcie sobie sytuację że chcemy teraz zdeployować 1000 Podów na raz.  nieważne ile to będzie kosztowało dokładnie CPU i Ramu,  natomiast no 1000 Podów na pewno się nie zmieści na naszej jednej maszynce wirtualnej którą tutaj widzimy.

 teraz co zrobi kubernetes w takiej sytuacji jeżeli nie ma informacji o tym i jakie są wymagane zasoby na podach?  w związku z tym że nie ma informacji o tym jakie są zasoby wymagane na Podach,  on po prostu  postara się umieścić 1000 Podów na raz na tej jednej maszynce wirtualnej. Sytuacji kiedy pierwsze Pody zaczną się pojawiać i w związku ze swoim pojawieniem się Zaczną działać,  a żeby poprawnie działać to będą  wykorzystywały CPU i RAM w jakiejś ilości,  to wraz z Właśnie pojawianiem się kolejnych Podów poprawnie zainicjowanych,  zaczną  powoli wyczerpywać zasoby dostępne na tej maszynce wirtualnej z każdym kolejnym kodem. Załóżmy że 50 podów zmieściło się na tym Nodzie,  a pozostałe 950 Podów  rzuca błędem Że nie ma wystarczających zasobów CPU lub Ramu,  Kubernetes to widzi  i w związku z tym będzie dostawał  kolejnego Node’a. I jak powstanie kolejny Note to te pozostałe 950 Podów znowu przerzuci na tą drugą maszynkę. I ten proces będzie tak postępował aż w końcu zostanie tyle Nodeów  żeby móc te 1000 Podów umieścić I żeby każdy z tych Podów  miał wystarczającą przestrzeń na wykorzystanie CPU i Ramu w takim zakresie w jakim potrzebuje do prawidłowego działania.  czyli kubernety sobie z tym poradzi,  Ale jakim kosztem?  No koszt jest ogromny dlatego że  będziecie widzieć mnóstwo podów z błędami związanymi z brakiem pamięci lub CPU.  po drugie cały proces  tych Deploymentu tych podów będzie trwał naprawdę bardzo długo. Problem też może być taki,  że  ta sytuacja może wpłynąć też na inne serwisy które zdeployowane są na node’ach.  jeżeli ta sytuacja doprowadzi do problemów z innym serwisami,  No to inne osoby mogą inwestować o co chodzi,  I generalnie po pewnym czasie klaster się ustabilizuje,  stworzy  tyle nodeów ile potrzeba żeby obsłużyć wszystkie Deploymenty,  ale nie chcemy  żeby taka sytuacja się zdarzyła bo jest ona problematyczna dla wszystkich osób które mają  postawione serwisy.

  

 w sytuacji kiedy ustawimy odpowiednią wartości w requestach i limitach  na CPU i RAM,  kubernetes po prostu na początku sprawdzi ile ma wolnej przestrzeni na obecnych Nodeach. Jeżeli na jakimś No udzie jest trochę przestrzeni to tam zacznie tworzyć mody ale tyle  na ile starczy miejsca,  a w przypadku pozostałych Podów po  najpierw otworzy odpowiednie Node  w odpowiedniej ilości,  i dopiero jak te nudy powstaną to wasze Pody zostaną tam wdrożone. 

  

Czyli Innymi słowy,  ten cały proces się odwróci.  najpierw powstaną Nodey  z odpowiednimi zasobami,  na których można te Pody wdrożyć. Dlaczego tak bardzo ważne jest to żeby każdy z waszych dyplomentów miał ustawione wymagane wartości w CPU i RAMie.

  

I generalnie rzecz biorąc  możecie spotkać się swojej pracy,  Że klastry będą tak  skonfigurowane  że np. Nie będzie możliwe wdrażanie Deploymentów  bez ustawionych requestów i limitów na CPU i RAM. Po prostu dostaniecie błąd. Tak samo może być w przypadku probingu  który wcześniej ustawiliśmy. My takiej konfiguracji Nie mamy na klastrze,  Natomiast jeżeli ktoś z was jest zainteresowany tą tematyką  to podsyłam  linka do obiektu w Kubernetesie który za to jest odpowiedzialny -> [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) oraz stosowany często w praktyce biblioteka, z którą takie rzeczy się implementuje [https://kyverno.io/](https://kyverno.io/) Ja osobiście jestem zwolennikiem blokowania takich dyploymentów na poziomie klastra bo naprawdę praktyka pokazuje że Deploymenty bez requestów i limitów oraz bez ustawionego probingu to rzeczy które psują życie wszystkim osobom korzystającym z klastrów do swojej pracy. 

  

## RAM

  

Okej w takim razie zaczniemy sobie od ustawienia na początek RAMu.  technice rzecz biorąc jak zobaczycie ustawienie requestów limitów to jest po prostu wstawienie wartości w manifeście i tyle.  Natomiast musicie wiedzieć o pewnych mankamentach związanych z ustawianymi zasobów szczególności w API do modeli uczenia maszynowego. 

  

Patrząc na pojedynczego poda widzimy że wykorzystanie ramki wynosi X MB.  Jednakże to już jest po wdrożeniu,  natomiast możecie znaleźć się w sytuacji że bez określenia requestów i limitów po prostu nie tego Deploymentu na klaster. Jak zatem sprawdzić ile potrzebujemy pamięci RAM dla naszego Poda? 

  

Sprawa jest prosta.  po prostu uruchamiamy nasze API lokalnie.  my skorzystamy tutaj z docker-compose którego mieliśmy zdefiniowanego:

  

JEŚLI NIE ZADZIAŁA TO ODPAL PO PROSTU OBRAZ W KTÓRYM NIE MA POŁĄCZENIA Z REDISEM I POSTGRESEM

  

```bash

make  api-compose-up

```

  

I Ważne jest to żeby wysłać przynajmniej jednego requestu do naszego API, bo może być tak  w wyniku pierwszego requesta coś jeszcze zostanie stworzone w naszym API. 

  

```bash

make request

```

  

A teraz po prostu możemy skorzystać sobie  `docker stats` żeby zobaczyć ile pamięci nasz kontener potrzebuje. Jak widać jest to mniej więcej ta wartość którą  tutaj widzimy na naszym wykresie. 

  

 nie polecam ustawiania oczywiście dokładnej wartości co do kilobajta,  zawsze dajcie sobie jakieś bufor,  kilkanaście MB więcej. na przykład niech to będzie 130 MB. 

  

Okej to jest pierwsze Sprawdzanie jakie  możemy wykonać żeby określić zapotrzebowanie na pamięć RAM.  wpisz mi osobę do naszego manifestu z deploymentem, na poziomie kontenera:

  

```yaml

          resources:

            requests:

              memory: 130Mi

```

  

Teraz w przypadku ustawiania Memory polecam Wam korzystać z jednostki mebibajtów albo gibibajtów, a nie megabajtów lub gigabajtów - czyli do literki M lub G  dodajemy małe i. dlatego, że zarówno `docker stats` jak i Wykres na chmurze będą pracować wam wyniki właśnie w mebibajtach albo gibibajtach  i dzięki  ustawianiu wartości w tych metrykach w których jest raportowana przez dockera i wykresy Będzie wam łatwiej trafić w te wartości które chcecie.

  

Natomiast w przypadku API MLowego trzeba sprawdzić jeszcze jedną rzecz -  Czy w waszym API nie występuje problem nazywany memory leak.  wykres   najlepiej Pokaże o co mi chodzi:

  

![](https://lh7-us.googleusercontent.com/E-aKNIyeOGr85EM3k4pBGF5Ny-srD72SbXAbaaoUpQznnrgWv8Cnrjn_sWv5T4egTPw1DyK59nnLfB04Xq1L9SAxHie59-8vZPIDmzHE-cWpA_05H1gCvU16XepheaZq_mAr8knULZ3y-_KnYlb2Rp0)

Źródło: [https://github.com/googleapis/cloud-debug-nodejs/issues/811#issuecomment-980311234](https://github.com/googleapis/cloud-debug-nodejs/issues/811#issuecomment-980311234)

  

To jest przykładowy serwis który  boryka się tak zwanym memory leakiem, tłumacząc na polski - wyciek pamięci. Chodzi o to że wraz z działaniem serwisu  wykorzystanie pamięci cały czas rośnie. Widać to na tym wykresie.  ten serwis Działa Sobie przez pewien czas,  osiągając wykorzystanie pamięci  takie jakie zostało określone w limitach.  w sytuacji kiedy wykorzystanie pamięci zaczyna przekraczać limity kubernetes widzi to i restartuje Poda.  Dlaczego Widzicie na wykresie taki spadek wykorzystania pamięci,  to jest moment w którym pod został zrestartowany  i działa na nowo.  i znowu wraz z jego działaniem Problem występuje,  i w kółko jest restartowany.

  

Teraz dlaczego jest to problem?

1.  Po pierwsze w takiej sytuacji Wasz serwis będzie cyklicznie się zrestartował w momencie kiedy  wykorzystanie pamięci sięgnie limitów,  zatem będziecie  obserwować w pewnym momencie że wasze serwisy są przez chwilę niedostępne  co może wpłynąc  negatywnie na korzystanie z masy serwisów czy to przez klientów czy przez inne serwisy w waszej firmie.
    
2.  Po drugie,  ten problem może by zniwelować po prostu alokując znacznie więcej pamięci dla naszego serwisu,  w ten sposób ta częstotliwość restartowania poda Będzie znacznie mniejsza.  ale jeżeli pójdziemy w to rozwiązanie to  alokując więcej pamięci niż jest to wymagane dla działania naszego serwisu w sposób prawidłowy  po prostu zapłacicie znacznie więcej pieniędzy, Bo po prostu dla swoich serwisów alokujecie znacznie więcej pamięci RAM,  więc koszty waszej infrastruktury znacznie rosną
    
3.  i po trzecie,  jeżeli naprawdę spotykacie się z taką sytuacją to oznacza że jest jakiś bug w kodzie.  Bug polegający na tym,  że po prostu jakiś obiekt cały czas rośnie w trakcie działania waszego serwisu -  to może być  jakaś lista do której coś jest appendowane  non stop w ramach działania serwisu,  to może To może być jakaś ramka danych pandasowa  do której jest dodawany jakiś nowy rekord za każdym razem kiedy otrzymujemy requesta do naszego API.  generalnie jest to problem bardzo poważny problem i należy go zidentyfikować przed wdrożeniem naszego API  i postarać ci go rozwiązać jeśli on wystąpi.
    

  

 Sytuacja pożądana jaką chcemy zobaczyć w takiej sytuacji jest to że po  po tym jak nasz serwis zostanie zainspirowany i wysłane do jego pierwszy request  to po tych dwóch zdarzeniach  zobaczycie wykorzystanie pamięci i one powinno być praktycznie stałe w czasie działania waszego API. W zależności oczywiście od tego jak API Wasze Ciała możecie zobaczyć drobne fluktuacje,  ale Trend powinny wskazywać na stagnację w wykorzystaniu Memory - powinien być płaski. Powinien być płaski i najlepiej żeby wykorzystanie pamięci cały czas było troszeczkę poniżej waszych requestów na memory.  to oznacza że wasze serwisy działały poprawnie oraz zalokowaliście poprawną wartość Memory na Wasz serwis.

  

Teraz pytanie czy można zidentyfikować memory leaka przed wdrożeniem naszego API?  odpowiem brzmi tak jest to proste i Chciałbym pokazać jak  sprawdzić czy wasze API ma memory leaka. 

  

W tym celu wykorzystamy bibliotekę o nazwie memory_profiler [https://github.com/pythonprofilers/memory_profiler](https://github.com/pythonprofilers/memory_profiler) 

  

```bash

poetry add memory_profiler

```

  

I po jej zainstalowaniu sprawdźmy czy jest dostępna:

  

```bash

mprof

```

  

Sprawdźmy też czy mamy matplotliba zainstalowanego, bo bedzie potrzebny do wygenerowania wykresu.

  

Teraz żeby z niej skorzystać w naszym celu wykrycia memory leak,  to chciałbym żebyś uruchomili API w sposób stary to znaczy oddzielnie redisa i postgresa za pomocą kontenerów, a api jako pythonowy skrypt, bo to niestety nie zadziała na api w kontenerze.

  

Czyli najpierw bazy danych:

  

```bash

make redis

make postgres

```

  

Dalej z modyfikujemy sobie nasz skryp `send_example_request.py`  w taki sposób żeby nie wysyłał nam tylko jednego requesta ale żeby wysłał nam ich nam ich na przykład 500.

  

Na początek zaimportuje sobie bibliotekę `random`:

  

```python

import random

```

  

Z której zaraz skorzystam.

  

I następnie zmodyfikuje sobie wysyłke requesta żeby wysłać ich 500 poprzez zwyklą petlę for:

  

```python

for _ in range(500):

    data = data | {"age_in_years": random.randint(a=18, b=65)}

    response = requests.post(url=url, headers=headers, json=data)

    print(f"Received {response.status_code=}")

    print(f"Received {response.json()=}")

```

  

Przy czym za każdym razem będę losował sobie wiek. Robię to specjalnie tak żeby zobaczyć działanie API wtedy kiedy przetwarza request oraz wtedy kiedy korzysta z Redisa żeby zasięgnąc cachea. To jest takie troszeczkę bardzo prosty sposób zasymulowania tego jak serwis może być odpytywany na produkcji.  na początku Cash nie będzie wypełniony więc nasz model będzie używany cały czas żeby wykonywać predykcje,  a potem Wraz z kolejnymi requestami ten Cash będzie coraz bardziej wypełniany więc od pewnego momentu powinniśmy widzieć tylko i wyłącznie sięganie do redisa bez odpalania naszego modelu.  więc to jest taki bardzo prosty schemat możecie naszego API który  w pewnym stopniu już symuluje nam nasze wykorzystanie serwis na produkcji. 

  

W takim razie Po tych zmianach po prostu uruchommy nasze API w specjalny sposób 

  

```bash

mprof run --python python src/service/main.py

```

  

i wyślijmy sobie te requesty i poczekajmy chwilę:

  

```bash

make request

```

  

Przejdźmy do okienka z API i po prostu zatrzymajmy je za pomocą CTRL +C możlie, że dwa razy. 

  

W wyniku tego działania Pojawiły nam się dwa pliki w naszym repozytorium zaczynające się od `mprofile`.  i teraz to co my musimy zrobić to powiedzieć memory_profiler żeby wygenerował nam zużycie pamięc inaszego serwisu:

  

```bash

mprof plot

```

  

I to co Zostańmy w wyniku to po prostu wykres wykorzystania  pamięci RAM na naszym serwisie w czasie.  na początku widzicie że ten wykorzystanie pamięci wzrosło do poziomu X.  to jest związane właśnie z faktem inicjalizacji Naszego API i wszystkich obiektów Które są wymagane.  natomiast widzimy że zużycie pamięci jest potem płaskie w trakcie jak  wysłaliśmy do naszego API 1000 requestów.  czyli na tej podstawie nie powinniśmy spodziewać się problemów z orlikiem w momencie kiedy wdrożymy nasz API na produkcję.  i tak przeważnie Powiem wam szczerze jest że jak tutaj lokalnie sobie sprawdzimy to to generalnie też to nie powinno występować na produkcji. Czyli powiecie się spodziewać takiej płaskiej linii jak widzicie W przypadku używania waszego API.

  

Okej A jak wygląda memory leak? Tak bym powiedziałem Memory x z kolei to jest jakiś bug  w kodzie który powoduje to że zużycie pamięci rośnie.  Może coś objawiać po prostu jaki obiektem którego rozmiar jest coraz większy.  taki memorlik można sobie łatwo zasymulować po prostu dodając do listy jakieś duży obiekt.

  

Dodajmy sobie do `startup` obiekt:

  

```python

    app.state.object = []

```

  

A następnie w endpoincie dodawajmy cały czas do niego jakiś element:

  

```python

    app.state.object.append(bytearray(1000000))

```

  

Uruchomimy redisa jeszcze raz żeby wyczyścić cache:

  

```bash

make redis

```

  

W takim razie Po tych zmianach po prostu uruchommy nasze API w specjalny sposób 

  

```bash

mprof run --python python src/service/main.py

```

  

i wyślijmy sobie te requesty i poczekajmy chwilę:

  

```bash

make request

``` 

  

Zatrzymajmy podwójnym CTRL + C i zobaczmy wykres:

  

```bash

mprof plot

```

  

I teraz widzimy że wraz z kolejnymi głosami te wykorzystanie pamięci nam rośnie.  zatem mamy problem z memory leakiem. 

  

Teraz jak sobie można z tym poradzić?  niestety dla was takich rad które od razu  pozwolą niewystykować i naprawić ten problem wypełnieniu oka, bo  niestety powodów memory leaka może być wiele  i  przy takim problemie

1.  trzeba prześledzić cały kod,  linijka po linijce żeby  dostrzec gdzie może być problem.  bo może być taka sytuacja jaką teraz zasymulowałem Czyli ktoś zrobił baga i zostawił appendowanie do listy jakichś elementów,  albo do pandasowej ramki danych. 
    
2. Problem może być też z biblioteką z jakiej korzystacie bo może tam jest jakiś bak w kodzie i przez to pamięć non stop rośnie.  to w takiej sytuacji  musicie zmienić wersję tej biblioteki albo na nową jeśli już jest naprawione ten bug albo wrócić na starą w której tego buga nie było. 
    

  

 Generalnie rzecz biorąc problem z memory leakiem zawsze leży w kodzie, tylko nie zawsze może leżeć w waszym. Możemy się wspomóc z lokalizacją takiego baga w kodzie właśnie korzystając z tej biblioteki memory_profile  który przed chwilą używaliśmy.  ja Ja ją tylko pokazuję w kontekście właśnie wyrysowania tego wykresu który wam wskaże Czy macie problemy z memory leakiem  Natomiast ta biblioteka dostarcza też do różnych funkcjonalności,  między innymi  możecie udekorować swoje funkcje  za pomocą specjalnego dekoratora i uruchamiając profilowanie może się dostać informację o każdej linijce kodu jak wykorzystuje pamięć i w ten sposób Możecie po prostu krok po kroku dojść do momentu w którym faktycznie jest jakiś kod w którym ta pamięć cały czas rośnie.  natomiast ten proces jest żmudny,  czasochłonny,  ale potrzebny jeżeli taki problemy napotkacie. Jeżeli go zignorujecie no to Gwarantuję wam że wasze serwisy będą cały czas się restartować i korzystali z niej będzie po prostu nieprzyjemne i będziecie mieli zgłoszenia albo od klientów albo od innych osób które korzystają z naszego serwisów żebyś się to po prostu naprawili. 

  

Więc to są dwa najważniejsze sprawdzenia jakie należy wykonać -  czyli na  początek morza skorzystać z docker Stars żeby zobaczyć jakie jest wykorzystanie pamięci,  i Następnie za pomocą tej bibliote memory_profiler  upewniamy się że nie mamy memory leaka,  i że wykorzystanie tej pamięci będzie stałe w czasie kiedy Nasz serwis będzie używany

  

Na koniec usuńmy naszą sztuczną listę z `startup` oraz endpointa `decisions`.

  

W ten sposób mamy zakończony temat związany z Memory teraz przechodzimy do CPU.

  

## CPU

Niestety z tematem CPU nie będzie już tak łatwo żeby go określić lokalnie .Dlatego, że architektura naszego procesora będzie zupełnie inna niż procesora  który będzie dostępny na Nodzie na którym działać będzie nasze API. Dlatego najlepiej po prostu wrzucić nasz deeployment na klaster  na docelowej  maszynce na której to będzie działać  i przeprowadzić pierwszy  bardzo prosty,  żeby zobaczyć jak działa  nasze pod obciążeniem.  generalnie my będziemy robić takie testy obciążeniowe na większą skalę ale dopiero później pod koniec naszego zjazdu.  teraz zrobimy to bardzo prosto  żeby tylko określić mniej więcej ile CPU będzie wymagane przed Poda.

  

Okej to co my musimy zrobić teraz to określić CPU dla pojedynczego poda.

  

 zrobimy to w następujący sposób:

  

 Chciałbym abyście na początek w Deploymencie ustawili liczbę replik na 1,  po to aby właśnie został zdeployowany tylko jeden Pod:

  

```yaml

spec:

  replicas: 1

```

  

Następnie ustalamy wysoki request na CPU i  tutaj Proponuję na początek ustawić  że potrzebujemy jednego całego procesora:

  

```yaml

          resources:

            requests:

              memory: 130Mi

              cpu: 1000m

```

  

Teraz ta jednostka m oznacza mili  czyli 1/1000  procesora.  Można oczywiście  ustawić to sobie po prostu wpisując 1,  i ta jedynka będzie oznaczać 1 procesor,  ale ja na przykład osobiście preferuję ustawiać te wartości w  w wartościach mili.  czyli w tym przypadku 1000m  oznacza 1000 mili CPU czyli 1 procesor.

  

Wdróżmy sobie taki deployment:

  

```bash

kubectl apply -f deploy/k8s/deployment.yaml

```

  

I następnie Przejdźmy sobie do naszego `send_example_request.py`  i wprowadźmy jeszcze kilka zmian:

  

1. zmieńmy url na odpytanie serwisu: `http://api.mrybinski.TWOJA_DOMENA.com/decisions`
    

  

2. Natomiast drugą zmianę jaką chciałbym żeby się prowadzili to losowali troszeczkę inną zmienną niż wiek.  to co my teraz chcemy sprawdzić to wymagania CPU pod pełnym obciążeniem naszego serwisu.  w tej pętli będziemy wysyłać requesty cały czas jak tylko:  jak tylko otrzymamy odpowiedź to wysłamy następny.  natomiast przez to że  wiek jest od 18 roku życia do 65  to  zdarzą się requesty które są takie same W związku z tym nasze API wyciągnie je z cache’a z Redisa,  w przypadku sprawdzania CPU nie chcemy tego chcę zobaczyć  wymagania CPU wtedy kiedy na serwis jest potrzebne obciążeniem,  czyli Chcemy żeby cały czas nasz model był uruchamiany w tym serwisie. Najprościej To uzyskać  w taki sposób że po prostu losujemy wartości dla jakiejś zmiennej która może przyjąć naprawdę bardzo dużo  różnych wartości przez co za każdym razem praktycznie nasz model będzie musiał używany bo takich Wpisów nie będzie w cashu.  Dlatego proponuję żebyśmy zmienili tutaj w naszej pętli i zapięli się na zmienną zmiennoprzecinkową `installment_rate_in_percentage_of_disposable_income`.
    

  
#### TUTAJ NIE USTAWIAJ POWYZEJ 900 W PETLI BO COS JEST NIE TAK I GCP-OWE FIREWALLE CIE BLOKUJA DZIWNIE?

```python

for idx in range(500):

    data = data | {"installment_rate_in_percentage_of_disposable_income": random.random()}

    response = requests.post(url=url, headers=headers, json=data)

    print(f"Received {response.status_code=}")

    print(f"Received {response.json()=}")

```

  

I po tych zmianach możemy po prostu uruchomić nasze requesty które będą wysyłane do naszego Poda na chmurze:

  

```bash

make request

```

  

To co jeszcze chciałbym podkreślić to powinniście zauważyć to to że jednak będziemy musieli dłużej poczekać na to aż ten eksperyment się zakończy niż gdybyśmy uruchamiali to  lokalnie tak to robiliśmy w przypadku sprawdzania wymagań odnośnie pamięci RAM.  robi się to dłużej dlatego że oprócz przeliczenia w wyniku przez model jest też opóźnienie związane z czasem danym przez sieć.  czyli wysyłka requesta z naszego komputera do serwisu który jest na chmurze i on w tym przypadku Siedzi sobie w Belgii  więc jest opóźnienie.

  

Teraz po przeprowadzeniu tych eksperymentów Możemy przejść teraz do chmury i zobaczyć wykres z wykorzystaniem CPU. 

  

Niestety z odczytem wartości o wykorzstaniu CPU i RAMu na chmurze Jest ten minus że przedstawiają one zagregowany stan z ostatniej minuty. Nie wiem jak jest na innych chmurach ale na Google cloudzie tutaj podstawową jednostką czasu jest jedna minuta.  i możemy teraz na wykresie odczytać że nasz Pod potrzebował X CPU. I muszę was martwić ale nie ma możliwości żeby uzyskać informacje o wykorzystaniu CPU  w czasie rzeczywistym.  Kubernetes wykorzystuje bibliotekę pod spodem  o nazwie cAdvisor,  która raportuje wykorzystanie zasobów kontenerów co około 15 sekund.  natomiast wykresy Clouda agregują te informacje do minuty.  natomiast my możemy  skorzystać z dedykowanej komendy kubectl, Do tego żeby właśnie odpytać klaster o wykorzystanie danych zasobów i dostać informację w agregacji 15 sekundowej,  co Będzie znacznie lepszym oszacowaniem niż wykresem na chmurze i zaraz to zrobimy.  ale niżej nie da rady zejść.  dlatego takie sprawdzenie które wykonujemy jest obarczone pewnym błędem.  natomiast proszę się nie obawiać tego że  nie traficie z ustawieniem CPU.  w przypadku Memory  ustawienie wartości znacznie prostsze bo spodziewamy się żeby to była linia płaska tak tutaj Widzicie na wykresie obok,  wiec z trafieniem w  wymogi pod względem RAMu jest bardzo łatwo,  natomiast CPU jest znacznie trudniej  i generalnie  lepiej jest ustawić za dużo CPU,  i potem sukcesywnie jak wasz serwis Działa ewentualnie zmniejszyć jak już zobaczycie wykorzystanie CPU ze znacznie dłuższego okresu,  niż ustawić za mało  i a w szczególności niż w ogóle nic nie ustawiać -  bo tak wam mówiłem Deployment bez ustawiony requestów naprawdę może  kiepsko działać i wydatku jeszcze wpływać negatywnie na pozostałe serwisy.  więc nie Bójcie się ustawić za dużo,  bo Potem w trakcie  trwa życia waszych serwisów po prostu to zaktualizujecie  po pewnym czasie.

  

Teraz w takim razie zróbmy ten test jeszcze raz,  ale zanim go puścimy to przygotujmy sobie kawałek komendy którą uruchomimy żeby sprawdzić nasze Pody: 

  

```bash

kubectl top pods --namespace=mrybinski | grep credit-scoring-api

```

  

Tą komendą sprawdzimy sobie po prostu wykorzystanie naszego poda.  w związku z tym że użyliśmy `grep`-a  to niestety straciliśmy nazwy kolumn ale pierwsza wartość to jest wykorzystanie CPU a druga wartość to jest wykorzystanie memory.

  

 to co my zrobimy to po prostu uruchomimy nasze testy jeszcze raz  i Będziemy sobie spamować tą komendą `kubectl top pods`  żeby po prostu odczytywać wykorzystanie CPU. 

  

Największa wartość jaką uzyskaliśmy to X wykorzystania CPU i można to porównać z tym co jest widoczne na wykresie. 

  

Jest rozjazd jak widać. 

  

Ja generalnie osobiście korzystam z tego co raportuje `kubectl top`.  mamy wartość  wykorzysta X CPU.  tak jak wam mówiłem to oszacowanie będzie miało duży błąd więc generalnie polecam Teraz ustawić trochę większą wartość niż to co Widzicie  w tym przypadku ustawię o około 100 mili CPU więcej. 

  

```yaml

          resources:

            requests:

              memory: 130Mi

              cpu: 250m

```

  

I wdróżmy sobie nową wersję:

  

```bash

kubectl apply -f deploy/k8s/deployment.yaml

```

  

Więc powtarzając:  lepiej Ustawić za dużo niż za mało i na pewno lepiej niż nic.  później w trakcie działania waszych serwisów po prostu zredukujecie to do bardziej adekwatnej wartości. 

  

 ten pierwszy test sprawdzania CPU jest takim pierwszym testem który możecie wykonać.  natomiast później Za jakiś czas będziemy robić bardziej adekwatne testy obciążeniowe które mogą zasymulować działanie waszych serwisów rzeczywistości i z tych testów obliczeniowych wyłoni się już znacznie bardziej precyzyjny obraz co do tego ile CPU potrzebują wasze Pody.

  

Teraz Pamiętajcie proszę o tym że każda zmiana w parametrze `resources` powoduje od razu, Że wasze pody muszą być zrestartowane,  Natomiast w przypadku stosowań Deploymentu i Rolling update'u,  generalnie to nie powinno być większego wpływu na  korzystanie z waszych serwisów Tym bardziej że ustawienie CPU i Memory to jest przeważnie praca głównie na początku wdrożenia waszych serwisów  i później ewentualnych zmian kolejnej wersji więc to nie jest coś co dzieje się non stop.

  

 Natomiast Muszę was przestrzec przed jedną rzeczą. Pytanie jakie możecie sobie zadać to czy jesteśmy w stanie Jakoś to ustawić automatycznie?  Tak żebyśmy my nie musieli w to w ogóle ingerować i Działo się to po prostu automatycznie na przykład na bazie danych historycznych?  generalnie jest taki obiekt o nazwie  VerticalPodAutoscaler [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler) Którego zadaniem Krótko mówiąc jest dostosowywanie wymagań odnośnie Memory oraz CPU w sposób dynamiczny Na podstawie danych historycznych. Natomiast ja generalnie przestrzegam jeszcze przed jego używaniem dlatego że każda zmiana wartości w requestach  powoduje restart waszych podów.  i w przypadku zmiany takiej wartości muszą być zrestartowane wszystkie Pody. I  w przypadku jego stosowania No możecie zobaczyć że wasze pody są dosyć często restartowane No bo on będzie starał się cały czas dostosowywać requesty Na podstawie danych historycznych,  a one mogą być fruktujące tak no chociażby fakt taki że w ciągu dnia wasze serwisy są mocne obciążone,  a w nocy np. znacznie mniej,  i to już będzie dla niego przesłanka żeby takie requesty pozmieniać ale to będzie oznaczało restart wszystkich Podów które są.  oczywiście można go też skonfigurować w taki sposób żeby tych update wykonywał znacznie mniej,  ale generalnie Ja jestem osobiście zdania że lepiej jest po prostu ustalić te wartości raz  i je zostawić ewentualnie później jeżeli faktycznie będzie potrzeba aktualizacji bo na przykład w nowa wersja waszego API wymaga więcej CPU bo się zmienił algorytm albo więcej pamięci bo coś więcej  do kodu musi być zaczytane  i sprawdzić to po prostu przed wdrożeniem nowej wersji.  natomiast  VerticalPodAutoscaler  może być atrakcyjny niedługo,  dlatego że twórcy Kubernetesa są świadomi tego minusa związanego z VerticalPodAutoscalerem I pracują nad Nowym ficzerem który na razie jest w wersji alfa [https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#container-resize-policies](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#container-resize-policies)  Który pozwoli ustalić Fakt żeby wasz pod nie był restartowany kiedy zmienią się na nim wymagania odnośnie CPU i RAM. I dopiero wtedy jak to wejdzie ale już w faze `stable` a nie `alfa`, Polecam wam zastanowić się nad tym czy warto będzie w waszym przypadku użyć po prostu VerticalPodAutoscalera. 

  

My niedługo na testach obciążeniowych Będziemy uczyć się zdepilnowania innego auto skalara tak zwanego HorizontalPodAutoscalera,  który wykorzystywany jest troszeczkę w innym celu.  ale zanim do niego przejdziemy to jeszcze musimy omówić jedną rzecz.

  

 do tej pory omawialiśmy kwestię związaną z ustawianiem wartości w `requestach`. Natomiast, Tak jak nawet tutaj można zauważyć tego dokumentacji jest jeszcze parametr `Limits` i o tym teraz chciałbym powiedzieć.

  

 tak jak powiedziałem wcześniej ustawienie `resources` jest bardzo ważne  bo dzięki temu kubernetes wie ile zasobów potrzebują wasze Pody  i jest w stanie oszacować czy Pody zmieszczą się na tych Node’ach które są teraz dostępne,  czy może wymagane będzie dostawienie nowych Nodeach.  w przypadku tej operacji,  czyli zdecydowanie gdzie tego poda postawić  kubernetes patrzy na wartości określone w parametrze `requests`.  Innymi słowy request można też potraktować jako minimalne wymagane zasoby dla naszego Poda.

  

 natomiast oprócz `reqeusts` możemy ustawić `limits`,  które oznacza maksymalne zasoby jakie pod może wykorzystać  podczas swojego działania.

  

 domyślną praktyką z którą się spotkacie na co dzień i jest to żeby ustalić limity na dwa razy więcej niż jest to określone w requestach.  ustawmy tak sobie na razie wartości w naszym manifeście:

  

```yaml

          resources:

            requests:

              memory: 130Mi

              cpu: 250m

            limits:

              memory: 260Mi

              cpu: 500m

```

  

Czyli  co te wartości oznaczają:

  

1.  w przypadku memory,  mówimy kubernesowi  że potrzebujemy minimalnie 130 mebibajtów na działanie naszego Poda,  ale pozwalamy mu na to że może skorzystać maksymalnie z 260 mebibajtów  pod warunkiem że są one dostępne na naszej maszynce wirtualnej.  to jest bardzo ważne.  request mówi nam o  Gwarantowanych minimalnych zasobach,  natomiast limits  mówią o maksymalnym, możliwym  wykorzystaniu.  chodzi o to że  na danym noudzie może być zdepilowanych wiele różnych powodów,   i Node  na którym działa wasz Pot  jest tak upchany innymi Podami  że generalnie nie ma wolnej pamięci RAM.  w takiej sytuacji to nie jest zagwarantowane że wasz płot będzie mógł skorzystać więcej niż 130 mebibajtów,  bo po prostu nie ma wolnych zasobów.  Co się stanie jeżeli w przypadku pamięci RAM  pod sięgnie limitów  i będzie chciał wykorzystać znacznie więcej niż jest to tutaj zdefiniowane?  w takiej sytuacji  po prostu kubernetes zrestartuje waszego poda.  i będziecie mieli ten sam wykres który pokazywała wam w przypadku omawiania tematu memory leaka , że  wykorzystanie pamięci dochodziło do pewnego poziomu Potem nagle widzieliście nagły spadek w dół to jest właśnie ten moment kiedy pod siegnął limitów i kubernetes go zrestartował.
    
2.  Natomiast w przypadku CPU jest zupełnie inaczej.  zasada działania requests i limits jest taka sama.  czyli request to jest tak gwarantowane minimalne CPU,  a limit to możliwe maksymalne CPU.  natomiast Co się stanie jeśli wasz pod będzie chciał wykorzystać więcej CPU niż jest to określone w limitach?  kubernetes nie zrestartuje waszego poda takie sytuacje,  on dalej będzie działał  tylko że będzie działał wolniej.  to znaczy że kubernetes będzie maksymalnie,  Jeżeli jest to możliwe,  udostępniał waszą Podowi tyle CPU ile jest określone w limitach,   i działania waszego Poda po prostu zwolni.  Czyli po prostu możecie zobaczyć Znacznie większy czas odpowiedzi z waszego serwisu niż to było wcześniej.  Jeżeli spojrzycie w takiej sytuacji na wykorzystanie CPU i ona będzie bardzo blisko limitów  określonych w manifeście  to macie odpowiedź -  wasze Pody potrzebują więcej CPU  niż jest to określone w limitach  w efekcie czego requesty są wolniej przetwarzane.  i rozwiązania takie sytuacje będzie po prostu zwiększenie zasobów waszemu Podowi.
    

  

Natomiast powiem szczerze że z tymi requestami limitami jest troszeczkę więcej zabawy,  Ale to chciałbym zostawić wam jako praca domowa,  bo nie chciałbym żeby po prostu ten temat był przeze mnie wałkowany przez cały zjazd i żebyś mi czas jeszcze na inne rzeczy.  Chciałem omówić z wami te  `resources` w  najważniejszym zakresie natomiast To co chciałbym wam  polecić to poczytania w domu i obejrzenia filmiku odnośnie ustawiania `resources`. Chciałbym żebyś się przytulić sobie w wolnej chwili tak zwanym Quality of Service [https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/) a potem  obejrzeli sobie filmik z konferencji poświęconej Kubernetesowi czyli KubeCon  odnośnie Obserwacji działania Klastra W momencie kiedy powody mają ustawione różne limity requesty. Filmik o ciekawej nazwie: How  to Blow up a Kubernetes Cluster [https://www.youtube.com/watch?v=rjSWVeAvb24](https://www.youtube.com/watch?v=rjSWVeAvb24) . Na tym filmiku też na końcu poznacie pewną propozycję Jak lepiej ustawić requesty limity żeby po prostu wykorzystać  działanie kubernetesa w jak najlepszy sposób  oraz jak nie popsuć nie popsuć innych serwisów  które działają na tym klastrze. 

  

## GPU

  

Na koniec jeszcze  dodatkowa rzecz która w naszym kontekście Mlowym jest istotna,  to GPU.  w tematach  resources  zawsze by cię spotykać w sumie z właśnie CPU i memory,  natomiast rzadko kiedy wspomniane GPU.  my na naszych nodeach GPU nie mamy, Bo to jest za drogi biznes  jak na takie zajęcia,  natomiast Chciałem wam pokazać jak w praktyce skorzystać z node’y w którym mam GPU.  czyli Innymi słowy jak w podzię można określić fakt że potrzebujemy GPU do naszego działania.  jak spożycie sobie na dokumentację kubernetesa  [https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/) To  tą dokumentacja kubernetesa podaje taki przykład z wykorzystaniem właśnie `resources`  i w nim możemy określić że potrzebujemy GPU,  natomiast powiem szczerze,  że z tym podejściem się rzadko spotkacie ponieważ to wymaga dedykowanych sterowników na klastrze które będą rejestrowały każdego nouda który ma GPU  i w dodatku będą w stanie interpretować takie customowe ustawienie w limitach że potrzebujecie GPU. 

  

Natomiast w praktyce spotkacie się ze znacznie prostszym podejściem,  to znaczy  przeważnie Będzie tak że Note które zostaną stworzona klastrze które mają w sobie karty graficzne GPU one będą odpowiednio oznakowane. Znaczy to się nie dzieje  automatycznie to po prostu twórca klastra musi tak zdefiniować żeby takie node’y miały lebelki na sobie,  Ale generalnie bez praktyce jest takie żeby właśnie no z GPU określa za pomocą dodatkowych etykiet czyli labelek,  i wtedy w  naszym diplomacie po prostu korzystamy z parametru który określa na którym nodzie Mają się pojawić nasze Pody. W tym celu skorzystamy z parametru `nodeSelector`:

  

```yaml

    spec:

      terminationGracePeriodSeconds: 60

      nodeSelector:

        gpu-type: nvidia-a100

```

  

Parametrze `nodeSelector` po prostu podajemy  labelki i ich wartości które zostały określone na Nodeach.   Załóżmy że na przykład nondy które mają GPU mają po prostu Label labelkę o nazwie `gpu-type`,  która  przyjmuje różne wartości w zależności od tego jaki to jest typ gpu. I wtedy prosty sposób po prostu można powiedzieć kubernesowi żeby nasze pody zostały wdrożone na nouda który ma te labelki. 

  

 jest też parametr o nazwie nodeName;

  

```yaml

    spec:

      terminationGracePeriodSeconds: 60

      nodeName: jakis-node

```

  

Ale przestrzegam przed jego używaniem dlatego że nudy w chmurze mają automatycznie nadawane nazwy,   i one za każdym razem się zmieniają więc użycie tego parametru po prostu jest bezsensowne bo no który teraz działa Działa Teraz ale za godzinę może go nie być i jesteś inny o zupełnie innej nazwie i teraz będą cały czas się zmieniać. 

  

Okej słuchajcie to tyle jeśli chodzi o określenie `resources`na naszych Deploymencie. Czy masz jakieś pytania zanim przejdziemy do ćwiczeń?

  
  
**