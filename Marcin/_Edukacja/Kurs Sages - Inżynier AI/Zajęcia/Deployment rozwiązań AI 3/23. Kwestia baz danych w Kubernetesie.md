# Kwestia baz danych w Kubernetesie

W PONIŻSZYM YAMLU ZMIEŃ IP POSTGRESA W ZMIENNEJ ŚRODOWISKOWEJ NA PRAWIDŁOWY, POTEM URUCHOM JOBA ABY WYPELNIĆ BAZE POSTGRESA SCHEMATAMI! 

  

I NASTĘPNIE USUŃ TEGO JOBA Z GUI W GOOGLE CONSOLE ZEBY NIE BYL WIDOCZNY

  

```yaml

apiVersion: v1

kind: Namespace

metadata:

  name: mrybinski

---

apiVersion: v1

kind: ServiceAccount

metadata:

  annotations:

    iam.gke.io/gcp-service-account: deployment-access-gcp@sotrender-rd.iam.gserviceaccount.com

  name: api-access-gcp

  namespace: mrybinski

---

apiVersion: v1

kind: ConfigMap

metadata:

  name: init-postgres-db-with-tables

  namespace: mrybinski

data:

  init-postgres-db-with-tables.sql: |

    CREATE TABLE decisions (

        id SERIAL PRIMARY KEY,

        timestamp TIMESTAMP default current_timestamp,

        installment_rate_in_percentage_of_disposable_income FLOAT,

        age_in_years SMALLINT,

        foreign_worker VARCHAR(3),

        present_employment_since VARCHAR(20),

        personal_status_and_sex VARCHAR(34),

        decision VARCHAR(7)

    );

    CREATE TABLE probabilities (

      id SERIAL PRIMARY KEY,

      timestamp TIMESTAMP default current_timestamp,

      installment_rate_in_percentage_of_disposable_income FLOAT,

      age_in_years SMALLINT,

      foreign_worker VARCHAR(3),

      present_employment_since VARCHAR(20),

      personal_status_and_sex VARCHAR(34),

      probability FLOAT

    );

    CREATE TABLE risk_categories (

      id SERIAL PRIMARY KEY,

      timestamp TIMESTAMP default current_timestamp,

      installment_rate_in_percentage_of_disposable_income FLOAT,

      age_in_years SMALLINT,

      foreign_worker VARCHAR(3),

      present_employment_since VARCHAR(20),

      personal_status_and_sex VARCHAR(34),

      risk_category VARCHAR(14)

    );

    CREATE TABLE texts (

      id SERIAL PRIMARY KEY,

      timestamp TIMESTAMP default current_timestamp,

      text VARCHAR(50),

      n_generated_texts SMALLINT,

      generated_texts TEXT[]

    );

  

    CREATE TABLE images (

      id SERIAL PRIMARY KEY,

      timestamp TIMESTAMP default current_timestamp,

      image_url TEXT,

      detected_objects JSON

    );

  

---

apiVersion: batch/v1

kind: Job

metadata:

  name: init-postgres-db

  namespace: mrybinski

spec:

  template:

    spec:

      serviceAccountName: api-access-gcp

      volumes:

        - name: init-postgres-db-with-tables

          configMap:

            name: init-postgres-db-with-tables

      containers:

      - name: postgres

        image: postgres:15.4

        command: [ "/bin/bash", "-c", "--" ]

        args: ["PGPASSWORD=12345 psql -f /tmp/init-postgres-db-with-tables.sql -h $POSTGRES_IP -p 5432 -U monitoring -d monitoring"]

        env:

          - name: POSTGRES_IP

            value: 172.28.177.3

        volumeMounts:

          - mountPath: /tmp

            name: init-postgres-db-with-tables

      restartPolicy: Never

```

  

Kolejny tematem jakiejś Chciałbym z nią poruszyć to jest kwestia baz danych na kubernetesie.  A jak dobrze pamiętacie my do tej pory cały czas używaliśmy redisa oraz postgresa razem z naszym API,  natomiast cały Czas bazujemy na kontenerze który  ma w sobie zakomendowaną implementację interakcji z redisem i postgresem,  specjalnie po prostu żeby ten temat przerobić później.

  

 Wróćmy sobie na chwileczkę dokumentacji o naszym Podzie i Chciałbym pokazać jedną rzecz,  którą specjalnie pominąłem i zostawiłem ją na ten moment. Spójrzcie proszę na to  jakiego typu jest parametr `containers`  jak widzicie jest to lista kontenerów.  i jak spojrzymy sobie na nasz manifest związany z deeplaymentem, to  widzimy że faktycznie jest to lista.  co to oznacza?  oznacza to że w danym poncie może być więcej niż jeden kontakt.  czyli nasza podstawowa jednostka diploymentowa w kubernetesie możemy mieć w sobie więcej komponentów niż tylko pojedynczy kontener.  można to troszeczkę odnieść poprzez analogię do docker compose,  Czyli mamy kontenery zamknięte w ramach jednego Poda które komunikują się ze sobą po wspólnej sieci. Wspólność oznacza to że te body widzą siebie nawzajem po localhoscie,  tak jest to rozwiązane w Kubernetesie.  w takim razie zmodyfikujmy sobie nasz deployment żeby znalazły się w nim trzy kontenery,  A nasz kontener z API był już tą wersją która wchodzi w interakcje z bazami danych. Ale zmieńmy proszę na chwilę liczbę replik na trzy:

  

```yaml

  replicas: 3

```

  

```yaml

    spec:

      containers:

        - name: api

          image: europe-central2-docker.pkg.dev/sotrender-rd/mrybinski-live-coding-api/api:with-services

          ports:

            - containerPort: 8080

              name: predictions

          readinessProbe:

            periodSeconds: 5

            timeoutSeconds: 5

            failureThreshold: 3

            successThreshold: 1

            httpGet:

              port: 8080

              path: /

        - name: redis

          image: europe-central2-docker.pkg.dev/sotrender-rd/mrybinski-live-coding-api/redis:latest

          ports:

            - containerPort: 6379

        - name: postgres

          image: europe-central2-docker.pkg.dev/sotrender-rd/mrybinski-live-coding-api/postgres:latest

          ports:

            - containerPort: 5432

          env:

            - name: POSTGRES_HOST_AUTH_METHOD

              value: trust

```

  

Chciałbym też żebyśmy ustawili sobie zmienną środowiskową `POSTGRES_HOST_AUTH_METHOD`  w Postgresie na wartość `trust`,  ale tylko na chwilę to jest zmiana środowiskowa która pozwala na logowanie się do postgresa bez hasła.

  

Wróżmy sobie nasz nowy deployment:

  

```bash

kubectl apply -f deploy/k8s/deployment.yaml

```

  

 i Spójrzmy sobie proszę do konsoli googlowskiej. poczekajmy aż się nowe wersje API wdrożą. 

  

Wejdźmy sobie do jakiegoś poda I możemy właśnie zobaczyć że mamy trzy teraz kontenery w jednym wodzie.  widać że nasze API już jest zielone Czyli działa bo wcześniej za każdym razem po prostu nam się wywalało bo nie było żadnych baz dostępnych.  możemy się jeszcze upewnić że faktycznie wszystko jest okej po prostu patrząc vlogi i widząc że nasze API połączyło się z postgres i redis. 

  

Czy jak widzicie udało nam się postawić dodatkowo po Skręcaj redisa po prostu dorzucałem do naszego poda nasze obrazy z naszymi bazami danych. 

  

tylko czy tak faktycznie jest to robione w praktyce? To o czym chciałbym z wami teraz by się porównować to jest kwestia deploymentu baz danych na Kubernetesie  i jakie ryzyka są z tym związane i dlaczego lepiej tego nie robić i korzystać z dedykowanych usług które są w chmurze.  

  

Dlaczego chcę o tym wspomnieć?  No bo jak widzicie Korzystając z kontenerów tak naprawdę możemy  wdrożyć różne usługi bardzo łatwo po prostu ściągamy kontener który już ma zaimpletowaną bazę danych w sobie możemy troszeczkę strujkować tak jak my to zrobiliśmy w przypadku swoich dockerfile’i dla redisa i postgresa  i po prostu wystarczy je wylistować w manifeście Kubernetesowym i bazy są już dostępne u nas na klastrze.  więc generalnie wrzucenie bazy danych która jest w kontenerze na kuberates jest naprawdę bardzo proste i może być kuszące żeby to zrobić.  Tym bardziej że na przykład w trakcie developmentu możemy używać docker componza  żeby sobie takie właśnie bazy lokalnie stawiać i na przykład testować czy nasze API działa  Czy to ręcznie  czy właśnie w ramach testów integracyjnych.  my akurat testy integracyjne napisaliśmy Korzystając z mokowania,  natomiast można również je tak wam wspominałem wcześniej oprzeć w zupełności o kontenery i nie używać w ogóle mocowania tylko w trakcie inicjalizacji połączenia do bazy wskazywać po prostu tą bazę lokalną która działa jako kontener i w taki sposób też można wykonać testy integracyjne. więc  więc skoro tak łatwo jest to diplować,  skoro lokalnie możemy  sobie takie bazy uruchomi ten przykład właśnie korzystałem tylko w oboza to pytanie powstaje To czemu byś przy okazji nie skorzystać tutaj z kuborytesa  i takie bazy sobie postawić w naszych podach po prostu żeby też mogły z tego korzystać.  Jak  Widzicie ja tutaj nie musiałem nic w kodzie zmieniać,  po prostu  dorzuciłem obrazy do naszego poda.

 dlatego postawienie sobie pytania czy jest sens stawianie baz danych na kuberatesie jest jak najbardziej zasadne i chciałbym to z wami teraz mówić. 

  

Dobra to zacznijmy od pierwszej rzeczy:  Spójrzcie proszę że my mamy trzy pody,  w każdym Podzie mamy oddzielną dedykowaną bazę danych Postgresa i Redisa, one nie są w żaden sposób powiązane. Czyli generalnie macie wyzwanie związane z danymi przechowywanymi w trzech różnych miejscach,  w trzech różnych podach. tych podów może być dziesiątki, jak nie setki.  czyli gdybyście chcieli zebrać dane ze wszystkich podów musielibyście wymyślić sobie mechanizm  który by pobierał wszystkie dane ze wszystkich podów, zebrać je w całośći dopiero na bazie takiej zebranej bazie danych moglibyście dalej wykonywać swoje analizy. W dodatku też pytanie powstaje czy my potrzebujemy tyle różnych baz danych Ile jest instancji naszego API?  No raczej chcielibyśmy mieć jedną bazę danych tak naprawdę w której to wszystko jest przechowywane żebyśmy nie musieli Właśnie borykać się z pobieraniem danych z wielu Podów naraz.

  

Zatem rozwiązanie takiego problemu Może być to żeby w ogóle nie wrzucać do naszego poda z API pozostałych kontenerów z  Postgresem i Redisem. Po prostu niech posgres i Redis będzie oddzielny diploymentem w naszym klastrze.  i na przykład tam umieścimy tylko jednego Poda. 

  

Pomysł oddzielny dyplementem jest jak najbardziej w porządku,  natomiast pomysł w tym jednym powodem bo wtedy mielibyśmy tylko jedną bazę jest niewystarczający.  znaczy on się sprawdzi przy małej skali,  natomiast przy większej skali kiedy odczytów z baz danych z zapisów będzie na tyle dużo że po prostu zobaczycie opóźnienie w waszej operacjach.  zatem,  z racji tego że korzystamy z Kubernetesa,  stworzonego z myślą o tym żeby skalować nasze rozwiązania w zależności od ruchu,  można w takim razie po prostu zwiększyć ilość podów naszych deploymentów z bazami danych. Tylko że zwiększając liczbę Podów  mamy wciąż ten sam problem o którym mówiłem wcześniej czyli mamy wiele instancji tej samej bazy danych więc potrzebowalibyśmy jakiegoś sposobu na to aby te dane ze sobą zagregować do jednego miejsca i odpytywać dopiero tak zebraną do kupy bazy danych. Słuchajcie nie chcę wnikać szczegóły takich rozwiązań bo my generalnie nie będziemy na tym zdjęciu takich rzeczy implementować ale oże to rozwiązać na bazie tak zwanego mastera I workerów,  gdzie Master po prostu opowiada  właśnie za zarządzanie i zapisywaniem takich danych do jednego miejsca, a z kolei workery mają replikę tej bazy danych i generalnie one są skalowane w góre, jest ich więcej niż jeden i ich zadaniem jest obsługa ruchu związane z odczytem bazy danych. 

  

 okej więc jakby ten problem można jakoś rozwiązać i są  w tym celu stworzone dedykowane obrazy które właśnie takich masterów i workerów mają zaimplementowanych też jest dokumentacja Jak można postawić na klastrze. 

  

 jest jeszcze inny problem o którym musicie wiedzieć.  pamiętajcie  o tym że wasze pody mogą w każdej chwili zniknąć.   ich zniknięcie może być powiązane Bo na przykład z Update waszego diplomentu tak,  kiedy stare Pody są usuwane i zastępowane nowymi Podami.  Może to też się dziać w takiej sytuacji,  kiedy wasz klaster ma wiele różnych maszynek wirtualnych  i jest ustawiony  tak żeby maksymalnie jak najwięcej powodów umieszczał na jak najmniejszej liczbie maszynek,  po to aby zaoszczędzić pieniądze dla was.  więc on będzie wykonywał ruchy jeżeli będzie widział że jest jakaś maszynka wirtualna  z której można by było przenieść to co jest na niej wdrożone  na inną maszynkę wirtualną i dzięki temu ta maszynka wirtualna może zniknąć,  oszczędzając Wam kosztów,  to kubernetes może zdecydować właśnie o tym żeby te pody które na nie działają Po prostu przenieść je na drugą.  w efekcie  czego wasze powody po prostu staną zabite i postawione nowe.   a Utrata podów może dla was oznaczać utratę danych.  czyli trzeba się jakoś zabezpieczyć przed tym  i mieć pewność że wasze dane nie znikną w momencie właśnie takich różnych operacji które dzieją się na klastrze.  Generalnie rzecz biorąc można utworzyć dyski które są dostępne dla modów tak zwane PersistentVolume,  My niestety nie posiedzimy na to czasu na tym zjeździe ale to jest koncert bardzo prosty i generalnie w wolnej chwili na bardzo łatwo to ogarniecie bo dzisiejszych zajęciach,  natomiast  z dyskami na  klastrze też jest wyzwanie bo  one są dynamicznie alokowane  dla naszych Podów.  oznacza to,  że wraz z powstaniem nowego poda  może powstać nowy dysk  dla niego.  a wraz z jego zniknięciem ten dysk może albo zniknąć albo jeszcze zostać.  A jak zostanie to jest wyzwanie żeby ten istniejący dysk podpiąć do nowego poda zamiast tworzyć Nowy dysk.  żeby to było osiągalne jest też dedykowane obiekt w kuberatesie który nazywa się StatefullSet,  który właśnie taką  dynamiczną logikę podpinania dysków do podów ma zaimplementowaną  i jeszcze on  on ma też zaimpetą logikę że w sytuacji kiedy nasz pod na przykład się zmieni,  to żeby do tego nowego poda podpiąć stary dysk.  w tej twórca że też dzisiaj nie umówimy bo na to nie ma przestrzeni Natomiast on jest bardzo podobny do Deploymentu i też spokojnie sobie z tym poradzicie  Jeżeli ktoś z was będzie chciał się zmierzyć z wdrażaniem baz danych na klastrze.

  

Teraz po co to wszystko wam mówię? Tak jak wcześniej powiedziałem to jest bardzo kuszące żeby postawić sobie bazę danych na klastrze bo to jest bardzo proste wystarczy wskazać konkretny obraz w manifeście I jak widzimy działa,  natomiast Szereg problemów z jakimi będziecie się mierzyć powoduje to że nie jest to rekomendowane. A te problemy które wymieniłem i ich jest dopiero kilka bo jeszcze jest ich znacznie więcej. Krótko mówiąc: praca Jaką będziecie musieli włożyć wokół tego żeby wasza baza danych działała na klastrze i żeby zniwelować ryzyko utraty danych  jest naprawdę ogromna.  tak jak powiedziałem na pewno w internecie znajdziecie informacje o tym jak można Faktycznie postawić sobie bazę danych na klastrze,  są do tego przygotowane Obrazy i dokumentacja tutoriale ale i tak i tak wciąż będziecie musieli pamiętać o tym że trzeba to bazą danych mocno zarządzać  i wciąż nawet Korzystając z tych gotowców będziecie mieli bardzo dużo pracy.  więc generalnie nie rekomendowany jest to żeby deployować bazę danych na klastrach Kubernetes. Podstawową przyczyną tych problemów o których pisałem jest fakt, Że właśnie te pody  które są podstawą jednostką implementową kubernetesie po prostu bardzo często znikają,  restartują się,  bądź aktualizują się na nową wersję.  czyli ich cykl życia  może być Dosyć krótki.  dlatego najłatwiej na kuberatesie depilować aplikację/ serwisy  które są z definicji tak zwane `stateless`  czyli bezstanowe, Czyli takie aplikacje które nie przechowują w sobie żadnych danych, nie zależą od stanu poprzedniego i generalnie to co zostanie zainicjowane w ramach kontenera jest w zupełności wystarczające do jej działania.  nasze API to modelu jest taką aplikacją,  i to że ona będzie restartowana,  aktualizowana,  usuwana i tak dalej  to generalnie nie wpływa na jej funkcjonowanie, W takim sensie że jakiś pod zniknie ale następny który powstanie po prostu będzie działał bez problemu  i bez potrzeby pobierania jakiegoś stanu poprzedniego tak to mają właśnie bazy danych.

  

Jeżeli natomiast ktoś z was będzie chciał deployować bazy danych na klastrze  to są oczywiście odpowiednie obiekty, które Was w tym wspomogą,  ale generalnie  utrzymanie tego będzie wyzwaniem.  Jaka jest rekomendacja z mojej strony?  w przypadku baz danych Skorzystajcie po prostu z dedykowanych usług które są w chmurze żeby taką baze danych stworzyć. Dzięki temu jakby cały proces zarządzania danymi oddajecie do dostarczenia chmury i generalnie takie jest rekomendowane podejście  w przypadku baz danych.

  

Jeszcze zanim pójdziemy dalej to może wspomnę o tej wielokrotnej liczbie kontenerów w podach jak tu widzicie.  My zaraz te kontenery z redisem i postgresem usuniemy,  natomiast jedno słowo komentarza odnośnie tego czy powszechne jest używanie wielu kontenerów w Podzie?  To zależy od aplikacji,  Natomiast generalnie większość  diploymentów z którymi się spotkacie to przeważnie jeden kontener w jednym podzie właśnie z tą główną aplikacją.  wszelkie dodatkowe kontenery które mogą się pojawić w Podzie są kontenerami pomocniczymi,  i takim standardowym przykładem dodawania dodatkowego kontenera do Poda jestem przykład autoryzacja z chmurą poprzez jakiś proxy.  przykład:  na pewno w pracy spotkacie się z taką technologią grafana jest to usługa do  tworzenia  dashboardów  i monitoringu waszych serwisów  i na przykład wraz z grafaną w przypadku Clouda  rekomendowane jest to że w podzie w którym grafana jest,  jest też kontener Proxy który po prostu autoryzuje grafane z Google i dzięki temu Grafana  może korzystać bez problemu z metryk które sam Google Cloud udostępnia.

  

Ja na nasze potrzeby utworzyłem nam bazę danych Redisa oraz Postgresa  które można odnaleźć tutaj.  redisa znajdziemy pod aplikacją o nazwie Memorystore  i wybierzmy Redis.

A z kolei po zgresa znajdziemy pod usługą SQL i mamy naszego Postgresa.  Utworzenie baz danych jest naprawdę bardzo prostą rzeczą i to jest do wyklikania i nie będziemy przez to przechodzić bo z tym sobie bez poradzicie  niezależnie od tego czy jest to Google Cloud, AWS czy Azure.

**