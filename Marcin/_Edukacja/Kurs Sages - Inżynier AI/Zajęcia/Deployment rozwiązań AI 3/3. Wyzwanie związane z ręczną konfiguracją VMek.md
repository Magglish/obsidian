# Wyzwanie związane z ręczną konfiguracją VMek

**

Ostatni nasz zjazd zakończyliśmy taki bardzo prostym diploymentem naszego API opakowanego w kontener Korzystając z prostej wirtualnej maszynki na chmurze.   maszynka została stworzona,  w trakcie tworzenia trzeba było określić tylko dodatkową rzecz jeszcze związaną z tym żeby  jeden z portów był otwarty,  uruchamialiśmy kontener wskazujący właśnie otwarty port tak żeby ruch z zewnątrz Mógł przychodzić.  skorzystaliśmy też z drugiej dedykowanej usługi Vertex Ai które jak widzieliście tam było nawet jeszcze prostsze bo generalnie można było wszystko wyklikać.  tak jak Wam powiedziałem Na poprzednim zjeździe tak jak wdrożenie na przykład na tą VMkę Można zastosować jako pierwsze wdrożenie, pierwsza iteracja, Która pozwoli wam już zabrać pierwszy feedback. Tak jak podczas wytwarzania softwaru mamy podejście iteracyjne,  dostarczamy pewne pierwsze MVP, Wczoraj już może generować jakiś przychód,  ale też przy okazji dużo informacji co dalej poprawiać.  tak samo mamy podejście iteracyjne w przypadku budowy modeli,  pierwsze modele są proste które już mogą przynosić pewne uzyski i dalej jak on działa może brać wyjazd kolejnej literacji.  takie same podejście można zastosować w przypadku wdrażania czyli zacząć od czegoś prostego, To da wam już szybki feedback w różnych wariantach:  przede wszystkim pierwsza informacja o czasie odpowiedzi waszego serwisu  kiedy działa już w chmurze,  które Załóżmy że jest docelowym naszym miejscem deploymentu,  o zasobach jakie będą wymagane (CPU, RAM) Na działania waszego modelu co od razu pozwoli wam oszacować mniej więcej koszty takiego rozwiązania w zależności od tego ile takich VMek musiałoby powstać żeby obsłużyć uch. Tak samo też inne zespoły które również mogą być zaangażowane w wasz projekt i wspólnie rozwijać jego iteracyjnie mogą już powoli  implementować po swojej stronie odpytywanie waszego API  i  zobaczyć jak To wszystko będzie działać, Dać wam już pierwszy Feedback odnośnie chociażby tego  Jaki output najlepszy byłby z naszego API żeby można było to dalej przetwarzać,  czy Może wasza Dokumentacja jest niejasna i nie do końca  zrozumiałe są wszelkie End pointy które stworzyliście.  generalnie mówiąc Krótko z takiego pierwszego prostego może naprawdę uzyskać wiele cennych informacji które pozwolą wam jeszcze lepiej wasze API dostosować, zmodyfikować Zanim faktycznie Wasze API zostanie używane na dużą skalę. Także do PoCa Takie podejście  jest słuszne może przynieść naprawdę bardzo dużo cennych informacji Dla was dla deweloperów.

  

Ale jeżeli chcielibyśmy iść z tym już “na produkcję” to znaczy  zostać przy tym podejściu nawet wtedy kiedy wasze API już jest używane na pełną skalę, już jest stworzony backend, frontend wokół pewnego produktu, który rozwijacie, który opiera się o wasze API i to już działa  na co dzień,  to takie stawianie na VMkach jak tutaj robiliśmy na poziomie zjeździe  jest dosyć sporym wyzwaniem pod względem zarządzania takimi wirtualnymi maszynkami. A przykładowe problemy z jakimi Moglibyście się mierzyć:

  

1.  co jeżeli Z jakich powodów kontener przestanie działać?  dajmy na to pojawi się błąd w naszym API i to od razu powoduje to że kontenery  zamyka się. Więc w jaki sposób musielibyśmy zadbać o jego restartowanie w takiej sytuacji. Akurat to jest np. proste do załatwienia, Dlatego że podczas uruchamiania kontenera to o tym nie wspomniałem na poprzednim zjeździe Można ustawić tak zwane restart-policy  które określa  Jakie warunki muszą zajść żeby nasz kontener został zrestartowany No nie Można tam ustawić wariant że jeżeli będzie błąd  w kontenerze to zrestartuj kondole. 
    
2.  co jeżeli zabraknie zasobów Na wirtualnej maszynce  RAM i CPU? Bo na przykład ktoś jeszcze inne kontenery deployuje na naszej VMce. Zatem potrzebny był by jakiś monitoring  wykorzystywania zasobów tej VMki przez pewne jej procesy  pewnie niesie że każdy proces on odpowiednio ze sobą.
    
3.  A co jeżeli wirtualna maszynka  Bez jakiegoś powodu padnie lub  zrestartuję się?  Zatem trzeba mieć mechanizm który odtworzy wszystkie procesy które tam zachodziły,  Czyli wszelkie kontenery które tam były muszą zostać odtworzone z powrotem
    
4.  co jeżeli jedna wirtualna maszynka to za mało żeby obsłużyć ruch?  to wtedy trzeba robić jakieś mechanizm który no pozwoli wam zostawić kolejną daną maszynkę jeżeli naprawdę jedna maszynka sobie nie daje rady.
    
5.  okej Ale jeżeli dostarpimy sobie kolejną maszynkę  to Zobaczcie że wtedy powstanie nowy IP takiej maszynki.  a przypomnijmy sobie jak odpytywaliśmy naszego requesta w `send_example_request.py`. Wskazywaliśmy bezpośrednio IP naszej maszynki.  To w takim razie w sytuacji kolejne maszynki wirtualnej,  o nowym IP,  musielibyśmy co zaktualizować nasz kod żeby odpytać drugą maszynkę.  A co jeżeli tych maszynek jest więcej niż dwie tylko 10,  i chcielibyśmy odpytywać inne maszynki bo one są mniej obciążone to czy to by oznaczało żeby za każdym razem w kodzie zmieniać IPka? do tego jeszcze dodam Fakt że jak wirtualna maszynka się zrestartuje to nie macie gwarancji że dostaniecie ten sam IP dla wirtualnej maszynki. Więc w takiej sytuacji trzeba by było zaimplementować pewną usługę która zarządza tymi ipkami za nas tak naprawdę.  jakiś serwer który będzie miał stałe IP,  i te IP będzie w naszym kodzie a natomiast on będzie miał logikę rozdysponowania tych requestów do oddzielnych maszynek -  może ktoś was słyszał o pojęciu load balancing, albo o web serverze np. nginx. To rzeczy też będziemy omawiać później naszym zjeździe. 
    
6.  okej A co jeżeli w danym momencie mamy 5 wirtualnych maszynek gotowy do tego żeby przyjmować requestr naszych klientów serwisów innych,  No ale okazuje się że ruch jest tak mały że w sumie wystarczająca będzie tylko 1 maszynka. Pozostałe cztery to jest za dużo,  one działają i tylko nam generują koszty.  Zatem chcielibyśmy żeby te wirtualne maszynki się zatrzymały,  ale na przykład dopiero uruchomiły wtedy kiedy widzimy że ruch do naszego sapi jest znacznie większy i Wymagane są kolejna instancje żebyśmy ten obsłużyli.
    

  

Takich case’ów Z zarządzaniem takiej infrastruktury opartej właśnie o takie proste wyjąki jest bardzo bardzo dużo jeszcze i można tego mnożyć cały czas. Generalnie rzecz biorąc wniosek z tego jest jeden,  pracy jest bardzo dużo żeby utrzymać takie rozwiązanie i oczywiście można to wszystko pewnie zakodzić od zera samemu ale nie ma co odkrywać koła na nowo i skorzystać z pewnej dedykowanych usług które pozwalają nam zarządzać taką infrastrukturą własną właśnie opartą o VMki  oraz o kontenery.  ponieważ na tej VMkach działać będą kontenery z naszymi aplikacjami,  tak to robiliśmy na ostatnim zjeździe na przykładzie jednej VMki. Jedną z takich technologii  która pozwala nam zarządzać taką infrastrukturą składającą się z wielu VMek i wielu kontenerów działających na nich Jest właśnie Kubernetes Który wspiera nas w rozwiązywaniu właśnie tych problemów które teraz przestawiłem i wiele wiele jeszcze innych.  dokładnie Czym jest kubernetes i jak nas wspiera powiem za chwileczkę.

  

Okej ale oprócz stawiania naszego rozwiązania  na VMce poszliśmy dalej to znaczy  skorzystaliśmy z takiego serwisu o nazwie Vertex AI Który jak sami nie będzie pamiętacie ale Pozwolił nam na postawienie tego naszego pierwszego kontenera dosłownie wyklikując wszelkie pewne operacje.  na początek rejestrowaliśmy sobie nasz kontener jako pewien model,  potem tworzyliśmy sobie w zakładce Online predictions Właśnie nasz serwis  który był dostępny z zewnątrz,  i mogliśmy go odpytac. W trakcie wdrażania poprzez to usługę tam też było takie opcje gdzie mogliśmy sobie określić  specyfikacja maszynki wirtualnej,  czyli ile ma mieć RAMu i ile CPU. Była też opcja którą akurat nie wtedy podkreśliłem bo nie było istotna ale teraz on nie wspomnę  ale Była tam też taka możliwość żeby ustalić na podstawie jakiej metryki Ta ilość instancji z naszym API ma rosnąć bądź maleć,  czyli To jest ta odpowiedź na nasz problem związany właśnie z skalowaniem ilości instancji zdolności od ruchu,  I tam w tym usłudze można było to po prostu ustawić.

  

 czyli Innymi słowy  w chmurze na którym będzie pracować, czy to jest GCP, AWS, Azure One wa będą dostarczać w swoje dedykowane rozwiązania które też adresują te same problemy które tam wypisałem tak,  dosłownie Out of the box dostaniecie kwestie związane z  stawianiem ponownie kontenera z API jeśli pojawi się błąd,  będą zarządzać za was VMkami jeśli się przed zrestartują,  pewno zarządzać za was ilością instancji tak  żeby mogły one obsłużyć mniejszy bądź większy ruch do serwisu, będą zarząðzać IPkami, load balancingiem do Waszych serwisów - dosłowanie dostaniecie wszystko w pakiecie. Czyli krótko mówiąc Korzystając z tych usług tak naprawdę macie cały pakiet zarządzania infrastrukturą która stoi pod spodem  w ramach tej usługi. Wy tylko skupiacie się Na tej części deweloperskiej tak Czyli zarejestrujemy sobie teraz nasz model nowy  powiemy mu żeby to wdrożył  i on nam wszystko to robi. To jest właśnie odpowiedź na te problemy które Opisałem Ale tylko kwestia jest też to żeby te  operacje i logika związane z wdrożeniem po prostu schować, I żebyście nie musieli się tym przejmować i też to ogarniać.  dlatego też takie rozwiązania serverless, jak ten Vertex AI, Jest świetny dla osób które mają  mniej doświadczenia w procesach wdrożeniowych,   albo w ogóle Całkowicie te kwestie wdrożenia tego nie interesują i to nie jest ścieżka która to się rozwijać No każdy z nas jest inny  każdego z nas interesują inne rzeczy,  i taki dla przykładu ten Vertex AI Będzie dla takiej osoby idealny bo on sobie wszystko wyklika,  i model w stanie wdrożony można go używać  i W dodatku cała Ta otoczka znalazła zarządzanie infrastruktury jest po stronie chmury. Jest to bardzo wygodne rozwiąza,  jak widzicie bardzo szybkie Bo to jest kilka kliknięć  i tak samo można tego użyć w ramach pierwszych iteracji wdrażania w ramach Proof of Concept. Teraz zaletą jeszcze takiego rozwiązania jest Nie tylko to że to jest łatwe do użycia,  i bardzo szybko się wdraża  ale też te usługi są generalnie stworzone z myślą o używaniu tego na dużą skale, Więc nawet jeżeli użylibyście tego Vertex w ramach próba w Concept  a potem chcielibyście  dalej używać tego przy większej skali, kiedy już wasze rozwiązanie będzie używane przez wszystkich i będzie firmie przynosiło to zysku,  to nie ma z tym żadnego problemu. Trzeba by oczywiście po prostu wtedy ustawić odpowiednie maszynki tak samo tą metrykę do auto skalowania żeby nas za dużo kosztem nie zjadły ale generalnie można jak najbardziej Nawet jeśli myślicie  o wdrażaniu modeli na dużą skalę i generalnie ten cały moduł jest właśnie  pomyślany w taki sposób aby cały ten proces rozwijania modeli był zrobiony w ramach jednej usługi jak to widzicie. Jest wiele różnych usług które pozwala wam uczyć modele,  feature,  potem wdrażanie itd. Jednakże musicie być świadomi dwóch istotnych rzeczy. Krótko mówiąc, ten moduł Vertex AI który Wam pokazuje, jest stworzony własnie z myślą nie tylko o rozwijaniu i uczeniu modeli ale również Skupiony jest wokół kwestii wdrożeniowej i można go użyć zarówno w ramach pierwszej iteracji w PoC  w jaki też używać go na pełną skalę. JEdnakże trzeba wziąc pod uwagę istotne rzeczy korzystając z takich usług:

  

1. Po pierwsze, za to że ten cały proces zarządzania jest po stronie Google a wy tylko i wyłącznie wyklikujecie kilka opcji i ustawiacie kilka parametrów za to wszystko płacicie znacznie więcej pieniędzy niż gdybyście chcieli zrobić to ręcznie za pomocą innych usług.  więc zawsze rozpatrujcie tutaj wariant ceny vs Jak to sobie nazywam time-to-deploy. Tutaj Time to deploy jest  bardzo krótki,  natomiast koszty mogą być bardzo duże.
    
2. Druga rzecz jest istotna taka że konfiguracja waszego wdrożenia jest zależna tylko i wyłącznie tego na co Wam pozwala ta usługa. Jak widzieliście nie ma tutaj za dużo opcji. W dodatku też ten Vertex Ai wymaga żeby wasze API było stworzone w pewien ustandaryzowany sposób dlatego nasza API teraz nie zadziałało,  Ja też nie chciałem wprowadzać modyfikacji w kodzie żeby jej całkowicie nie rozwalić,  ale generalnie trzeba dostosować je do wymagań Vertex AI i tak żeby to zadziałało.  i generalnie tak będzie tych usługach serwerless że one wymagają pewnej struktury naszego API tak żeby można było je tutaj wdrożyć.  to jest jedna rzecz.  Druga rzecz jest taka że no te opcje które mamy jest ich niewiele, Jeżeli chcielibyśmy  jeszcze bardziej skonfigurować nasze rozwiązanie mamy jakieś customowe potrzeby to niestety tutaj tego nie zrobimy.  jesteśmy ograniczeni tylko wyłączę do tego co on tutaj w usłudze pozwolili ustawić.  a tam Jeżeli ktoś myśli o was o możliwości konfigurowania dosłownie wszystkiego to musimy zejść poziom niżej, do kodów/manifestów które definiują wdrażane obiekty. Natomiast ta usługa nam to nie pozwoli.  zatem przy wyborze usług  serwer less trzeba wziąć pod uwagę fakt że możliwości konfiguracyjne są bardzo małe, Decydując na taką usługę trzeba mieć to na względzie i zapytać się czy to akceptujemy.  Jeżeli nie trzeba wybrać coś innego co pozwoli wam na większą modyfikację.
    

  

Więc generalnie rzecz biorąc podczas wdrażania waszych modeli będziecie cały czas musieli balansować pomiędzy łatwością wdrożenia na przykład właśnie Korzystając z takiej dedykowanej usługi jak i też kosztem pieniędzmi jakie musicie wydawać na to usługę oraz możliwością customizacji. Niestety tutaj nie ma żądnych Złotych reguł które można tam przekazać po prostu jest bardzo mocno zależy od tego co implementujecie,  W jakiej skali będzie działać, Jak kluczowy jest wasze rozwiązanie w całym tym potoku przychodów naszej firmy no i najważniejsza rzecz Która jest zawsze istotna to kompetencje  wasze i zespołu. Że nie czujecie się w polipas zespół po to aby robić coś bardziej niskopoziomowo customowego  to nic nie stanie nie przeszkodzie żeby właśnie skorzystać z tych usług serwerless  które widzieliście są bardzo łatwe w użytku i generalnie dostarczają wam wszystko to co potrzebujecie.

**