# Ingress

  

### Przed rozpoczęciem tej sekcji

**Teraz upewnij się, że Load Balancery z live-codingu i z exercises są usunięte i nie zajmują IPków, bo teraz musisz utworzyć External IPs dla wszytkich osób i zarejestrować je jako rekordy typu A w Cloud DNS.**

  

Okej Przechodzimy teraz do najtrudniejszego tematu jeśli chodzi o wystawienie serwisu naszego API dla dostępu z zewnątrz.  poznamy teraz sposób który jest polecany jeśli chodzi o wystawienie serwisów produkcyjnych, jednakże jest on trudny. Trudność Jest zwiększona faktem w jakim środowisku wdrażącie czy to jest chmura Google,  AWS czy Azure oraz czy wdrażacie na bare-metalu. A ten proszę proszę mi teraz na uwadze rzeczy które będę wskazywał jako rzeczy typowo googlowskie.  wskaże też Jaka jest standardowa wersja ale będzie musieli  u siebie podczas zdarzenia modeli sprawdzić dwa razy czy  pewny parametr definiuje się sposób standardowy czy może chmura wymaga czegoś innego.  skąd wynikają tę różnicę,  dlatego że każda chmura chce optymalizować pewne elementy kubernetesa pod siebie No i to też jest jeden ze sposobów żeby was zatrzymać przy tej chmurze.  bo zdecydowanie mniej chętnie zmigrujecie do innych chmury jeżeli będzie wiedzieć że będziecie musieli poświęcić dużo pracy na rekonfigurację manifestów. poznamy teraz obiekt o nazwie Ingress  który będzie odpowiedzialny za  udostępnienie naszego API na zewnątrz.  jest to on szeroko stosowany jeśli chodzi  właśnie w kontekście wystawienia API na zewnątrz Jednakże musicie być świadomi tego że Kubernetes cały czas się rozwija  i obecnie obiekty które są wykorzystywane do  udostępnienia API na zewnątrz to serwis i właśnie ingres który teraz poznamy.  

  

Wróćmy sobie na chwileczkę do poprzedniego slajdu.  Tak Wam powiedziałem, Mieliśmy Dwie istotne wady tego rozwiązania: 

  

1.  przystosowaniu load balancerów które dostarczy nam  możecie mieć zwiększone koszty dlatego, że  każde taki lot Balance który stworzony dla każdego z serwisu oddzielnie
    
2.  jesteście ograniczonej do tego co googlowski Load Balancer dostarcza,  więc w Bardziej wymagające środowiskach bardziej customowych może to być pewne ograniczenie dla was.
    

  

Sposobem na obejście tego jest zastosowanie obiektu o nazwie Ingress. Teraz to jak zdefiniujemy ingressa zależy od tego jak uda nam się te problemy rozwiązać Czy w pełni czy tylko w pewnym stopniu.  

  

 zacznijmy sobie od podstawowego pytania Czym jest ten ingres? Nic innego jak obiekt który zarządza dostępem z zewnątrz do naszych serwisów.  jak sami zobaczycie w jego manifeście który Będziemy tworzyć to generalnie jest to obiekt w którym definiujemy proste reguły,  co ma się zadziać w momencie kiedy przyjdzie nasz request.  czyli Innymi słowy do którego serwisu ma zostać wysłany request, jeżeli my coś wyślemy do ingressa. 

  

Najlepiej to się zrozumie w sytuacji kiedy po prostu napiszemy sobie pierwszej Ingress i zobaczycie  jego działania  oraz jak jego manifest.

  

Na początek musimy wrócić do naszego serwisu i zmienić jego typ na `ClusterIP`. 

  

```yaml

spec:

  type: ClusterIP

``` 

  

Teraz uwaga.  ingres którego będziemy wdrażać na Google zakłada że serwis jest typu `ClusterIP`,  Ma to związek z tym jak pewne rozwiązania sieciowe na Google działają  i jest to jeden ze sposobów ich optymalizacji.  Nie będę wnikał szczegóły,  natomiast wjedźcie że standardowe podejście  w Service  żeby działał wraz z ingresem  jest takie aby Serwis był typu NodePort  I  takie kejsy znajdziecie w dokumentacji kubernetesa Jak będziecie na przykład uczyć się dodatkowo jak taki ingres postawić to tam zobaczycie NodePort  i to jest standardowe podejście,  natomiast podkreślam,  że w  Google  serwis budujemy typu `ClusterIP`,  ze względu na to że Google troszeczkę inaczej podchodzi do zarządzania kwestiami sieciowymi  sytuacje Kiedy używamy ingressa  w środowisku Google Cloud.

  

Dodatkowo chciałbym poprosić was o to żebyśmy zdepilowali sobie inną aplikację  na naszym klastrze po to abyście mogli zobaczyć w pełni działanie Ingress a najlepiej zobaczyć to wtedy  kiedy do indressa podepniemy dwie aplikacje a nie tylko jedną.

  

Wysyłam Wam teraz manifest  na zdeployowanie przykładowej aplikacji:

  

```yaml

apiVersion: apps/v1

kind: Deployment

metadata:

  name: hello-app

  labels:

    app: hello

  namespace: mrybinski

spec:

  selector:

    matchLabels:

      app: hello

  template:

    metadata:

      labels:

        app: hello

    spec:

      containers:

      - name: hello-app

        image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0

        ports:

        - containerPort: 8080

---

apiVersion: v1

kind: Service

metadata:

  name: hello-app

  labels:

    app: hello

  namespace: mrybinski

spec:

  type: ClusterIP

  selector:

    app: hello

  ports:

  - port: 80

    targetPort: 8080

```

  

Prośba do was żebyście zmienili namespace z mojego na swój własny i po prostu dyploiowali to za pomocą

  

```bash

kubectl apply -f deploy/k8s/hello-world.yaml

```

  

Spójrzmy to wszystko jest - powinniśmy mieć deployment o nazwie `hello-app` oraz service o nazwie `hello-app`  

  

Otwórzmy sobie proszę dokumentację właśnie na Ingressie, I znowu nas przywitają cztery elementy do ustawienia Więc zacznijmy od tego:

```yaml

apiVersion:

kind:

metadata:

spec:

``` 

  

`apiVersion` to `networking.k8s.io/v1`, `kind` to po prostu `Ingress`, `metadata` jest nam dobrze znana i tutaj znowu ustawimy sobie takie same dane:

  

```yaml

apiVersion: networking.k8s.io/v1

kind: Ingress

metadata:

  name: credit-scoring-api

  namespace: mrybinski

  labels:

    phase: live-coding

    app: rest-api

``` 

  

Okej w takim razie do określenia zostało nam jeszcze `spec`. Dokumentacja wskazuje, że jest to `IngressSpec` więc przejdźmy do niego. 

  

Teraz Słuchajcie troszeczkę zmienię kolejność mowy na parametrów bo zaczęcie od defaultBackend oraz od ingressClassName będzie zbyt trudne. Zaimplementuemy najprostszego Ingressa jakiego się da.  Przejdźmy sobie do parametru `rules`.

  

Tak jak powiedziałem na początku Ingress to obiekt który implementuje w sobie zestaw reguł definiujących To w jaki sposób requesty do niego wysyłane mają być przekierowane do dalszych serwisów.  Zatem w parametrze `rules`  będziemy definiować listę  obiektów typu `IngressRule`.

  

Pominiemy sobie parametr `host` na chwileczkę,  jego użyjemy później. 

  

Dalej mamy parametr `http`  w którym to właśnie będziemy definiować  To do jakiego serwisu mają wpaść nasze requesty w zależności od tego jak zostanie odpytany nasz Ingress.  natomiast w tym parametrze chciałbym zrobić wyjątek i  omówić go już na przykładowych wartościach które, już będziecie widzieć w manifeście Tak będzie znacznie prościej.

  

Wyślij in ten manifest

  

```yaml

spec:

  rules:

    - http:

        paths:

        - pathType: Exact

          path: "/hello-app"

          backend:

            service:

              name: hello-app

              port:

                number: 80

        - pathType: Prefix

          path: "/"

          backend:

            service:

              name: credit-scoring-api

              port:

                number: 80

```

Teraz Spójrzcie proszę na konfigurację tych reguł które wam wysłałem. 

  

1.  przede wszystkim mamy ten parametr `rules`  który właśnie nam określa reguły  które decydują o tym jak daye request ma być przekierowane, do którego serwisu.  jest to lista reguł,  Na razie mamy tylko jedną regułę zdefiniowaną  zobaczycie później jak  zdefiniujemy to ale troszeczkę inny sposób który pojawi się druga reguła. 
    
2.  w ramach tej pierwszej reguły mamy  parametr ścieżki `paths`  które znów jest listą  ścieżek.
    
3.  I teraz jeżeli wyślemy requesta do ingressa to na podstawie ścieżek Czyli generalnie endpointów które wskażemy w adresie  następuje przekierowanie tego requesta do konkretnego serwisu.
    
4.  pierwszą ścieżką którą widzimy to jest reguła związana z wysłaniem  requesta do naszej aplikacji `hello-app`. wartość parametru `path` Mówi nam o tym że jeżeli wyślemy requesta do ingressa na endpoint `/hello-app`  to zostanie  przekierowany ten request do `backend`  który w tym przypadku jest naszym serwisem  identyfikowanym poprzez nazwę `name`  oraz port  do którego ma być wysłany ten request. Bardzo ważne jest też parametr tutaj `pathType` który przyjmuje wartość `Exact`,  który oznacza że ten endpoint musi być wprost określony  i musi być zgodny w stu procentach z tym co jest w parametrze `path`.
    
5. Druga reguła która macie pod spodem jest bardzo podobna w użytych parametrach ale wartości są inne. Tutaj widzimy że jeżeli przekażemy coś  do ingressa  do endpointów które po prostu zaczynają się od `/`  to zostaną przekazane do naszego serwisu z API.  teraz parametr `patType`  jest ustawiony jako `Prefix`.  prefiks można inaczej uznać jako po prostu wszystkie requesty do endpointów zaczynających się od tego co jest napisane w `path`. 
    

  

Czyli Innymi słowy  te dwie reguły mówią coś takiego.  jeżeli Wyślemy requesta do ingressa na endpoint `/hello-app`  to zostanie on przekazany to zostanie on przekazany do service’u `hello-app`.  natomiast wszystkie inne requesty  które będą trafiać do tego endpointa będą przekierowane do naszego serwisu z API. 

  

Zaraz zobaczymy sobie na konkretnym przykładzie jak to działa jak będziemy wysyłać requesty,  natomiast na ingresie trzeba jeszcze ustawić dwa nowe parametry tak żebyśmy mieli pewność że zadziała to tak jak chcemy.

  

 musimy wrócić teraz do naszych metadanych i w nich określić nowy parametr  zwany `annotations`,  który do tej pory pomijałem bo nie był potrzebny Natomiast teraz będzie wymagane do zdefiniowania:

  

```yaml

  annotations:

    kubernetes.io/ingress.class: gce

    kubernetes.io/ingress.global-static-ip-name: mrybinski TUTAJ_ZAMIEN

```

  

Chciałbym żebyście wstawili takie dwie adnotacje które oznaczają że musimy użyć klasy `gce` oraz użyć IP, które ma nazwe jak pierwsza litera waszego imienia i nazwisko, Podobnie jak definiowane były pozostałe zasoby w GCPie przeze mnie. Zaraz wyjaśnię Co to oznacza natomiast chciałbym żebyśmy zdeployowali tego Ingressa  i zobaczy jak działa.

  

Po tym wszystkim możemy go wdrożyć

  

```bash

kubectl apply -f deploy/k8s/ingress.yaml

```

  

Jeżeli Wam się teraz wyświetla komunikat o tym że adnotacja `kubernetes.io/ingress.class` Jest depreceated  to zignorujcie to bo Google Cloud wciąż w adnotacji używa. 

  

Tego ingressa znajdziemy właśnie tam gdzie serwisy ale dedykowanej zakładce Ingress.  teraz musimy zaczekać z dobre 5 minut zanim ingres w pełni zacznie działać nawet jeżeli będziecie widzieć że on jest już wdrożony i ma status okej.  Słuchajcie zrobimy sobie teraz dosłownie tą króciutko 5 minut to przerwę na odsapnięcie,  bo naprawdę wolałbym żeby to już działało zanim pójdziemy dalej,  łatwiej będzie to wszystko zrozumieć i przyswoić.

  

Okej dobra To widzimy że nasz ingres powstał.  typ jest External LB o czym teraz powiem bo to jest bardzo istotne,  bo w praktyce w przypadku ingressa którego dyplorujemy na chmurze generalnie powstają dwa obiekty zaraz o tym powiem więcej. 

  

 następnie mamy taką zakładkę frontads i to co Widzicie To są ipki naszego ingressa i te ipki są dostępne z zewnątrz.  klikając w nie Przechodzimy do przeglądarki możemy zobaczyć czy te nasze serwisy coś tam pokażą.  jeżeli wejdziemy sobie do `hello-app` To powinien przywitać nas komunikat właśnie Hello World z tej aplikacji którą dodatkowo sobie zdefiniowaliśmy przed chwilą. Natomiast jeśli przyjdziemy se do tego linku drugiego który  nie ma żadnego endpointa to jest to właśnie ta druga Reguła która zdefiniowaliśmy czyli w ten sposób nasze zapytania o wyświetlenie trafi do naszego. I powinna przywitać nasz komunikat z naszego API.

  

 Możemy również spróbować wejść na nasze dokumentację Odpytując endpoint `/docs` albo `/redoc`.

  

Czyli Spójrzmy sobie jeszcze raz na to co widzimy w naszej konsoli googlowej z tym jak wygląda nasz manifest. 

  

Jeszcze raz Spójrz na tę regułę tak czyli mamy zdefiniowane to jak trafiają nasze requesty w sytuacji kiedy odpytujemy konkretne endopoty na naszym interesie.  czyli nasz ingres to jest pewien teraz obiekt dostępny z zewnątrz który ma tylko i wyłącznie do zaimponował ten logikę którą zajmował się w regułach,  i jest on identyfikowany poprzez jakieś IP dostępne z zewnątrz.  dosłownie jest to analogiczna sytuacja do LoadBalancera z którego wcześniej definiowaliśmy -  tam też przecież był właśnie IP, Do którego wysłaliśmy requesta i on przekierował ten request do naszego serwisu.  natomiast proszę to jest spojrzeć że ja mam generalnie dwa serwisy -  nasze API oraz tą aplikację Hello App,  Ale generalnie mam jednego Ingressa,  czyli jeden obiekt  dla którego przyporządkowałem jeden IP.  i w tych ścieżkach które to macie można zdefiniować wiele różnych  reguł które po prostu definiują to jak Dan request ma trafiać do konkretnych serwisów.  Dzięki czemu możemy w ten sposób rozwiązać ten pierwszy problem o którym wam mówiłem.  Pierwszy problem z stosowaniem podejścia z LoadBalancera, czyli poprzednie podejście,  był taki że dla danego serwisu definiowany jest oddzielny LoadBalancer co spowoduje wzrost kosztów.  w przypadku ich gresa możecie mieć tylko i wyłącznie jeden obiekt który rozdysponuje requesty do dowolnej liczby serwisów którą macie.

  

## TUTAJ NAJLEPIEJ ZROBIC WYKRES TAKI PODOBNY TROCHE JAK TUTAJ JEST: [https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress) ALE ABY PASOWAŁ STYLEM DO POZOSTALYCH SLAJDÓW KTÓRE TWORZYSZ

  

Teraz czy drugi problem jest rozwiązany?  drugi problem o którym mówiłem  dotyczył tego że w przypadku stosowania LoadBalancera, czyli tego poprzedniego podejścia,  mówiłem że jesteśmy ograniczonej tylko i wyłącznie do tego co tamten LoadBalancer nam dostarcza. To w przypadku ingresa jest troszeczkę bardziej skomplikowane.  patrząc na ten manifest w tak naprawdę widzicie tylko i wyłącznie definicje reguły. Natomiast wraz z ingresem musi powstać obiekt który faktycznie przyjmuje te requesty i jest zgodnie z naszymi regułami przekierowuje do serwisów -  takim obiektem jest tak zwany `IngressController`. IngressController może być dedykowanym  deploymentem na naszym klastrze  lub też dedykowaną usługą dostępną w chmurze. Ingress, czyli zestaw reguł,  można traktować  analogicznie jako frontend,  natomiast logika związana z przekierowaniem tych requestów,  czyli nasz backend to właśnie IngressController. W przypadku pracy na chmurze takie IngressControlery są już zaimplementowane i dla nas dostępne. Żeby go wykorzystać  musimy  dodać do naszego ingressa adnotację mówiącą o tym jaki IngressControler chcemy użyć. 

  

Te adnotacje które zdefiniowaliśmy właśnie w metadanych  to są takie specjalne  dodatkowe meta dane  które są używane przez na przykład przez chmury w konkretnych usługach  albo przez jakieś inne serwisy. Generalnie ta adnotacje mogą być dowolną strukturę i można w nich zabierać dowolne informacje.  Generalnie rzecz biorąc Nie musicie zapamiętywać tych adnotacji bo zawsze w dokumentacji  niektórych Zasobów które będzie wdrażać po prostu będzie napisane że  żeby coś ją zadziało to trzeba taką ustawić adnotację na odpowiednią obiekcie kubernesowym  więc generalnie z tym nie ma nigdy problemu bo jest to dokładnie opisane Kiedy należy tego użyć,  gdzie  i jakie wartości tam powinny się znaleźć.

  

Żeby użyć IngressControllera  który udostępnia Google  w naszym Ingresie  to musiałem dodać po prostu taką adnotację  jak `kubernetes.io/ingress.class: gce`. I w ten sposób Google widzi że deployuje Ingressa  z taką adnota I On tworzy dodatkowe usługi w chmurze to zaraz zobaczymy  które  zawierają w sobie cały backend,  który po prostu przekierowuje requesty zgodnie z naszymi regułami tutaj zdefiniowanymi.

  

Gdybyśmy chcieli skorzystać z innego indeks kontrolera czyli innej implementacji backendu dla naszego ingressa to po prostu tymi adnotacjami odpowiednio byśmy je wskazywali.  Przykładem takiego ingressa controllera, Który jest customowy  i niewystarczany przez chmurę tylko po prostu jest to projekt Open Source  to jest szeroko znany nginx   [https://kubernetes.github.io/ingress-nginx/deploy/#contents](https://kubernetes.github.io/ingress-nginx/deploy/#contents) . Jeżeli ktoś nie chce korzystać z rozwiązań chmurowych,  a z Open wzorcowego rozwiązania które zna lepiej  albo nawet Open starsowe rozwiązanie dostarcza nowych funkcjonalności  dla Ingressa  których nie ma dostarczyć chmury no to wtedy jest to zasada.  w takiej sytuacji po prostu  IngressController  to dedykowany deployment który działa na naszym klastrze  i żeby z niego skorzystać to po prostu tutaj w ingresach trzeba wskazać  Korzystając z odpowiednich adnotacji  to  Korzystając z odpowiednich adnotacji to żeby ingres Korzystał z odpowiedniego IngressControllera. 

  

  My natomiast skorzystamy z tego co dostarczy nam chmura bo jest to w na naszym use-cae w zupełności wystarczające,  i określiliśmy że chcemy IngressController dostarczony przez chmurę. I teraz was zaskoczę,  odpowiada na pytanie Czym jest ten IngressController? Jest to ten sam low balancer który powstał w sytuacji kiedy używaliśmy Serwis typu LoadBalancer. 

  

Jak sobie wejdziemy do naszego okienka z naszym ingresem  to na samym dole jest właśnie LoadBalancer. I jak do niego przejdziemy to trafimy właśnie na LoadBalancer Którego wcześniej wzięliśmy  przy okazji omówienia serwisu typu LoadBalancer.

  

To teraz może się głowić po co to wszystko zrobiliśmy?  po co stworzyliśmy ingressa nowy obiekt żeby stworzyć sobie load balancera,  który de facto można było tak samo stworzyć ale korzystając po prostu z serwisu typu LoadBalancer?  w efekcie czego mielibyśmy zupełnie jeden obiekt mniej.

  

okazuje się że stworzony:

1.  LoadBalancer poprzez Ingressa Ma znacznie więcej opcji do konfiguracji,  niż Load Balancer stworzony przez service typu LoadBalancer. 
    
2. I druga ważniejsza rzecz jest taka  że lot balancer stworzony przez serwis, a nie przez Ingressa Jest oczywiście możliwy do konfiguracji  i zmian pod nasz use case,  ale byłoby to możliwe Korzystając z komend `gcloud`.  na temat przypadku rząd balancera zdefiniowanego  przez Ingressa Okazuje się że możemy modyfikować jego parametry  poprzez manifesty Kubernetesowe, Co znacznie ułatwia zarządzanie  bo ustawienie Load balancera  powstałego przez ingres Będzie trzymane w repozytorium,  jest ona wersjanowane  itd. itd.  jest to znacznie lepsze i łatwiejsze  niż konfiguracja balanserów poprzez  ich zmiany korzystając z tej biblioteki `gcloud`.
    

  

To tyle jeśli chodzi o naszą  adnotację związaną z IngressControllerem.

  

Omówimy sobie jeszcze czym jest ta anotacja związana z satic IP.  i spróbujemy to sobie wszystko jeszcze raz podsumować bo wiem że tutaj ilość informacji jest bardzo duża więc jak już to wszystko mówimy to spojrzymy sobie jeszcze raz na to z większej perspektywy.

  

 i jeszcze została jedna rzecz do ustawienia.  jest jeszcze jedna adnotacja którą tutaj kazała mu dodać `kubernetes.io/ingress.global-static-ip-name` Która będzie zaraz bardzo istotna w danym etapie pracy nad Ingressem. Jak widzicie nadałam adnotację związaną z Global Static IP name. Nazwa sugeruje że naszemu Ingresowi nadaliśmy jakiś statyczny IP. A dokładnie będą precyzyjnym to naszemu Load Balancerowi nadaliśmy konkretny IP. To jest bardzo istotne dlatego że gdybyśmy tego nie mieli to IP które będzie przyporządkowane będzie po prostu za każdym razem losowe Jak powstaje nasz ingres.   Ja chciałbym tego uniknąć dlatego że później ten IP nam się przyda Zobaczcie do czego.  jak w takim razie upewnić się że IP naszego load balancera który będzie rozdysponował ruchy do naszych serwisów zgodnie z tymi regułami która decydowaliśmy był stały. 

  

Na chmurze macie możliwość rezerwacji konkretnych ipków  na wasze potrzeby.  odnajdźmy sobie w konsoli googlowskiej zakładkę o nazwie VPC Network  i tam odnajdźmy sekcje IP Adressess.  tutaj po prostu znajdziecie listę adresów IP które przez was są wykorzystywane.  to co  zrobiłem to po prostu dla każdego z was Zarezerwowałem adres IP po to aby każdy ingres który wy stworzycie miał cały czas stałe IP bo będzie nam to potrzebne za chwilę.  teraz każdy z tych ipków ma nazwę żeby łatwiej go odnaleźć i tak jak to robiłem w przypadku innych zasobów w chmurze tak też i tutaj Czyli po prostu Nazwałem go pierwszą literę imienia i potem nazwisko. 

  

Możemy odszukać mojego IPka  i widzimy że mój IP ma numer taki i taki. I możemy spojrzeć czy jest on używany przez tak zwane Forwarding rule. Jak sobie w nią klikniemy to mi informacje że to jest jakaś reguła przekierowująca. Tutaj pod spodem można zobaczyć taki mały tekst mówiący o tym jaki ingres to wykorzystuje  i widać, że jest wskazanie na mój `mrybinski/credit-scoring-api`  potem możemy kliknąć sobie Target. I tak dalej można dojść do tego że jest to wykorzystywane właśnie naszym LoadBalancerze.

  

 czyli wracając do naszego Manifestu poprzez tą adnotację którą tutaj nadaliśmy ja mówię że chcę żeby mój LoadBalancer który powstanie  na skutek wdrożenia ingressa na klaster miał IP takie jakie sobie do tej stworzyłam i nazwałem jako mrybinski.

  

 Okej mamy jakby omówione wszystko co tutaj się znajduje.  natomiast chciałbym żebyśmy jeszcze raz przez to przeszli Korzystając z  slajda na którą to wszystko narysowałem,  tak żebyśmy sobie spojrzeli na to wszystko jeszcze raz i Użyj to sobie w głowie od początku.

  

1. To co zrobiliśmy tutaj to stworzyliśmy sobie obiekt Ingress który ma proste zadanie chcemy żeby nam przekierował dane requesty do konkretnych serwisów.  przekierowanie jak widzicie odbywa się poprzez konkretne reguły które są bardzo proste,  jeżeli odpytamy naszego interesa poprzez jakiś endpoint to na podstawie matowania nazwy tego endpointa po prostu będzie on przekierowany do konkretnej aplikacji.
    
2.  natomiast jest to tylko i wyłącznie pewien zestaw reguł i można go poprzez analogię traktować jako frontend. Bakenem z kolei jest ten tak zwany indes Controller który po prostu ma w sobie zaimplementowaną całą logikę wysyłania naszego requesta do konkretnego serwisu zgodnie z naszymi regułami które zdefiniowaliśmy. 
    
3. Teraz tym IngressControllerem może być Usługa która jest dostarczana przez chmurę.  w naszym przypadku jest to właśnie googlowski globalanser i w tym przypadku ta usługa jest poza klastrem.  możemy jednak stwierdzić że  te funkcjonalności które dostarcza jest to wystarczające dla nas i możemy  zdeployować swojego własnego  IngressControllera na naszym klastrze. Taki interes kontroler na naszym teatrze po prostu będzie dedykowanym deploymentem,  czyli dosłownie to co robiliśmy wcześniej.  i takim standardowym przykładem ingres kontrola jest właśnie nginx,  który również moglibyśmy sobie definiować na klasycznym na Google Cloudzie.  i następnie wskazać go po prostu w konkretnej adnotacji w ingressie które określa który ten IngressController chcemy użyć. 
    

Jest jeden z takich elementów którego brakuje w implementacji googlowskiego IngressControllera - Mianowicie nie ma on takiej techniki zwanej `rewrite`,  która polega ona na tym aby zmienić naszego urla zanim zostanie  on wysłany  do serwisu.  takimi standardowym Przykładem tego wykorzystania może być Jaka jest w który macie zdefiniowanego ingressa dla tej samej aplikacji ale w różnej wersji.  i można zarobić tak jak jest teraz widoczne na manifeśćie Czyli mamy różne wersje tej samej aplikacji Ale odróżnia nas ona poprzez `v1`, `v2` czy `v3`  poprzedzające nazwę konkretnych endpointów. Technika `rewrite` polega na tym że my  wysyłamy requesta do urla  który ma w sobie określoną, konkretną wersję,  ale zanim ten rekord zostanie przekazany dalej do serwisu to ta część związana z wersją `v1`, `v2` czy `v3`  jest po prostu usuwana  z endpointa  i dopiero taki url wchodzi do serwisu. A teraz dlaczego ta technika jest wykorzystywana?  dlatego że Spójrzcie jak mamy zdefiniowane przygodowe endpol dla naszych decyzji `/decisions`. On jest zdefiniowany po prostu  jako `/decisions`. Natomiast bez techniki `rewrite`,  Która właśnie jest na przykład w NGinx ingress controller, a nie ma jej w googlowskim ingress controlerze Musielibyśmy nasze endpointy definiować jeszcze z wersją  przed  nazwą końcówki, co jest bardzo niewygodne. Więc jeżeli w przyszłości spotkacie się z taką potrzebą wersjonowania naszego API i utrzymania poprzednich wersji to Rozwiązania są trzy:  

1) albo użycie innego ingress controllera który ma w sobie zaimplementowany `rewrite` np. Nginx 

2) oddzielne ingressy dla różnych wersji - możliwe, ale słabe rozwiązanie pod względem kosztów

3) zastosowanie Gateway’ów o których powiem trochę więcej później

4) name-based virtual hosting, które zaraz pokaże

4. Dodatkowo też  w ingresie ustawiliśmy wprost IP dla naszego LoadBalancera.  Zrobiliśmy to z dwóch powodów:  Pierwszy jest to żeby cały czas IP nam się nie zmieniało  w sytuacji kiedy nasz infraz będzie restartowany,  zmieniany itd.  jest takie ryzyko że możemy utracić IP więc chcemy się przy tym zabezpieczyć.  natomiast druga rzecz której zaraz przyjdziemy to to że odejdziemy sobie już od ipków i chcielibyśmy mieć ładną nazwę hosta,  strony internetowej  przez którą możemy po prostu odwołać się do naszego API. W takiej sytuacji potrzebujemy stałego ipka,  po to aby zarejestrować  w jakiejś konkretnej domenie. 
    

  

 Czy macie jakieś pytania do tej części?

  

Okej w takim razie przejdziemy teraz do dalszej części że ona jest interesem czyli Odchodzimy już do ipków a będziemy wprost nadawać nazwy  hostów naszych ze sobą sieciowym tak żeby można było je identyfikować po jakieś konkretnej ładnej nazwie.

  

Na początek zanim zacznę tłumaczyć to zaimplementujmy to sobie w Ingresie  żeby zobaczyć że to faktycznie działa:

  

```yaml

spec:

  rules:

    - host: api.mrybinski.inzynier-ai-example-domain.com

      http:

```

  

Pamiętajcie prosze o zmianie `mrybinski` na swoją osobę I po tych zmianach wdróżmy sobie Ingressa.

  

```bash

kubectl apply -f deploy/k8s/ingress.yaml

```

  

To co zrobiłem to dodałem teraz parametry wcześniej pominąłem o nazwie `host` - Hosto nic innego jak po prostu tekstowa etykieta jakiegoś zasobu sieciowego który pod spodem jest określony za pomocą IP-ka. 

  

Teraz Spójrzmy sobie na naszego ingressa w konsoli googlowskiej i zobaczymy właśnie że nasze ipki zamieniły się wprost na nazwy hostów. Możemy sobie kliknąć w linka i zobaczyć że działa.  Ale przyjrzymy sobie też requesta do naszego API i zobaczę czy faktycznie to działa:

  

```bash 

curl http://api.mrybinski.inzynier-ai-example-domain.com/decisions -X POST -H "Content-Type: application/json" -d '{"installment_rate_in_percentage_of_disposable_income": 0.25, "age_in_years": 40, "foreign_worker": "yes", "present_employment_since": "unemployed", "personal_status_and_sex": "male: single"}'

```

  

Jak widzicie działa. 

  

Teraz To co tutaj zaimplementowaliśmy jest pod jednym hostem i Różne aplikacje są rozróżnialne poprzez endpointy do których wysyłamy requesty. Natomiast docelowo chcielibyśmy żeby każda aplikacja miała generalnie swój host i wszystkie  requesty były do niej wysyłane a po prostu na to rozróżnialne po nazwie hosta.

  

Teraz na poprzednim slajdzie mówiłem o tym że brakuje w Gracji takie techniki zwanej `rewrite`.  Wyobraźcie sobie że mamy teraz takie reguły:

  

```yaml

spec:

  rules:

    - host: api.mrybinski.sotrender-rd-test-domain.com

      http:

        paths:

        - pathType: Prefix

          path: "/v1"

          backend:

            service:

              name: credit-scoring-api-v1

              port:

                number: 80

        - pathType: Prefix

          path: "/v2"

          backend:

            service:

              name: credit-scoring-api-v2

              port:

                number: 80

```

  

Wyobraźmy sobie że na chwilę mamy nasz API w dwóch wersjach i moglibyśmy to rozmawiać w ten sposób.  Czyli mamy naszego hosta i po prostu po endpointach będzie ta wersja rozróżnialna. Problem jest taki że jeżeli odpytamy sobie  takiego ingressa i wyślemy właśnie requesta na końcówkę `/v1/decisions` Żeby odpytać model o decyzję to niestety ale to naszego API przyjdzie request też z taką samą końcówką `/v1/decisions`, Ale to nie zadziała bo nasz endpoint to po prostu `/decisions` bez tego `v1`. Dlatego W tym celu można użyć  techniki zwanej właśnie `rewrite`,  która usunie  ten prefiks `/v1`  zanim reques trafi do naszego serwisu.  Natomiast musielibyśmy zmienić adres IngressControllera  z Google’owskiego na coś innego,  ale nie będziemy tak sobie komplikować pracy.  natomiast można to osiągnąć w inny sposób. 

  

Wróćmy sobie do tego co mieliśmy wcześniej zdefiniowane.

  

W takim razie Wyobraźmy sobie że chcemy tego mieć tą aplikację na innym hoście.  sprawa to jest prosta po prostu  musimy zrobić jeszcze jedną regułę I pozmieniać `pathType` z `Exact` na `Prefix`, a `path` z `/hello-app` po prostu na `/`:

  

```yaml

spec:

  rules:

    - host: hello-app.mrybinski.sotrender-rd-test-domain.com

      http:

        paths:

        - pathType: Prefix

          path: "/"

          backend:

            service:

              name: hello-app

              port:

                number: 80

    - host: api.mrybinski.sotrender-rd-test-domain.com

      http:

        paths:

        - pathType: Prefix

          path: "/"

          backend:

            service:

              name: credit-scoring-api

              port:

                number: 80

```

  

Czyli wcześniej mieliśmy jedną regułę i po tą jedną regułą były dwa serwisy ale po tym samym hostem. Natomiast Tutaj generalnie daliśmy w regułę  i dzięki temu mogliśmy  umieścić drugą aplikację po prostu na innym hoście.

  

Wdróżmy to (JEŻELI NIE ZADZIAŁA TO LEPIEJ USUNĄĆ I WDROŻYĆ NA NOWO)

  

```bash

kubectl apply -f deploy/k8s/ingress.yaml

```

  

I spójrzmy sobie do konsoli google’owskiej jak to wygląda.

  

Czyli widzimy że tworzy nam się ingres  który będzie przekazywał nasze requesty do dwóch serwisów zgodnie z tymi regułami.  Ingress jest osiągalny Pod jednym ipkiem, A jak widzimy będziemy mieli teraz dwa hosty a tym samym ipku i po tych hostach będą rozróżnialne requesty do naszych serwisów. 

  

W ten sposób można by było obejść brak tego `rewrite` i po prostu  mieć wtedy tutaj na początku hosta `v1`, czy też `v2`.

  

Okej jak Widzicie Działa Czyli teraz po prostu  w takiej sytuacji requesty będą kierowane na podstawie nazwy hosta a nie na podstawie endpointa który definiowaliśmy wcześniej. To co jeszcze nam brakuje to  dodaję certyfikatów i dzięki  Czemu odpytujemy nasz serwis za pomocą HTTPS,  natomiast Skupmy się najpierw na kwestii hosta. 

  

Okej czyli udało nam się w ten sposób odejść od tych brzydkich ipików i dostać taką ładną nazwę hosta dla naszego serwisu z API naszego modelu. I teraz pytanie powstaje Czy możemy tutaj wstawić dowolnego hosta a ingres sam z siebie jakby spowoduje to że te nasze API będzie dostępne pod tym adresem?  Niestety sprawa  jest bardziej skomplikowana niż się tutaj wydaje dlatego że ja już przygotowałem hosting dla nas po to żeby takie adresy  można  zastosować w naszym przypadku. Chciałbym teraz pokazać kroki jakie należy wykonać żeby móc w chmurze zarejestrować naszą Domana,  następnie zarejestrować po to domenę nasze IP ingressa żeby w ingresie  móc nadawać dowolne hosty naszym serwisom do których będą nadawane requesty. Ja niestety nie dałem wam takich uprawnień żebyście mogli te opcje przeglądać i włączać, Więc chciałbym żebyście prosto spojrzeli na mój ekran i zobaczyli  gdzie można to ustawić w chmurze. Wiadomo każda inna chmura będzie miała troszeczkę inne zestaw opcji Natomiast generalnie logika postępowania będzie taka sama w każdym przypadku. I jeszcze na marginesie od razu ważną rzecz że  to co  będę Wam teraz pokazywał to jest Case w którym wy sami chcecie zarejestrować domenę  i podpiąc do niej IPki, Natomiast w rzeczywistości  w pracy będzie już zespół  odpowiedzialny za zarządzanie kwestiami sieciowymi między innymi domeną waszej firmy I to  z nimi będziecie konsultować w tej kwestii żeby konkretne IPki waszego LoadBalancera był zarejestrowany pod konkretną domenę, tak abyście np. w Ingressie mogli nadawać hosty waszym regułom/serwisom. 

  

Pokaże Wam jak można to zrobić na chmurze:

  

1. W konsoli googlowej odnajdujemy usługę zwaną Network Services,  a potem w niej Cloud Domains. Jest usługa związana z rejestracją domen.  po wejściu do niej to co widzimy to właśnie zarejestrowana domena  na nasze potrzeby zajęć z Kubernetesa. Tego Domana możemy zarejestrować po prostu sobie klikając Register Domain. Oczywiście rejestracja domeny może odbywać się w dowolnym miejscu jeżeli Znacie jakieś inne usługę nie znajdującą się w chmurze to też możecie z niej skorzystać.  na początek Wyszukujemy sobie jakąś domenę, I w zależności od  dostępności mamy różne opcje i różne ceny. Później jak się przeklikamy to właśnie trzeba podać swoje dane związane z domeną i w ten sposób udaje nam się zarejestrować domena w chmurze googlowskiej.
    
2. Jak już nam się to uda zrobić to musimy teraz pod naszą domenę podpiąć ipki naszych ingresów tak żebyśmy mogli w nich skorzystać z naszej domeny w parametrze `host`.  aby to uzyskać musimy w dns-ach wpisać nasze adresy. Po lewej stronie mamy taką opcję jak Cloud DNS. Po utworzeniu domeny w Google cloudzie taki DNS się u nas tutaj utworzy,  więc możemy od razu z niego skorzystać.  jak chcemy do niego wejdziemy to możemy znaleźć listę adresów IP wraz z DNSami.  Czym jest DNS? DNS to skrót od Domain Name Server, Natomiast w prostym językiem tnn jest to nic innego jak powiązanie ipk właśnie z naszą domeną.  jak to widzicie teraz na tą listę jest to lista powiązań ipików ingressa z po prostu nazwami  hostów.   możemy sobie wejść w jedno z nich,I zauważymy w nim właśnie ten routing data jako lista ipków która jest powiązana z tą domeną. I ta gwiazdka która stoi  na początku oznacza to że w sumie jakakolwiek nazwa jaka tam się znajdzie  w miejsce gwiazdki oznacza  to że jest ona podporządkowana pod IP które kryje się pod tym  DNSem.  w ten sposób mogliśmy właśnie w interesie nadawać sobie dowolną nazwę na początku,  dlatego mieliśmy raz `hello-app` a za drugim razem `api`. Dzięki temu po prostu pod IP jednego ingressa można podpiąć wiele różnych nazw hostów w ten sposób rozdzielić serwisy. 
    
3. Dodali takiego rekordu jest bardzo proste, klikamy sobie w Add Standard. Tutaj wypełniamy właśnie  informacje  jaki to ma być DNS,  możemy zarejestrować tak samo jak mieliśmy  że  z gwiazdką,  albo wprost określić pełny adres  i tylko ten jeden adres będzie miał IP-ka którego sobie piszemy.
    
4. I na tym kończy się nasza praca z rejestracją domenę i  podpisania ipków do nich.  Generalnie zasada jest analogiczna w przypadku korzystania z innych chmur tylko trzeba znaleźć odpowiedni usługi które są odpowiedzialne podobnie też jest w przypadku  innych hostingu których możecie używać. 
    
5. Natomiast trzeba pamiętać o bardzo ważnej rzeczy że te zmiany które tutaj wprowadzicie  mogą trochę potrwać. Można to sobie sprawdzić na mapce DNSów np. takiej [https://dnsmap.io/#A/api.mrybinski.sotrender-rd-test-domain.com](https://dnsmap.io/#A/api.mrybinski.sotrender-rd-test-domain.com) czy Wasze domeny są już dostępne wszędzie, bo może być tak, że jest to osiągalne Z Polski ale dlaczego nie będzie jeszcze oszczędzalne z Ameryki po prostu trzeba poczekać  aż wszystkie  serwery odpowiedzialne za DNS się po prostu zaktualizują. 
    

  

Okej w ten sposób możemy sobie dodać domenę  oraz podpiąć ipki pod naszą domena Dzięki czemu możemy w ingressie wskazać te hosty i wtedy odpytywać nasze serwisy po takich ładniejszych nazwach a nie po ipikach.

  

 natomiast jeszcze jedna rzecz jest do skonfigurowania to znaczy mającym z własną domenę możemy zadbać o certyfikaty TLS Tak żeby nasze połączenie z API było szyfrowane i można było skorzystać z protokołu HTTPS czyli http w wersji secure. Jednakże uzyskając certyfikatów tyle z naszej domeny jest już troszeczkę bardziej złożonym procesem i niestety ale nie będziemy tego nawet na naszym zjeździe bo to jest zbyt duży temat jak na początek w ogóle tym bardziej kontaktu z kubratesem oraz jest to temat związany stricte z kwestiami sieciowymi a nie uczeniem maszynowym, wiec jest poza scopem naszego spotkania, natomiast dla osób zainteresowanych po prostu podzielę się linkiem z dokumentacją która dokładnie wyjaśnia jak to może zaimplementować na gcp i tam też są linki do innych chmur z wykorzystaniem bardzo popularnego sposobu na uzyskanie tego certyfikatu jak lets encrypt [https://letsencrypt.org/pl/](https://letsencrypt.org/pl/) 

[https://cert-manager.io/docs/tutorials/getting-started-with-cert-manager-on-google-kubernetes-engine-using-lets-encrypt-for-ingress-ssl/](https://cert-manager.io/docs/tutorials/getting-started-with-cert-manager-on-google-kubernetes-engine-using-lets-encrypt-for-ingress-ssl/) . Krótko mówiąc trzeba na klastrze zdeployować Tak zwanego cert managera  czyli to jest po prostu deployment  którego zadaniem jest dbanie o to że certyfikaty  waszych domen są trzymane na klastrze,  dostępne dla Ingressa,  oraz  są co odpowiedni czas odświeżane. 

  

Na co dzień tak jak wspominałem na pewno ktoś inny u was z firmy będzie się zajmował kwestiami za nami z siecią certyfikatami TLS i tak dalej. Najczęściej one będą po prostu trzymane jako Sekrety w kubretesie o której powiemy sobie troszkę później,  natomiast jeżeli Już takim certyfikatem dysponujemy to jest to nic innego jak pewien zaszyfrowane plik z pewnymi danymi.  i aby z takiego certyfikatu w naszym ingresie skorzystać to jest w tym celu właśnie stworz `tls`  który ma w sobie dwie rzeczy: `hosts` oraz `secretName`. Host czyli nasza lista hostów Dla których ten  certyfikat TLS jest,  oraz jak sami widzicie później nazwa secretu w którym te dane są przechowywane. I tak właśnie przed u mnie jest w pracy Ja nie Zarządzam oczywiście tymi certyfikatami bo to jest poza skąpem mojej w ogóle umiejętności i obowiązków natomiast jestem współpracuje  z innym devopsem który zjazdu na takimi rzeczami i po prostu ten  certyfikat TLS jest dostępny na klastrze jako secret.  jeżeli wystawiam nowy serwis właśnie Korzystając z ING  to Proszę go o to żeby właśnie  zarejestrował  w naszej domenie IP którego  którego przypisałem dla LoadBalancera który tworzony jest przez Ingressa  i następnie  dzieli Się ze mną certyfikatem TLS,  tak żebym mógł go zapisać na klastrze jako Secret i mógł się do niego odwołać.  jak już odwołamy się do niego tutaj w tych parametrach TLS  to jest w zupełności wystarczające żeby połączenie z waszymi API było szyfrowane i odbywało się poprzez HTTPS. 

  

W ramach ingressa jeszcze chciałbym umówić jeden parametr na początku pominąłem, a mianowicie `defaultBackend`, Który jak sami widzicie i dokumentacja o tym pisze Jest to Parametr do ustawienia dla sytuacji w której wysłamy requesta do naszego ingressa i nie spełnia on żadnych reguł zdefiniowanych.  czyli  ten defaultBackend to jak taki ELSE w IFie. 

  

W takim razie odpytajmy  naszego ingressa wpisując jakiś błędny adres I zobaczmy co dostaniemy.

  

Otrzymujemy taką informację że `response 404 (backend NotFound), service rules for the path non-existent` Oczywiście po prostu nie ma żadnych reguł naszym backendzie w tym przypadku chodzi o Ingressa. To czy zaakceptujecie taki stan rzeczy  czy Jednak chcecie żeby takie zapytanie było przekierowane To zależy od was. `defaultBackend` nie jest wymagany kiedy zdefiniujemy reguły w Ingressie, Ale jest wymagany gdy tych reguł nie ma.  ale Wyobraźmy sobie dla ćwiczeń że jednak chcemy go dodać do naszego ingressa żeby  te od pytania które teraz przed chwilą wysłałem faktycznie zadziałało.

  

```yaml

spec:

  defaultBackend:

    service:

      name: credit-scoring-api

      port:

        number: 80

  rules:

```

  

Czy w tej sytuacji zdefiniowałem sobie że po prostu defaultem bęðzie nasze API. Zaktualizujmy sobie naszego ingressa:

  

```bash

kubectl apply -f deploy/k8s/ingress.yaml

```

  

A teraz poczekamy chwilę czasu to po prostu od pytanie Po dowolnym   dowolnej nazwie po prostu spowoduje że nasze zapytanie wyleci do API. 

  

Okej Słuchajcie i na tym byśmy zakończyli omawiany Ingresa natomiast na koniec  chciałbym powiedzieć wam o jego wadach i co jest jego następcą. 

  

Ingres jest jakby docelowym obiektem którego będziecie używać żeby udostępnić wasze API na zewnątrz natomiast musicie wiedzieć o jego dwóch istotnych wadach:

  

1.  po pierwsze jest to że ingres   działać na tym samym namespace na którym są wasze serwisy,  co jest rzeczą problematyczną bo Zobaczcie że  Tak naprawdę każdy z was musiał stworzyć swojego własnego ingressa.  czyli de facto my musieliśmy  zarezerwować kilkanaście ipków,  każdy ingres miał ten IP dedykowany i stworzone zostało kilkanaście load balancerów więc generalnie  Pierwszy problem który wam pokazywałem że tworzonych jest wiele balanserów w przypadku serwisu typu Load Balancer,   nie jest rozwiązany w pełni.  niemożliwe jest zdeployowanie takiego jednego ingressa który  byłby globalny i w ramach niego stworzylibyśmy wszystkie reguły przekierowujące do odpowiednich serwisów Niezależnie od tego na jakim namespace są.  na szczęście ingres pozwala na  na to aby zdybrować go w wersji multiplasta czyli jeżeli mielibyśmy wiele klastrów to może być jeden ingres który przekierowuje ruch do wielu krasno jednocześnie natomiast wciąż jest założenie takie że te wszystkie serwisy do których ingres będzie przekierował ruch muszą być w tym samym Space nawet jeżeli mówimy o wielu klastrach. Przeważnie spotkacie się z taką sytuacją gdzie po prostu są tworzone różne namespace dla różnych aplikacji także po prostu łatwiej tym zarządzać Niestety w związku z tym ingressa też trzeba będzie tworzyć kilka razy.
    
2.  druga wada jest taka że ingres nie pozwala wam na stworzenie jakiś bardzo skomplikowanych reguł sieciowych które przekierowują wasze serwisy.  tutaj jak widzicie sprawa jest prosta jeśli taki host to wyślij do tego serwisu,  a jeśli inny host to wyślij do drugiego service’u. Natomiast w naszych przykładach uczenie maszynowego możecie znaleźć się z taką sytuacją że na przykład chcielibyście się przeprowadzić testy AB,  albo chcielibyście wdrożyć model wersji Kanary czyli na przykład 90% ruchu sieciowego przechodzi do jednego modelu,  a 10% do nowego modelu. Albo jeszcze chcielibyście stworzyć tak zwany Shadow Deployment  które zakłada że wasz ruch sieciowy był replikowany i jeden w request który otrzymuje cię do ingressa Chcielibyście żeby on był jednocześnie wysłany i do jednego serwisu i do drugiego.  to jest jeden ze sposobów na Dipol nowego modelu żeby zobaczyć Jaką on ma skuteczność ale w taki sposób żeby nie  podejmował jeszcze ostateczne decyzje,  po prostu działa sobie w tle.  no i niestety w takich scenariuszach naszych uczeń Masz nowego nie da rady  zastosować takich regułów ingressie żeby nam się to udało. 
    

  

Dlatego przy takich zaawansowanych kwestiach sieciowych używa się już innych obiektów i rozmawiają są dwa My niestety ich nie mówimy bo to są tematy już dosyć naprawdę bardzo trudne i  i zalecam zrobienie strategii materiałów dopiero wtedy kiedy już więcej nabierzecie obycia z Kubernetesem: Rozwiązania są dwa:

  

1.   Od 5 lat budowany jest następca ingressa tak zwany Gateway,  i dopiero na początku tego roku 2024  Gateway wyszedł w wersji 1.0.0  czyli w takiej wersji oficjalnej,  w której można by było już używać na produkcji.  nawet jak otworzymy sobie stronę internetową z ingresem [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/) To pierwsze zdanie jakie was przywita to “Note: Ingress is frozen. New features are being added to the Gateway API.” Czyli widzicie że nawet oficjalna dokumentacja  kubernetesa wskazuje na to że  po prostu następcą Ingressa Gateway. Tylko że Gateway jest jeszcze trudniejsze niż imprez ale pozwala na znacznie więcej możliwości.  przede wszystkim pozwala on na  właśnie ważenie  na waszych serwisach w taki sposób że na przykład 10% ruchu przyjdzie do nowego modelu  a 90% ruchu przyjdzie do starego modelu.  czyli można ważyć ruch do naszych serwisów tak jak nam się potem podoba.  w dodatku też można zdefiniować jednego gatewaya tylko i wyłącznie  który wyśle wam requesty do dowolnych serwisów na dowolnych namespace,  a jeszcze jest opcja tak zwanego MultiClusterGateway’a  który w ogóle jeszcze redysponuje wam ruch po wszystkich klastrach więc generalnie rozwiązanie jest naprawdę  bardzo fajne i niweluje to podstawową wadę ingressa  plus dostarcza nowych funkcjonalność. (TUTAJ DODAJ SLAJD Z PRZYKLADOWYM MANIFESTEM GATEWAYA, HTTPROUTE)
    
2. Natomiast Jak się to robiło przed Gatewayem,  bo tak powiedziałem on dopiero jest od tego roku w wersji 1.0.0. Rozwiązaniem tych wszystkich rzeczy jest zlipywanie na swoim klastrze tak zwanego service mesh,  który Krótko mówiąc jest takim globalnym Proxy działające na naszym  klastrze,  każdy ruch sieciowy który się odbywa w naszym klastrze musi przychodzić przez ten proxy,  niezależnie od tego czy to jest ruch ze mną naszą klastra,  czy ruch pomiędzy Podami. Po prostu upraszczając żeby nie wchodzić za bardzo w szczegóły I was tutaj nie obciążyć zbyt dużą wiedzą tym bardziej że nie będziemy do tego robić na naszym zjeździe,  po prostu serwis mesh to po prostu globalne proxy zainstalowane na klastrze. Takim  standardowym przykładem  service mesh jest tak zwany Istio, Jeżeli wdrożone macie istio na klastrze  to pozwala on nam na tworzenie nowych obiektów Kubernetesowych,  które między innymi mają taką możliwość żeby zreplikować requesta  Ile razy chcemy żeby został on przekierowany do wielu serwisów na raz I w ten sposób można na przykład wdrożyć właśnie te podejście z shadow deploymentem, czyli drugim modelem działającym w tle,  albo  po prostu przekierować jakąś część ruchu do tego serwisu a jakoś mniejszą część ruchu do drugiego serwisu. (TUTAJ DODAJ Z TAMTEJ PREZKI PRZYKLADOWY MANIFEST Z ISTIO - TEN NA ROUTING 90/10, CZY SHADOW DEPLOYMENT) [https://www.cncf.io/wp-content/uploads/2020/08/CNCF-Presentation-Template-K8s-Deployment.pdf](https://www.cncf.io/wp-content/uploads/2020/08/CNCF-Presentation-Template-K8s-Deployment.pdf) 
    

  

I tak naprawdę Poznanie serwisu  oraz Ingressa To moim zdaniem nie Koniec tematów sieciowych w kubernetesie a dopiero początek  bo  bo naprawdę jeżeli chcemy zrobić coś bardziej skomplikowanego a z taką sytuacją  na pewno znajdziecie się w czasie swojej pracy pryzmy czy później to trzeba uczyć znacznie bardziej skomplikowanych narzędzi i niestety ale jeszcze trudniejszy obiektów do wdrożenia na Kubernetesie niż Service i Ingress. Natomiast jak widzicie serwis i Ingress to są dwie podstawowe rzeczy które trzeba znać, Z ich wykorzystaniem znajdziecie się najczęściej i generalnie te dwa obiekty pozwalają wam już na udostępnienie API na środowiskach produkcyjnych. 

  

I to może tyle jest z mojej strony przychodzi i dresa,  mam dla was kilka ćwiczeń żebyście sami po swojej pokonfigurowali i zobaczyli czy to wszystko działa.**