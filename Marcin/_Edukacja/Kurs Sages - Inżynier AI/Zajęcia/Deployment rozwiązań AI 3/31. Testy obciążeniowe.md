# Testy obciążeniowe

  
  

### MOŻE BYĆ PROBLEM PRZY INGRESSIE, ŻE BĘDZIE RZUCAŁ 504, WTEDY TRZEBA ZAKTUALIZOWAĆ JEGO HEALTH_CHECKA JAKO BACKENDCONFIG I PODAĆ GO DO SERWISU. 

  

Backend:

  

```yaml

apiVersion: cloud.google.com/v1

kind: BackendConfig

metadata:

  name: credit-scoring-api

  namespace: mrybinski

spec:

  timeoutSec: 3600

  healthCheck:

    checkIntervalSec: 60

    timeoutSec: 60

    healthyThreshold: 1

    unhealthyThreshold: 3

    type: HTTP

    requestPath: /health

    port: 8080

```

  

W serwisie trzeba wtedy dorzucić adnotacje:

  

```yaml

apiVersion: v1

kind: Service

metadata:

  name: credit-scoring-api

  namespace: mrybinski

  labels:

    phase: live-coding

    app: rest-api

  annotations:

    cloud.google.com/backend-config: '{"default": "credit-scoring-api"}'

spec:

  type: ClusterIP

  selector:

    phase: live-coding

    app: rest-api

  ports:

  

    - port: 80

      targetPort: predictions

```

  

Okej w takim razie Przechodzimy sobie do następnego tematu w ramach którego zobaczymy działanie HorizontalPodAutoscalera i czy go dobrze skonfigurowaliśmy. Mianowicie teraz przechodzimy do testów obciążeniowych naszego API.  wcześniej na 1-wszym zjeździe poświęconym  budowania API Fast API mieliśmy testy jednostkowe oraz testy integracyjne.  natomiast oprócz tych testów musimy również wykonać testy obciążeniowe naszego serwisu  po to aby  sprawdzić właśnie jak będzie zachowywał się pod konkretnym obciążeniem,  czy Spodziewamy się  takich czas odpowiedzi jakie akceptujemy .  takie testy też wam pokażą lepsze oszacowanie tego ile CPU będą wymagać wasze Body,  czy konfiguracja waszych HorizontalPodAutoscalerów pozwoli wam osiągnąć Wasz cel. 

  

Okej zacznijmy w takim razie od tworzenia tych testów. Do testów obciążeniowych wykorzystamy bardzo fajną bibliotekę o nazwie `locust` https://locust.io/, która w sumie dostarcza wszystko co chcemy a pisanie testów obciążeniowych  z jej wykorzystaniem jest bardzo bardzo proste. 

  

Zainstalujmy ją sobie w naszym środowisku:

  

```bash

poetry add locust

```

  

Przejdźmy do `tests/stress` stwórzmy `__init__.py` oraz `test_decisions.py` 

  

Zaimportujmy sobie biblioteki które będziemy potrzebować:

  

```python

from locust import HttpUser, task, between

import random

from src.features.categories import ForeignWorkerCategory, PresentEmploymentSinceCategory, PersonalStatusAndSexCategory

```

  

Teraz to co musimy zrobić w ramach `locust` to stworzyć sobie przykładowego użytkownika naszego API i określić co ma robić - w naszym przypadku po prostu ma wysłać requesta.

  

Zaczne od bazowej klasy, która nam posłuży jako abstrakcja dla właściwych klas:

  

```python

class APIUser(HttpUser):

```

  

W nim zdefiniuje sobie atrybut `data`. Chcemy po prostu za każdym razem wysłać inny request żebyśmy sprawdzili to jak API działa w momencie kiedy zwraca cały czas predykcje z modelu. Najprościej zrobić to tak, że będziemy losować sobie wartości w tych zmiennych, czyli analogivznie tak jak jest to zrobione w skrypcie `send_example_request.py`. 

  
  

```python

    @property

    def data(self):

        return {

            "installment_rate_in_percentage_of_disposable_income": random.random(),

            "age_in_years": random.randint(18, 65),

            "foreign_worker": random.choice(list(ForeignWorkerCategory.mapping().values())),

            "present_employment_since": random.choice(list(PresentEmploymentSinceCategory.mapping().values())),

            "personal_status_and_sex": random.choice(list(PersonalStatusAndSexCategory.mapping().values())),

        }

```

  

Następnie jeszcze headers:

  

```python

    @property

    def headers(self):

        return {

            "Content-Type": "application/json",

        }

```

  

I zdefiniujemy sobie teraz pierwszy schemat użytkownika

  

```python

class ConstantAPIUser(APIUser):

    host = "http://api.mrybinski.TWOJA_DOMENA.com/decisions"

```

  

Parametr `host` określający na jakim adresie będzie nasze API działało. `locust` wymaga abyśmy zdefiniowali ten parametr jako `host`.

  

Następnie definiujemy sobie funkcję `post_decisions` I to co musimy w niej zrobić to po prostu napisać kod który wyśle nam danego requesta do naszego API. 

  

```python

def post_decisions(self):

        self.client.post(url=self.host, headers=self.headers, json=self.data)

```

  

Ten `self.client` to atrybut udostępniany przez `HttpUser` - to jest ten sam client, który używany jest w bibliotece `request`, dlatego też potem wywołanie metody `post` jest praktycznie takie same jak `requests.post(...)`

  

I ostatnia rzecz to jest dodanie decoratora `@task` do naszej funkcji, który powie `locust`-owy, że ma uruchomić właśnie tą funkcję.

  

```python

@task

def post_decisions(self):

```

I to wystarczy, żeby przeprowadzić pierwszy test. To co zostało do zrobienia to stworzenie konfiguracji naszego testu.  W tym celu stwórzmy `locust.conf` w tym samym folderze, Czyli plik konfiguracyjny naszych testów w którym zawrzemy takie wartości

  

**pierwszy test:**

```

locustfile = test_decisions.py

headless = true

users = 1

spawn-rate = 1

run-time = 1m

html = report_1_constant_user.html

```

  

1. `locustfile` - Wskazuje na plik z naszymi testami

2. `headless` - `locust` pozwala na uruchamiać testów z poziomu interfejsu graficznego więc ten parametr to wyłącza - bedziemy uruchamiać nasze testy z poziomu terminala

3. `users` - Liczba użytkowników naszego API - na początek ustawmy jeden, potem bedziemy to zmieniać

4. `spawn-rate` -  spawn-rate określa jak często ci użytkownicy mają się pojawiać - 1 oznacza jeden na sekundę, będziemy to zmieniać potem

5. `run-time` - Jak długo będę trwały testy - ustawmy na minutę

6. `html` -  nazwa naszego raportu który zostanie wygenerowany z wynikami testu

  

Teraz przejdźmy sobie do terminala i wejdmy do tego folderu z naszymi testami

  

```bash

cd tests/stress

```

  

I uruchomimy nasze testy korzystając z komendy `locust` I następne musimy wskazać nazwę klasy czyli nasz `ConstantAPIUser`, bo w jednym pliku może zawierać się wiele scenariuszy użytkowania.

  

```bash

locust ConstantAPIUser

```

  

Poczekajmy minutkę aż się testy wykonają I zobaczmy co otrzymamy. 

  

Okej to po pomyślnym przeprowadzeniu testów powinniśmy otrzymać nasz raport w tym samym folderze `report_1_user.html`,  Otwórzmy go i zobaczmy co nam `locust` przygotował.

  

1. Mamy informacje o tym kiedy dany test został przeprowadzony,  na jaki adres wysłany był requesty i jaki był skrót używany - czyli po prostu podstawowe te dane

2. Dalej mamy informacje o naszych requestach które zostały wysłane Zobaczcie mamy liczbe requestów wysłanych w trakcie tego testu. Informacje też o tym czy jakiś request z nich został nieprzetworzony czyli liczba faili - czyli wszelkie błędy 400 bądź 500. informacje o średnim czasie przetwarzania requestów, min, max, No i też `RPS` czyli tylko jest Request Per Seconds, które mówi o naszej przepustowości czyli throughput.

3. Pod spodem mamy z kolei informacje o czasie  zwracania odpowiedzi przez nasze API wartości medialne oraz konkretnych percentyli. 

4. I pod spodem mamy wykresy liczba requestów wysłana na sekundę czas odpowiedzi gdzie dwie metryki są podane i w sumie najważniejsze - mediana oraz 95 percentyl. Oraz liczba użytkowników która odpytywała nasz API - w tym przypadku cały czas był to tylko jeden użytkownik.

  

Zobaczmy sobie szybko czy liczba podów nam się zwiększyła:

  

```bash

kubectl get deployment credit-scoring-api --namespace=mrybinski

```

  

TO MOŻE SIĘ ZMIENIĆ -> Powinniśmy dostrzec zwiększenie się liczby Podów, zatem HPA zadziałał. 

  

Ja tam jak widzicie pierwszy prosty test obciążający został wykonany,  na razie na małą skalę.

  

Teraz wróćmy sobie do naszego skryptu pythonowego. Ten użytkownik którzy został przez nas zdefiniowany ma nazwę `ConstantAPIUser` - nie bez przyczyny.  `Locust` pozwala nam na definiowane różnych użytkowników, czyli różnych scenariuszy testowania. W tym celu można skorzystać z parametru `wait_time` w atrybutach danej klasy. Co określa ile użytkownik Czeka przed wysłaniem kolejnych requestów.  My takiego parametru nie ustawiliśmy, dlatego nasz użytkownik działa tak że jak tylko otrzymujesz odpowiedź naszego API to wysyła do niego request. To jest scenariusz testowy taki który nasze API jest ciągle odpytywane, jest pod ciągłym obciążeniem, ale przez jednego użytkownika.

  

Jak widzicie nasze API sobie poradziło sobie z zapytaniami od jednego użytkownika. To teraz sprawdźmy w takim razie jak nasze API działa kiedy otrzymywałby zapytania od 5 użytkowników na raz.

  

Zmieńmy sobie `locust.conf`

  

**drugi test**

  

```

locustfile = test_decisions.py

headless = true

users = 5

spawn-rate = 0.25

run-time = 1m

html = report_5_constant_user.html

```

Ustawiamy sobie pięć użytkowników oraz Spawn rate równy 0,25 który oznacza że co 4 sekundy będzie dołączony nowy użytkownik. Włączmy testy poczekamy minutę i sprawdźmy jak wygląda raport.

  

```bash

locust ConstantAPIUser

```

  

Test jaki w tej chwili przeprowadzamy to sytuacja w której pięciu użytkowników odpytuje nasz API praktycznie non stop to znaczy jak tylko otrzymuje od nas odpowiedź to wysyła następny request.

  

Zanim zobaczymy sobie raport wynikami rzućmy szybko okiem na to ile Podów mamy na naszym deploymencie:

  

```bash

kubectl get deployment credit-scoring-api --namespace=mrybinski

``` 

  

Wynika że mamy ich Max czyli 5. 

  

 to teraz zobaczmy wyniki w `report_5_users.html`.

  

Jak widzicie Response Times ten percenty 95 już wzrósł z wartości X milisekund na wartość Y. Czyli widzicie już  sobie gorzej poradziliśmy z taką skalą użytkowników, Nawet przy znacznie większej ilości Podów niż to było Przy poprzednim teście.

  

To teraz zróbmy kolejny eksperyment jeszcze na znacznie większą skalę. Zobaczmy jak zachowuje się nasze API przy stu użytkownikach 

  

**trzeci test**

  

```

locustfile = test_decisions.py

headless = true

users = 100

spawn-rate = 5

run-time = 1m

html = report_100_constant_user.html

```

  

Teraz tutaj mamy 100 użytkowników i co sekundę pojawia się 5 nowych. Czyli po 20-stej sekundzie trwania testów powinno już API być odpytywane przez 100 użytkowników.

  

```bash

locust ConstantAPIUser

```

  

Zobaczmy sobie raport.

  

Pojawiają się już pierwsze problemy - 500-tki, które rzucane są nie przez nasze API, a bardziej przez LoadBalancery, które przy tak dużym ruchu i tak małej ilości Podów mogą być zapchane przez requesty.

  

Teraz dla odmiany zrobimy troszeczkę inny schemat dlatego że locust Pozwala na przykład na to żeby  poczekać sobie zanim wyślemy następny request. 

  

Zrobimy sobie teraz użytkownika który wysyła requesty co od jeden do pięciu sekund:

  

```python

class RandomAPIUser(APIUser):

    host = "http://api.mrybinski.sotrender-rd-test-domain.com/decisions"

    wait_time = between(1, 5)

    time_accept_in_seconds = 0.1

```

  

I dodatkowo damy sobie też parametr który świadczy o akceptowalnym czasie dla nas i przyjmiemy że jest to 1/10 sekundy czyli 100 milisekund.

  

W dodatku też zmienimy sobie troszeczkę naszą  naszą funkcje `post_decisions`,  aby uwzględniała ten parametr z czasem:

  

```python

    @task

    def post_decisions(self):

        with self.client.post(url=self.host, headers=self.headers, json=self.data, catch_response=True) as response:

            if response.elapsed.total_seconds() > self.time_accept_in_seconds:

                response.failure("Exceeded maximum response time")

            else:

                response.success()

```

  

To co robię to po prostu Raportuję porażkę kiedy nasz czas przekroczy ten akceptowany czas a w przeciwnym wypadku sukces.

  
  

Zanim odpalimy testy to jeszcze  zeskanujemy nasz deployment do jednej repliki,  żeby wystartować od początku:

  

```bash

kubectl scale deployment credit-scoring-api --namespace=mrybinski --replicas=1

```

  

I jak już to mamy to zaktualizujmy sobie jeszcze nasz `locust.conf`  przede wszystkim żeby zmienić nazwę naszego raportu.  pozostałe opcje zostawmy takie same.

  

```

locustfile = test_decisions.py

headless = true

users = 100

spawn-rate = 5

run-time = 1m

html = report_100_random_user.html

```

  

Poczekajmy ze 30 sekund aż się Deployment zescaluje do 1 repliki.

  

następnie odpalamy testy jako:

  

```bash

locust RandomAPIUser

```

  

Jeżeli spojrzymy sobie na raport to widzimy  na wykresach że na początku  był moment kiedy Zdarzały się requesce które przekraczały tą wartość,  ale potem widać spadek w Response który jest bezpośredni powiązany z tym że po prostu dostępne zostały kolejne POD  przez HPA,  ruch został rozłożony na więcej podów i widzimy że te czasy odpowiedzi też tam spadły.

  

I na tym zakończymy nasze testy  obciążeniowe.  Zależało mi na tym żebyście zobaczyli po prostu jak można je łatwo zdefiniować z pomocą biblioteki locus i ona tutaj dostarcza nam raportów właśnie które pozwolą opowiedzieć na wasze pytania czy Wasze serwisy odpowiadają  w założonym akceptowalnym czasie.

  

Może teraz słowo komentarza jak to najlepiej Moim zdaniem zrobić w praktyce.  generalnie jeżeli wdrażacie kolejną wersję waszego serwisu który już działa na produkcji to możecie zobaczyć w danych historycznych,  na przykład Załóżmy że jest to w Google cloudzie to On dostarcza różnych metryk i wykresów tak jak widzieliście na przykład na Ingressie  jaki jest latencji na waszych serwisach,  Jaki jest request per Seconds  i na tej bazie można tak stworzyć testy obciążeniowe żeby one maksymalnie opowiadały o tym wartościom które Widzicie.  ale oprócz tego warto też  na przykład potraktować jeszcze dwa scenariusze takie same ale z większym obciążeniem o 50% i o 100%, Tak żeby zobaczyć Po prostu jak  nowa wersja waszych serwisów  działałaby pod znacznie większą obciążeniem.

  

 A skoro jeżeli wdrażacie nowy Serwis o którym nie wiecie dokładnie na jaką skalę będzie używany -  oczywiście można to mniej więcej oszacować z kontekstu biznesowego w jakim on będzie działał,  jakieś szacunki ze sprzedaży czy spojrzenie na plany  waszej firmy generalnie taki rzut okiem na to W jakich warunkach może mniej więcej działać pozwalam też zdefiniowanie na to ile potencjalnie użytkowników może korzystać z waszego API,  albo ile  różnych serwisów w waszej firmie może korzystać z waszego API i w takiej sytuacji po prostu trzeba powymyślać sobie scenariusze mniej więcej które mogłoby opowiadać rzeczywistości i takich testów przeprowadzić  jak najwięcej w różnych wariantach.  jak widzicie porównanie tych testów jest generalnie proste bo to jest kwestia napisania kilku linii jak kodu i już możemy takie pierwsze testy przeprowadzać więc generalnie  używanie locusa do  testu obciążeniowych jest naprawdę bardzo wygodne i bardzo go wam polecam.

  

Polecam Wam też przeprowadzanie testów obciążonych nie tylko pod względem sprawdzenia czy  wasze serwisy będą w stanie obsłużyć wzmożony ruch,  ale również przy tych testach wychodzą różne kwiatki związane z tym że no przy takim dużym ruchu mogło być jakieś problemy i te problemy mogą być w różnych miejscach na przykład właśnie z lot balancerami które się w jakiś sposób zapychają.  więc naprawdę  Chociażby ze względu na poprawność działania waszych serwisów warto taki load testy przeprowadzać.

  

Tak samo mogą trafić Wam się jakieś bugi, bo Te dane które są używane do testów obciążeniowych bardzo często są po prostu generowane w sposób losowy więc może zdarzyć się Case w którym wasz model  w którym wasz model nie zadziała,  więc naprawdę jest bardzo dużo benefitów z przeprowadzania takich testów obciążeniowych. W przypadku testów jednostkowych czy integracyjnych tam jest wysłany jeden bądź kilka przypadków,  które mogą być z hardkorowane,  albo też w jakiś sposób losowane,  Ale jest ich mało.  natomiast W przypadku testów obciążeniowych po prostu Wasz serwis zostanie przepytany ogromną ilością danych  i te dane będą bardzo różne,  więc może troszeczkę je też potraktować jako takie testy jednostkowe na większą skalę  i Jeżeli z lokusa będzie wychodzi że faktycznie dostajecie same dwusetki - No to należy się cieszyć

  

To tyle moje stany słowa komentarza odnośnie test obciążeniowych przejdźmy teraz sobie do ćwiczeń.

**