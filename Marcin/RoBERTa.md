# RoBERTa

[RoBERTa](https://arxiv.org/abs/1907.11692) - Robustly optimized [[BERT]] approach. 

Taka sama architektura jak [[BERT]], ale lepsza efektywność ponieważ:

1. Większy batch size
2. Więcej danych
3. RoBERTa nie jest uczona na [[Next-sentence prediction]] tak jak [[BERT]]